/home/jasonzhang/anaconda2/lib/python2.7/site-packages/numpy/ma/core.py:867: RuntimeWarning: invalid value encountered in less_equal
  return umath.less_equal(x, self.critical_value)
N, d, L:  181 7 7
loss_valid 0 1.74865429218
loss_train 0 1.72344418268
loss_valid 1 1.75032981592
loss_train 1 1.71261708092
early stop at the end of epoch:  1
[0.0, 0, 0, 0.70588235294117652, 0.37254901960784315, 0.52941176470588236, 0.52941176470588236, 0.49019607843137253, 0.62745098039215685, 0.74509803921568629, 0.7058823529411765, 0.37254901960784315, 0.5294117647058824, 0.5294117647058824, 0.49019607843137253, 0.6274509803921569, 0.7450980392156863, 51.0, 51.0, 51.0, 51.0, 51.0, 51.0, 51.0, 0, 0.55822518440819291, 0.053221288515406168, nan, 0.8181818181818182, 0.4444444444444444, 0.7142857142857143, 0.6923076923076923, 0.4166666666666667, 0.6071428571428571, 0.15151515151515152, nan, 0.3103448275862069, 0.42424242424242425, 0.6296296296296297, 0.4482758620689655, 0.46875, 0.42857142857142855, 0.5, nan, 0.6296296296296297, 0.5, 0.38461538461538464, 0.5666666666666667, 0.48148148148148145, 0.5909090909090909, 0.8214285714285714, nan, 0.36, 0.4230769230769231, 0.45161290322580644, 0.4482758620689655, 0.4642857142857143, 0.5185185185185185, 0.4666666666666667, nan, 0.5517241379310345, 0.5588235294117647, 0.5161290322580645, 0.5384615384615384, 0.5862068965517241, 0.5517241379310345, 0.6923076923076923, nan, 0.5454545454545454, 0.6666666666666666, 0.6521739130434783, 0.72, 0.5416666666666666, 0.42857142857142855, 0.5, nan, 0, 20, 25, 26, 24, 22, 26, 31, 0, 27, 31, 25, 27, 30, 26, 24, 0, 25, 26, 24, 28, 25, 20, 26, 0, 23, 24, 29, 27, 26, 25, 28, 0, 27, 32, 29, 24, 27, 27, 24, 0, 31, 25, 21, 23, 22, 19, 20, 0, 0.51048086056169306]
N, d, L:  181 7 7
loss_valid 0 1.73384048207
loss_train 0 1.57399469731
loss_valid 1 1.7180548862
loss_train 1 1.51794714812
loss_valid 2 1.7041857045
loss_train 2 1.45850834355
loss_valid 3 1.69289255203
loss_train 3 1.39653199691
loss_valid 4 1.68442101835
loss_train 4 1.33364245589
loss_valid 5 1.67855776567
loss_train 5 1.27190113904
loss_valid 6 1.67495264136
loss_train 6 1.2133031845
loss_valid 7 1.6732399049
loss_train 7 1.15926122199
loss_valid 8 1.67311464933
loss_train 8 1.11052298728
loss_valid 9 1.67427061279
loss_train 9 1.06722194322
early stop at the end of epoch:  9
[0.0, 0, 0, 0.47058823529411764, 0.5490196078431373, 0.70588235294117652, 0.58823529411764708, 0.5490196078431373, 0.5490196078431373, 0.58823529411764708, 0.47058823529411764, 0.5490196078431373, 0.7058823529411765, 0.5882352941176471, 0.5490196078431373, 0.5490196078431373, 0.5882352941176471, 51.0, 51.0, 51.0, 51.0, 51.0, 51.0, 51.0, 0, 0.56777598079866565, 0.15966386554621848, nan, 0.5384615384615384, 0.23076923076923078, 0.5769230769230769, 0.5517241379310345, 0.47619047619047616, 0.46153846153846156, 0.7586206896551724, nan, 0.6923076923076923, 0.59375, 0.6666666666666666, 0.5769230769230769, 0.5714285714285714, 0.6896551724137931, 0.6896551724137931, nan, 0.6896551724137931, 0.6666666666666666, 0.5862068965517241, 0.5185185185185185, 0.7586206896551724, 0.6521739130434783, 0.4230769230769231, nan, 0.6666666666666666, 0.48, 0.6296296296296297, 0.5384615384615384, 0.45454545454545453, 0.4642857142857143, 0.5, nan, 0.5, 0.5769230769230769, 0.6764705882352942, 0.5517241379310345, 0.4230769230769231, 0.5, 0.6551724137931034, nan, 0.4642857142857143, 0.6896551724137931, 0.6666666666666666, 0.5, 0.42857142857142855, 0.6896551724137931, 0.5185185185185185, nan, 0, 24, 24, 24, 27, 19, 24, 27, 0, 24, 30, 31, 24, 26, 27, 27, 0, 27, 25, 27, 25, 27, 21, 24, 0, 25, 23, 25, 24, 20, 26, 26, 0, 24, 24, 32, 27, 24, 28, 27, 0, 26, 27, 25, 26, 26, 27, 25, 0, 0.55818694382353451]
N, d, L:  182 7 7
loss_valid 0 1.76104030481
loss_train 0 1.7321415245
loss_valid 1 1.75351834523
loss_train 1 1.72037975988
loss_valid 2 1.74743918074
loss_train 2 1.70980668548
loss_valid 3 1.74312236894
loss_train 3 1.70107840528
loss_valid 4 1.74084566178
loss_train 4 1.69435164899
loss_valid 5 1.740738776
loss_train 5 1.68916490383
loss_valid 6 1.7427460342
loss_train 6 1.68476300534
early stop at the end of epoch:  6
[0.0, 0, 0, 0.57999999999999996, 0.5, 0.66000000000000003, 0.56000000000000005, 0.52000000000000002, 0.69999999999999996, 0.56000000000000005, 0.58, 0.5, 0.66, 0.56, 0.52, 0.7, 0.56, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 0, 0.57912751050426736, 0.10095238095238095, nan, 0.5555555555555556, 0.30434782608695654, 0.6153846153846154, 0.7931034482758621, 0.4, 0.6666666666666666, 0.3333333333333333, nan, 0.5, 0.6538461538461539, 0.6060606060606061, 0.48, 0.6666666666666666, 0.6451612903225806, 0.6071428571428571, nan, 0.68, 0.7586206896551724, 0.6071428571428571, 0.5769230769230769, 0.5714285714285714, 0.42857142857142855, 0.2413793103448276, nan, 0.6071428571428571, 0.5769230769230769, 0.6153846153846154, 0.36, 0.3333333333333333, 0.4, 0.4230769230769231, nan, 0.2608695652173913, 0.7142857142857143, 0.5862068965517241, 0.6551724137931034, 0.5, 0.6785714285714286, 0.8709677419354839, nan, 0.7241379310344828, 0.5555555555555556, 0.4666666666666667, 0.35714285714285715, 0.4642857142857143, 0.5151515151515151, 0.4, nan, 0, 25, 21, 24, 27, 23, 25, 25, 0, 24, 24, 31, 23, 22, 29, 26, 0, 23, 27, 26, 24, 26, 26, 27, 0, 26, 24, 24, 23, 19, 23, 24, 0, 21, 19, 27, 27, 24, 26, 29, 0, 27, 25, 28, 26, 26, 31, 23, 0, 0.51973331778940957]
N, d, L:  182 7 7
loss_valid 0 1.67680200904
loss_train 0 1.61894538567
loss_valid 1 1.67557549839
loss_train 1 1.57055474838
loss_valid 2 1.68287475889
loss_train 2 1.517894385
early stop at the end of epoch:  2
[0.02, 0, 0, 0.52000000000000002, 0.38, 0.40000000000000002, 0.62, 0.54000000000000004, 0.85999999999999999, 0.83999999999999997, 0.52, 0.38, 0.4, 0.62, 0.54, 0.86, 0.84, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 0, 0.56818159257810652, 0.15238095238095239, nan, 0.7741935483870968, 0.5185185185185185, 0.78125, 0.5161290322580645, 0.14285714285714285, 0.3448275862068966, 0.2608695652173913, nan, 0.6428571428571429, 0.6333333333333333, 0.4642857142857143, 0.07142857142857142, 0.25, 0.4444444444444444, 0.6153846153846154, nan, 0.48484848484848486, 0.38461538461538464, 0.07692307692307693, 0.21428571428571427, 0.8181818181818182, 0.6666666666666666, 0.6666666666666666, nan, 0.7083333333333334, 0.42857142857142855, 0.5384615384615384, 0.6521739130434783, 0.5384615384615384, 0.7142857142857143, 0.7333333333333333, nan, 0.16666666666666666, 0.4117647058823529, 0.9615384615384616, 0.9615384615384616, 0.9285714285714286, 0.7692307692307693, 0.8333333333333334, nan, 0.6206896551724138, 0.8, 0.8181818181818182, 0.8461538461538461, 0.75, 0.75, 0.6, nan, 0, 29, 25, 30, 29, 26, 27, 21, 0, 26, 28, 26, 26, 30, 25, 24, 0, 31, 24, 24, 26, 20, 22, 19, 0, 22, 26, 24, 21, 24, 26, 28, 0, 28, 32, 24, 24, 26, 24, 22, 0, 27, 23, 20, 24, 26, 18, 23, 0, 0.502604675820155]
N, d, L:  182 7 7
loss_valid 0 1.74399346848
loss_train 0 1.73234142052
loss_valid 1 1.73965893657
loss_train 1 1.72301903432
loss_valid 2 1.73673231452
loss_train 2 1.71549113017
loss_valid 3 1.73553127403
loss_train 3 1.70983758513
loss_valid 4 1.73626430479
loss_train 4 1.70592148689
early stop at the end of epoch:  4
[0.02, 0, 0, 0.64000000000000001, 0.85999999999999999, 0.68000000000000005, 0.44, 0.69999999999999996, 0.46000000000000002, 0.47999999999999998, 0.64, 0.86, 0.68, 0.44, 0.7, 0.46, 0.48, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 0, 0.59189975539614581, 0.051428571428571414, nan, 0.5294117647058824, 0.4230769230769231, 0.5555555555555556, 0.6296296296296297, 0.5652173913043478, 0.5416666666666666, 0.6, nan, 0.5416666666666666, 0.8421052631578947, 0.8095238095238095, 0.8095238095238095, 0.6666666666666666, 0.7142857142857143, 0.4666666666666667, nan, 0.6666666666666666, 0.5185185185185185, 0.6956521739130435, 0.5789473684210527, 0.25925925925925924, 0.22857142857142856, 0.16666666666666666, nan, 0.32142857142857145, 0.5333333333333333, 0.52, 0.5925925925925926, 0.42424242424242425, 0.5555555555555556, 0.7307692307692307, nan, 0.5833333333333334, 0.7407407407407407, 0.5483870967741935, 0.30303030303030304, 0.4838709677419355, 0.5416666666666666, 0.3333333333333333, nan, 0.6428571428571429, 0.5666666666666667, 0.3888888888888889, 0.5714285714285714, 0.5517241379310345, 0.37037037037037035, 0.38461538461538464, nan, 0, 32, 24, 25, 25, 21, 22, 18, 0, 22, 17, 19, 19, 16, 26, /home/jasonzhang/anaconda2/lib/python2.7/site-packages/numpy/lib/nanfunctions.py:703: RuntimeWarning: Mean of empty slice
  warnings.warn("Mean of empty slice", RuntimeWarning)
/home/jasonzhang/anaconda2/lib/python2.7/site-packages/numpy/lib/nanfunctions.py:1202: RuntimeWarning: Degrees of freedom <= 0 for slice.
  warnings.warn("Degrees of freedom <= 0 for slice.", RuntimeWarning)
28, 0, 28, 25, 21, 17, 25, 33, 22, 0, 26, 28, 23, 25, 31, 25, 24, 0, 22, 25, 29, 31, 29, 22, 28, 0, 26, 28, 34, 33, 27, 25, 24, 0, 0.50890814136713147]
{'perf': [array([  8.00000000e-03,   0.00000000e+00,   0.00000000e+00,
         5.83294118e-01,   5.32313725e-01,   5.95058824e-01,
         5.47529412e-01,   5.59843137e-01,   6.39294118e-01,
         6.42666667e-01,   5.83294118e-01,   5.32313725e-01,
         5.95058824e-01,   5.47529412e-01,   5.59843137e-01,
         6.39294118e-01,   6.42666667e-01,   5.04000000e+01,
         5.04000000e+01,   5.04000000e+01,   5.04000000e+01,
         5.04000000e+01,   5.04000000e+01,   5.04000000e+01,
         0.00000000e+00,   5.73042005e-01,   1.03529412e-01,
                    nan,   6.43160845e-01,   3.84231389e-01,
         6.48679792e-01,   6.36578788e-01,   4.00186335e-01,
         5.24368448e-01,   4.20867748e-01,              nan,
         5.37435266e-01,   6.29455435e-01,   6.35233285e-01,
         4.77230264e-01,   5.24702381e-01,   5.84423610e-01,
         5.75769862e-01,              nan,   6.30159991e-01,
         5.65684252e-01,   4.70108078e-01,   4.91068269e-01,
         5.77794364e-01,   5.13378506e-01,   4.63843628e-01,
                    nan,   5.32714286e-01,   4.88380952e-01,
         5.51017737e-01,   5.18300781e-01,   4.42973693e-01,
         5.30529101e-01,   5.70769231e-01,              nan,
         4.12518741e-01,   6.00507553e-01,   6.57746415e-01,
         6.01985371e-01,   5.84345243e-01,   6.08238600e-01,
         6.77022903e-01,              nan,   5.99484998e-01,
         6.55708812e-01,   5.98515591e-01,   5.98945055e-01,
         5.47249589e-01,   5.50749697e-01,   4.80626781e-01,
                    nan,   0.00000000e+00,   2.60000000e+01,
         2.38000000e+01,   2.58000000e+01,   2.64000000e+01,
         2.22000000e+01,   2.48000000e+01,   2.44000000e+01,
         0.00000000e+00,   2.46000000e+01,   2.60000000e+01,
         2.64000000e+01,   2.38000000e+01,   2.48000000e+01,
         2.66000000e+01,   2.58000000e+01,   0.00000000e+00,
         2.68000000e+01,   2.54000000e+01,   2.44000000e+01,
         2.40000000e+01,   2.46000000e+01,   2.44000000e+01,
         2.36000000e+01,   0.00000000e+00,   2.44000000e+01,
         2.50000000e+01,   2.50000000e+01,   2.40000000e+01,
         2.40000000e+01,   2.50000000e+01,   2.60000000e+01,
         0.00000000e+00,   2.44000000e+01,   2.64000000e+01,
         2.82000000e+01,   2.66000000e+01,   2.60000000e+01,
         2.54000000e+01,   2.60000000e+01,   0.00000000e+00,
         2.74000000e+01,   2.56000000e+01,   2.56000000e+01,
         2.64000000e+01,   2.54000000e+01,   2.40000000e+01,
         2.30000000e+01,   0.00000000e+00,   5.19982788e-01]), array([ 0.00979796,  0.        ,  0.        ,  0.08362182,  0.17744058,
        0.11495364,  0.06157157,  0.07292878,  0.13625888,  0.1309119 ,
        0.08362182,  0.17744058,  0.11495364,  0.06157157,  0.07292878,
        0.13625888,  0.1309119 ,  0.48989795,  0.48989795,  0.48989795,
        0.48989795,  0.48989795,  0.48989795,  0.48989795,  0.        ,
        0.0115199 ,  0.04645341,         nan,  0.12599793,  0.10305303,
        0.0858006 ,  0.09936038,  0.14106866,  0.11276714,  0.22448961,
               nan,  0.13273177,  0.1335793 ,  0.110879  ,  0.23911449,
        0.15561691,  0.12289099,  0.08144696,         nan,  0.07546145,
        0.13171703,  0.22137346,  0.14012209,  0.2006318 ,  0.16557761,
        0.24838302,         nan,  0.16049721,  0.05961956,  0.06530544,
        0.10374042,  0.06644984,  0.10579518,  0.13392963,         nan,
        0.16712238,  0.11876739,  0.16110434,  0.21356165,  0.17983982,
        0.10020627,  0.19020049,         nan,  0.08839374,  0.08950034,
        0.1530389 ,  0.17016584,  0.11146877,  0.14678343,  0.07972445,
               nan,  0.        ,  4.14728827,  1.46969385,  2.22710575,
        1.74355958,  2.31516738,  1.72046505,  4.54312668,  0.        ,
        1.74355958,  5.09901951,  4.45421149,  2.78567766,  5.30659966,
        1.356466  ,  1.6       ,  0.        ,  2.71293199,  1.0198039 ,
        2.05912603,  3.74165739,  2.41660919,  4.7581509 ,  2.87054002,
        0.        ,  1.62480768,  1.78885438,  2.0976177 ,  2.        ,
        4.33589668,  1.09544512,  1.78885438,  0.        ,  2.72763634,
        5.0039984 ,  2.63818119,  2.57681975,  1.8973666 ,  2.15406592,
        2.60768096,  0.        ,  1.8547237 ,  1.74355958,  5.08330601,
        3.49857114,  1.74355958,  4.89897949,  1.67332005,  0.        ,
        0.01987236])]}
SMPrank.py:290: RuntimeWarning: invalid value encountered in double_scalars
  self.probfeature[k] = self.probfeature[k] / sum_probf
dataset, N,d,L,K bodyfat 252 7 7 5
N, d, L:  181 7 7
loss_valid 0 1.61692357103
loss_train 0 1.53756731065
loss_valid 1 1.5085902621
loss_train 1 1.4195968612
loss_valid 2 1.43019940802
loss_train 2 1.33489015376
loss_valid 3 1.37089569016
loss_train 3 1.2719979911
loss_valid 4 1.32548865751
loss_train 4 1.22377474173
loss_valid 5 1.29015931822
loss_train 5 1.18539940964
loss_valid 6 1.26306761381
loss_train 6 1.15411769861
loss_valid 7 1.24187021513
loss_train 7 1.12821660741
loss_valid 8 1.22541723122
loss_train 8 1.10652177388
loss_valid 9 1.21223757796
loss_train 9 1.08826994113
loss_valid 10 1.20148349645
loss_train 10 1.07280239816
loss_valid 11 1.19278366476
loss_train 11 1.05944335509
loss_valid 12 1.18556192067
loss_train 12 1.04788955219
loss_valid 13 1.17946245199
loss_train 13 1.03782263301
loss_valid 14 1.17403403362
loss_train 14 1.02902933157
loss_valid 15 1.16901311316
loss_train 15 1.02130632008
loss_valid 16 1.16437246832
loss_train 16 1.01451928336
loss_valid 17 1.16007865363
loss_train 17 1.00845831871
loss_valid 18 1.15625490122
loss_train 18 1.00303211364
loss_valid 19 1.15267933154
loss_train 19 0.99820042161
loss_valid 20 1.14949544017
loss_train 20 0.993805094124
loss_valid 21 1.14677699087
loss_train 21 0.98978586666
loss_valid 22 1.14407331249
loss_train 22 0.986123519218
loss_valid 23 1.14172006085
loss_train 23 0.982728103404
loss_valid 24 1.13967788343
loss_train 24 0.979568476537
loss_valid 25 1.13801548439
loss_train 25 nan
loss_valid 26 nan
loss_train 26 nan
loss_valid 27 nan
loss_train 27 nan
loss_valid 28 nan
loss_train 28 nan
loss_valid 29 nan
loss_train 29 nan
loss_valid 30 nan
loss_train 30 nan
loss_valid 31 nan
loss_train 31 nan
loss_valid 32 nan
loss_train 32 nan
loss_valid 33 nan
loss_train 33 nan
loss_valid 34 nan
loss_train 34 nan
loss_valid 35 nan
loss_train 35 nan
loss_valid 36 nan
loss_train 36 nan
loss_valid 37 nan
loss_train 37 nan
loss_valid 38 nan
loss_train 38 nan
loss_valid 39 nan
loss_train 39 nan
loss_valid 40 nan
loss_train 40 nan
loss_valid 41 nan
loss_train 41 nan
loss_valid 42 nan
loss_train 42 nan
loss_valid 43 nan
loss_train 43 nan
loss_valid 44 nan
loss_train 44 nan
loss_valid 45 nan
loss_train 45 nan
loss_valid 46 nan
loss_train 46 nan
loss_valid 47 nan
loss_train 47 nan
loss_valid 48 nan
loss_train 48 nan
loss_valid 49 nan
loss_train 49 nan
loss_valid 50 nan
loss_train 50 nan
loss_valid 51 nan
loss_train 51 nan
loss_valid 52 nan
loss_train 52 nan
loss_valid 53 nan
loss_train 53 nan
loss_valid 54 nan
loss_train 54 nan
loss_valid 55 nan
loss_train 55 nan
loss_valid 56 nan
loss_train 56 nan
loss_valid 57 nan
loss_train 57 nan
loss_valid 58 nan
loss_train 58 nan
loss_valid 59 nan
loss_train 59 nan
loss_valid 60 nan
loss_train 60 nan
loss_valid 61 nan
loss_train 61 nan
loss_valid 62 nan
loss_train 62 nan
loss_valid 63 nan
loss_train 63 nan
loss_valid 64 nan
loss_train 64 nan
loss_valid 65 nan
loss_train 65 nan
loss_valid 66 nan
loss_train 66 nan
loss_valid 67 nan
loss_train 67 nan
loss_valid 68 nan
loss_train 68 nan
loss_valid 69 nan
loss_train 69 nan
loss_valid 70 nan
loss_train 70 nan
loss_valid 71 nan
loss_train 71 nan
loss_valid 72 nan
loss_train 72 nan
loss_valid 73 nan
loss_train 73 nan
loss_valid 74 nan
loss_train 74 nan
loss_valid 75 nan
loss_train 75 nan
loss_valid 76 nan
loss_train 76 nan
loss_valid 77 nan
loss_train 77 nan
loss_valid 78 nan
loss_train 78 nan
loss_valid 79 nan
loss_train 79 nan
loss_valid 80 nan
loss_train 80 nan
loss_valid 81 nan
loss_train 81 nan
loss_valid 82 nan
loss_train 82 nan
loss_valid 83 nan
loss_train 83 nan
loss_valid 84 nan
loss_train 84 nan
loss_valid 85 nan
loss_train 85 nan
loss_valid 86 nan
loss_train 86 nan
loss_valid 87 nan
loss_train 87 nan
loss_valid 88 nan
loss_train 88 nan
loss_valid 89 nan
loss_train 89 nan
loss_valid 90 nan
loss_train 90 nan
loss_valid 91 nan
loss_train 91 nan
loss_valid 92 nan
loss_train 92 nan
loss_valid 93 nan
loss_train 93 nan
loss_valid 94 nan
loss_train 94 nan
loss_valid 95 nan
loss_train 95 nan
loss_valid 96 nan
loss_train 96 nan
loss_valid 97 nan
loss_train 97 nan
loss_valid 98 nan
loss_t/home/jasonzhang/anaconda2/lib/python2.7/site-packages/numpy/ma/core.py:867: RuntimeWarning: invalid value encountered in less_equal
  return umath.less_equal(x, self.critical_value)
rain 98 nan
loss_valid 99 nan
loss_train 99 nan
[0.0, 0, 0, 0.11764705882352941, 0.19607843137254902, 0.49019607843137253, 0.60784313725490191, 0.6470588235294118, 0.82352941176470584, 1.0, 0.11764705882352941, 0.19607843137254902, 0.49019607843137253, 0.6078431372549019, 0.6470588235294118, 0.8235294117647058, 1.0, 51.0, 51.0, 51.0, 51.0, 51.0, 51.0, 51.0, 0, 0.448715149456205, -0.030812324929971987, nan, 0.045454545454545456, 0.037037037037037035, 0.03571428571428571, 0.038461538461538464, 0.041666666666666664, 0.03571428571428571, 0.9696969696969697, nan, 0.034482758620689655, 0.030303030303030304, 0.037037037037037035, 0.034482758620689655, 0.03125, 0.9642857142857143, 0.9615384615384616, nan, 0.037037037037037035, 0.03571428571428571, 0.038461538461538464, 0.03333333333333333, 0.9629629629629629, 0.9545454545454546, 0.9642857142857143, nan, 0.04, 0.038461538461538464, 0.03225806451612903, 0.9655172413793104, 0.9642857142857143, 0.9629629629629629, 0.9666666666666667, nan, 0.034482758620689655, 0.029411764705882353, 0.967741935483871, 0.9615384615384616, 0.9655172413793104, 0.9655172413793104, 0.9615384615384616, nan, 0.030303030303030304, 0.9629629629629629, 0.9565217391304348, 0.96, 0.9583333333333334, 0.9523809523809523, 0.9545454545454546, nan, 0, 20, 25, 26, 24, 22, 26, 31, 0, 27, 31, 25, 27, 30, 26, 24, 0, 25, 26, 24, 28, 25, 20, 26, 0, 23, 24, 29, 27, 26, 25, 28, 0, 27, 32, 29, 24, 27, 27, 24, 0, 31, 25, 21, 23, 22, 19, 20, 0, 0.18495516923296976]
N, d, L:  181 7 7
loss_valid 0 1.55615459918
loss_train 0 1.54529002654
loss_valid 1 1.4289581232
loss_train 1 1.40775514433
loss_valid 2 1.34236072177
loss_train 2 1.31104785336
loss_valid 3 1.28120842258
loss_train 3 1.24271030725
loss_valid 4 1.23670911293
loss_train 4 1.19239095872
loss_valid 5 1.20184107161
loss_train 5 1.15403797744
loss_valid 6 1.17413820929
loss_train 6 1.12364878924
loss_valid 7 1.15136058939
loss_train 7 1.09899152078
loss_valid 8 1.13136837694
loss_train 8 1.07880910712
loss_valid 9 1.11586887187
loss_train 9 1.06185445665
loss_valid 10 1.10382293183
loss_train 10 1.04744501988
loss_valid 11 1.09306721701
loss_train 11 1.03517792836
loss_valid 12 1.08369441751
loss_train 12 1.02457318429
loss_valid 13 1.07522386871
loss_train 13 1.0153926184
loss_valid 14 1.07013403893
loss_train 14 1.00693445449
loss_valid 15 1.06538396579
loss_train 15 0.999437580887
loss_valid 16 1.06106415677
loss_train 16 0.99254924927
loss_valid 17 1.05679147605
loss_train 17 0.986345457242
loss_valid 18 1.05367365626
loss_train 18 0.980869847491
loss_valid 19 1.05062061464
loss_train 19 0.97617419648
loss_valid 20 1.04750122882
loss_train 20 0.971999372207
loss_valid 21 1.04491210697
loss_train 21 0.967902273159
loss_valid 22 1.04300495055
loss_train 22 0.964561183393
loss_valid 23 nan
loss_train 23 nan
loss_valid 24 nan
loss_train 24 nan
loss_valid 25 nan
loss_train 25 nan
loss_valid 26 nan
loss_train 26 nan
loss_valid 27 nan
loss_train 27 nan
loss_valid 28 nan
loss_train 28 nan
loss_valid 29 nan
loss_train 29 nan
loss_valid 30 nan
loss_train 30 nan
loss_valid 31 nan
loss_train 31 nan
loss_valid 32 nan
loss_train 32 nan
loss_valid 33 nan
loss_train 33 nan
loss_valid 34 nan
loss_train 34 nan
loss_valid 35 nan
loss_train 35 nan
loss_valid 36 nan
loss_train 36 nan
loss_valid 37 nan
loss_train 37 nan
loss_valid 38 nan
loss_train 38 nan
loss_valid 39 nan
loss_train 39 nan
loss_valid 40 nan
loss_train 40 nan
loss_valid 41 nan
loss_train 41 nan
loss_valid 42 nan
loss_train 42 nan
loss_valid 43 nan
loss_train 43 nan
loss_valid 44 nan
loss_train 44 nan
loss_valid 45 nan
loss_train 45 nan
loss_valid 46 nan
loss_train 46 nan
loss_valid 47 nan
loss_train 47 nan
loss_valid 48 nan
loss_train 48 nan
loss_valid 49 nan
loss_train 49 nan
loss_valid 50 nan
loss_train 50 nan
loss_valid 51 nan
loss_train 51 nan
loss_valid 52 nan
loss_train 52 nan
loss_valid 53 nan
loss_train 53 nan
loss_valid 54 nan
loss_train 54 nan
loss_valid 55 nan
loss_train 55 nan
loss_valid 56 nan
loss_train 56 nan
loss_valid 57 nan
loss_train 57 nan
loss_valid 58 nan
loss_train 58 nan
loss_valid 59 nan
loss_train 59 nan
loss_valid 60 nan
loss_train 60 nan
loss_valid 61 nan
loss_train 61 nan
loss_valid 62 nan
loss_train 62 nan
loss_valid 63 nan
loss_train 63 nan
loss_valid 64 nan
loss_train 64 nan
loss_valid 65 nan
loss_train 65 nan
loss_valid 66 nan
loss_train 66 nan
loss_valid 67 nan
loss_train 67 nan
loss_valid 68 nan
loss_train 68 nan
loss_valid 69 nan
loss_train 69 nan
loss_valid 70 nan
loss_train 70 nan
loss_valid 71 nan
loss_train 71 nan
loss_valid 72 nan
loss_train 72 nan
loss_valid 73 nan
loss_train 73 nan
loss_valid 74 nan
loss_train 74 nan
loss_valid 75 nan
loss_train 75 nan
loss_valid 76 nan
loss_train 76 nan
loss_valid 77 nan
loss_train 77 nan
loss_valid 78 nan
loss_train 78 nan
loss_valid 79 nan
loss_train 79 nan
loss_valid 80 nan
loss_train 80 nan
loss_valid 81 nan
loss_train 81 nan
loss_valid 82 nan
loss_train 82 nan
loss_valid 83 nan
loss_train 83 nan
loss_valid 84 nan
loss_train 84 nan
loss_valid 85 nan
loss_train 85 nan
loss_valid 86 nan
loss_train 86 nan
loss_valid 87 nan
loss_train 87 nan
loss_valid 88 nan
loss_train 88 nan
loss_valid 89 nan
loss_train 89 nan
loss_valid 90 nan
loss_train 90 nan
loss_valid 91 nan
loss_train 91 nan
loss_valid 92 nan
loss_train 92 nan
loss_valid 93 nan
loss_train 93 nan
loss_valid 94 nan
loss_train 94 nan
loss_valid 95 nan
loss_train 95 nan
loss_valid 96 nan
loss_train 96 nan
loss_valid 97 nan
loss_train 97 nan
loss_valid 98 nan
loss_train 98 nan
loss_valid 99 nan
loss_train 99 nan
[0.0196078431372549, 0, 0, 0.058823529411764705, 0.21568627450980393, 0.37254901960784315, 0.60784313725490191, 0.74509803921568629, 0.88235294117647056, 1.0, 0.058823529411764705, 0.21568627450980393, 0.37254901960784315, 0.6078431372549019, 0.7450980392156863, 0.8823529411764706, 1.0, 51.0, 51.0, 51.0, 51.0, 51.0, 51.0, 51.0, 0, 0.40821252601407365, 0.014005602240896352, nan, 0.038461538461538464, 0.038461538461538464, 0.038461538461538464, 0.034482758620689655, 0.047619047619047616, 0.038461538461538464, 0.9655172413793104, nan, 0.038461538461538464, 0.03125, 0.030303030303030304, 0.038461538461538464, 0.03571428571428571, 0.9655172413793104, 0.9655172413793104, nan, 0.034482758620689655, 0.037037037037037035, 0.034482758620689655, 0.037037037037037035, 0.9655172413793104, 0.9565217391304348, 0.9615384615384616, nan, 0.037037037037037035, 0.04, 0.037037037037037035, 0.9615384615384616, 0.9545454545454546, 0.9642857142857143, 0.9642857142857143, nan, 0.038461538461538464, 0.038461538461538464, 0.9705882352941176, 0.9655172413793104, 0.9615384615384616, 0.9666666666666667, 0.9655172413793104, nan, 0.03571428571428571, 0.9655172413793104, 0.9629629629629629, 0.9642857142857143, 0.9642857142857143, 0.9655172413793104, 0.9629629629629629, nan, 0, 24, 24, 24, 27, 19, 24, 27, 0, 24, 30, 31, 24, 26, 27, 27, 0, 27, 25, 27, 25, 27, 21, 24, 0, 25, 23, 25, 24, 20, 26, 26, 0, 24, 24, 32, 27, 24, 28, 27, 0, 26, 27, 25, 26, 26, 27, 25, 0, 0.18881207833762206]
N, d, L:  182 7 7
loss_valid 0 1.55399801729
loss_train 0 1.54228301151
loss_valid 1 1.43284059442
loss_train 1 1.42354468499
loss_valid 2 1.3455753674
loss_train 2 1.33871078686
loss_valid 3 1.2830644131
loss_train 3 1.27671759673
loss_valid 4 1.23795211231
loss_train 4 1.23023405919
loss_valid 5 1.20444899409
loss_train 5 1.1945047178
loss_valid 6 1.1785014916
loss_train 6 1.16642808441
loss_valid 7 1.15743739586
loss_train 7 1.14383289216
loss_valid 8 1.1400304283
loss_train 8 1.12532173637
loss_valid 9 1.12500277162
loss_train 9 1.10988708352
loss_valid 10 1.11174501384
loss_train 10 1.09675092592
loss_valid 11 1.10009442565
loss_train 11 1.08556817631
loss_valid 12 1.08981480675
loss_train 12 1.07599621492
loss_valid 13 1.08091295352
loss_train 13 1.0677496516
loss_valid 14 1.07301990595
loss_train 14 1.06058678506
loss_valid 15 1.06625789343
loss_train 15 1.05428735296
loss_valid 16 1.06079651735
loss_train 16 1.04857118885
loss_valid 17 1.05613866434
loss_train 17 1.04320768995
loss_valid 18 1.05223230655
loss_train 18 1.03816768537
loss_valid 19 1.04968996658
loss_train 19 1.0333783741
loss_valid 20 1.04739194283
loss_train 20 1.02863732123
loss_valid 21 1.04657132667
loss_train 21 1.02432035506
loss_valid 22 1.04636137258
loss_train 22 1.02033210895
loss_valid 23 1.0461148329
loss_train 23 1.01664413648
loss_valid 24 1.04349760099
loss_train 24 1.01323233988
loss_valid 25 1.04178937491
loss_train 25 1.01022529438
loss_valid 26 1.04124728541
loss_train 26 1.00744005468
loss_valid 27 nan
loss_train 27 nan
loss_valid 28 nan
loss_train 28 nan
loss_valid 29 nan
loss_train 29 nan
loss_valid 30 nan
loss_train 30 nan
loss_valid 31 nan
loss_train 31 nan
loss_valid 32 nan
loss_train 32 nan
loss_valid 33 nan
loss_train 33 nan
loss_valid 34 nan
loss_train 34 nan
loss_valid 35 nan
loss_train 35 nan
loss_valid 36 nan
loss_train 36 nan
loss_valid 37 nan
loss_train 37 nan
loss_valid 38 nan
loss_train 38 nan
loss_valid 39 nan
loss_train 39 nan
loss_valid 40 nan
loss_train 40 nan
loss_valid 41 nan
loss_train 41 nan
loss_valid 42 nan
loss_train 42 nan
loss_valid 43 nan
loss_train 43 nan
loss_valid 44 nan
loss_train 44 nan
loss_valid 45 nan
loss_train 45 nan
loss_valid 46 nan
loss_train 46 nan
loss_valid 47 nan
loss_train 47 nan
loss_valid 48 nan
loss_train 48 nan
loss_valid 49 nan
loss_train 49 nan
loss_valid 50 nan
loss_train 50 nan
loss_valid 51 nan
loss_train 51 nan
loss_valid 52 nan
loss_train 52 nan
loss_valid 53 nan
loss_train 53 nan
loss_valid 54 nan
loss_train 54 nan
loss_valid 55 nan
loss_train 55 nan
loss_valid 56 nan
loss_train 56 nan
loss_valid 57 nan
loss_train 57 nan
loss_valid 58 nan
loss_train 58 nan
loss_valid 59 nan
loss_train 59 nan
loss_valid 60 nan
loss_train 60 nan
loss_valid 61 nan
loss_train 61 nan
loss_valid 62 nan
loss_train 62 nan
loss_valid 63 nan
loss_train 63 nan
loss_valid 64 nan
loss_train 64 nan
loss_valid 65 nan
loss_train 65 nan
loss_valid 66 nan
loss_train 66 nan
loss_valid 67 nan
loss_train 67 nan
loss_valid 68 nan
loss_train 68 nan
loss_valid 69 nan
loss_train 69 nan
loss_valid 70 nan
loss_train 70 nan
loss_valid 71 nan
loss_train 71 nan
loss_valid 72 nan
loss_train 72 nan
loss_valid 73 nan
loss_train 73 nan
loss_valid 74 nan
loss_train 74 nan
loss_valid 75 nan
loss_train 75 nan
loss_valid 76 nan
loss_train 76 nan
loss_valid 77 nan
loss_train 77 nan
loss_valid 78 nan
loss_train 78 nan
loss_valid 79 nan
loss_train 79 nan
loss_valid 80 nan
loss_train 80 nan
loss_valid 81 nan
loss_train 81 nan
loss_valid 82 nan
loss_train 82 nan
loss_valid 83 nan
loss_train 83 nan
loss_valid 84 nan
loss_train 84 nan
loss_valid 85 nan
loss_train 85 nan
loss_valid 86 nan
loss_train 86 nan
loss_valid 87 nan
loss_train 87 nan
loss_valid 88 nan
loss_train 88 nan
loss_valid 89 nan
loss_train 89 nan
loss_valid 90 nan
loss_train 90 nan
loss_valid 91 nan
loss_train 91 nan
loss_valid 92 nan
loss_train 92 nan
loss_valid 93 nan
loss_train 93 nan
loss_valid 94 nan
loss_train 94 nan
loss_valid 95 nan
loss_train 95 nan
loss_valid 96 nan
loss_train 96 nan
loss_valid 97 nan
loss_train 97 nan
loss_valid 98 nan
loss_train 98 nan
loss_valid 99 nan
loss_train 99 nan
[0.02, 0, 0, 0.12, 0.32000000000000001, 0.35999999999999999, 0.57999999999999996, 0.71999999999999997, 0.88, 1.0, 0.12, 0.32, 0.36, 0.58, 0.72, 0.88, 1.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 0, 0.47018430359481994, 0.028571428571428571, nan, 0.037037037037037035, 0.043478260869565216, 0.038461538461538464, 0.034482758620689655, 0.04, 0.037037037037037035, 0.9629629629629629, nan, 0.038461538461538464, 0.038461538461538464, 0.030303030303030304, 0.04, 0.041666666666666664, 0.967741935483871, 0.9642857142857143, nan, 0.04, 0.034482758620689655, 0.03571428571428571, 0.038461538461538464, 0.9642857142857143, 0.9642857142857143, 0.9655172413793104, nan, 0.03571428571428571, 0.038461538461538464, 0.038461538461538464, 0.96, 0.9523809523809523, 0.96, 0.9615384615384616, nan, 0.043478260869565216, 0.047619047619047616, 0.9655172413793104, 0.9655172413793104, 0.9615384615384616, 0.9642857142857143, 0.967741935483871, nan, 0.034482758620689655, 0.9629629629629629, 0.9666666666666667, 0.9642857142857143, 0.9642857142857143, 0.9696969696969697, 0.96, nan, 0, 25, 21, 24, 27, 23, 25, 25, 0, 24, 24, 31, 23, 22, 29, 26, 0, 23, 27, 26, 24, 26, 26, 27, 0, 26, 24, 24, 23, 19, 23, 24, 0, 21, 19, 27, 27, 24, 26, 29, 0, 27, 25, 28, 26, 26, 31, 23, 0, 0.19190290901485918]
N, d, L:  182 7 7
loss_valid 0 1.68262387972
loss_train 0 1.63494994612
loss_valid 1 1.56942035831
loss_train 1 1.52616436173
loss_valid 2 1.4780053896
loss_train 2 1.4408158912
loss_valid 3 1.40039871141
loss_train 3 1.37064214435
loss_valid 4 1.33569231987
loss_train 4 1.31076620253
loss_valid 5 1.2830508235
loss_train 5 1.25931299196
loss_valid 6 1.24129395001
loss_train 6 1.21547552547
loss_valid 7 1.20759229609
loss_train 7 1.17802073365
loss_valid 8 1.17981795391
loss_train 8 1.14581282632
loss_valid 9 1.15620567171
loss_train 9 1.11956525665
loss_valid 10 1.13385399734
loss_train 10 1.0980805433
loss_valid 11 1.11479833423
loss_train 11 1.08032276925
loss_valid 12 1.09873235162
loss_train 12 1.06517841248
loss_valid 13 1.08403044816
loss_train 13 1.0517393325
loss_valid 14 1.07248083826
loss_train 14 1.04046477178
loss_valid 15 1.06471902853
loss_train 15 1.03069787448
loss_valid 16 1.05847553701
loss_train 16 1.02232171024
loss_valid 17 1.05125995238
loss_train 17 1.01471208747
loss_valid 18 1.04363037496
loss_train 18 1.00771871245
loss_valid 19 1.03639522982
loss_train 19 1.00133081658
loss_valid 20 1.03121308934
loss_train 20 0.995106664629
loss_valid 21 1.02479678784
loss_train 21 0.989779214176
loss_valid 22 1.01944270056
loss_train 22 0.984358646654
loss_valid 23 1.01547364793
loss_train 23 0.978499243012
loss_valid 24 1.01079760952
loss_train 24 0.97376377937
loss_valid 25 nan
loss_train 25 nan
loss_valid 26 nan
loss_train 26 nan
loss_valid 27 nan
loss_train 27 nan
loss_valid 28 nan
loss_train 28 nan
loss_valid 29 nan
loss_train 29 nan
loss_valid 30 nan
loss_train 30 nan
loss_valid 31 nan
loss_train 31 nan
loss_valid 32 nan
loss_train 32 nan
loss_valid 33 nan
loss_train 33 nan
loss_valid 34 nan
loss_train 34 nan
loss_valid 35 nan
loss_train 35 nan
loss_valid 36 nan
loss_train 36 nan
loss_valid 37 nan
loss_train 37 nan
loss_valid 38 nan
loss_train 38 nan
loss_valid 39 nan
loss_train 39 nan
loss_valid 40 nan
loss_train 40 nan
loss_valid 41 nan
loss_train 41 nan
loss_valid 42 nan
loss_train 42 nan
loss_valid 43 nan
loss_train 43 nan
loss_valid 44 nan
loss_train 44 nan
loss_valid 45 nan
loss_train 45 nan
loss_valid 46 nan
loss_train 46 nan
loss_valid 47 nan
loss_train 47 nan
loss_valid 48 nan
loss_train 48 nan
loss_valid 49 nan
loss_train 49 nan
loss_valid 50 nan
loss_train 50 nan
loss_valid 51 nan
loss_train 51 nan
loss_valid 52 nan
loss_train 52 nan
loss_valid 53 nan
loss_train 53 nan
loss_valid 54 nan
loss_train 54 nan
loss_valid 55 nan
loss_train 55 nan
loss_valid 56 nan
loss_train 56 nan
loss_valid 57 nan
loss_train 57 nan
loss_valid 58 nan
loss_train 58 nan
loss_valid 59 nan
loss_train 59 nan
loss_valid 60 nan
loss_train 60 nan
loss_valid 61 nan
loss_train 61 nan
loss_valid 62 nan
loss_train 62 nan
loss_valid 63 nan
loss_train 63 nan
loss_valid 64 nan
loss_train 64 nan
loss_valid 65 nan
loss_train 65 nan
loss_valid 66 nan
loss_train 66 nan
loss_valid 67 nan
loss_train 67 nan
loss_valid 68 nan
loss_train 68 nan
loss_valid 69 nan
loss_train 69 nan
loss_valid 70 nan
loss_train 70 nan
loss_valid 71 nan
loss_train 71 nan
loss_valid 72 nan
loss_train 72 nan
loss_valid 73 nan
loss_train 73 nan
loss_valid 74 nan
loss_train 74 nan
loss_valid 75 nan
loss_train 75 nan
loss_valid 76 nan
loss_train 76 nan
loss_valid 77 nan
loss_train 77 nan
loss_valid 78 nan
loss_train 78 nan
loss_valid 79 nan
loss_train 79 nan
loss_valid 80 nan
loss_train 80 nan
loss_valid 81 nan
loss_train 81 nan
loss_valid 82 nan
loss_train 82 nan
loss_valid 83 nan
loss_train 83 nan
loss_valid 84 nan
loss_train 84 nan
loss_valid 85 nan
loss_train 85 nan
loss_valid 86 nan
loss_train 86 nan
loss_valid 87 nan
loss_train 87 nan
loss_valid 88 nan
loss_train 88 nan
loss_valid 89 nan
loss_train 89 nan
loss_valid 90 nan
loss_train 90 nan
loss_valid 91 nan
loss_train 91 nan
loss_valid 92 nan
loss_train 92 nan
loss_valid 93 nan
loss_train 93 nan
loss_valid 94 nan
loss_train 94 nan
loss_valid 95 nan
loss_train 95 nan
loss_valid 96 nan
loss_train 96 nan
loss_valid 97 nan
loss_train 97 nan
loss_valid 98 nan
loss_train 98 nan
loss_valid 99 nan
loss_train 99 nan
NONERECALL: 0.0 for emoticon: 0
[0.0, 0, 0, 0.0, 0.23999999999999999, 0.44, 0.64000000000000001, 0.69999999999999996, 0.85999999999999999, 1.0, 0.001, 0.24, 0.44, 0.64, 0.7, 0.86, 1.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 0, 0.23592675678933842, -0.078095238095238093, nan, 0.03225806451612903, 0.037037037037037035, 0.03125, 0.03225806451612903, 0.03571428571428571, 0.034482758620689655, 0.9565217391304348, nan, 0.03571428571428571, 0.03333333333333333, 0.03571428571428571, 0.03571428571428571, 0.03125, 0.9629629629629629, 0.9615384615384616, nan, 0.030303030303030304, 0.038461538461538464, 0.038461538461538464, 0.03571428571428571, 0.9545454545454546, 0.9583333333333334, 0.9523809523809523, nan, 0.041666666666666664, 0.03571428571428571, 0.038461538461538464, 0.9565217391304348, 0.9615384615384616, 0.9642857142857143, 0.9666666666666667, nan, 0.03333333333333333, 0.029411764705882353, 0.9615384615384616, 0.9615384615384616, 0.9642857142857143, 0.9615384615384616, 0.9583333333333334, nan, 0.034482758620689655, 0.96, 0.9545454545454546, 0.9615384615384616, 0.9642857142857143, 0.95, 0.96, nan, 0, 29, 25, 30, 29, 26, 27, 21, 0, 26, 28, 26, 26, 30, 25, 24, 0, 31, 24, 24, 26, 20, 22, 19, 0, 22, 26, 24, 21, 24, 26, 28, 0, 28, 32, 24, 24, 26, 24, 22, 0, 27, 23, 20, 24, 26, 18, 23, 0, 0.18239855399501095]
N, d, L:  182 7 7
loss_valid 0 1.47973324948
loss_train 0 1.50175769665
loss_valid 1 1.35934061221
loss_train 1 1.37875372844
loss_valid 2 1.2701940301
loss_train 2 1.28956531513
loss_valid 3 1.20222714432
loss_train 3 1.22407122209
loss_valid 4 1.14896262305
loss_train 4 1.17532569596
loss_valid 5 1.10771777329
loss_train 5 1.13842865528
loss_valid 6 1.07620325834
loss_train 6 1.10951334431
loss_valid 7 1.052546153
loss_train 7 1.08637175553
loss_valid 8 1.03280976124
loss_train 8 1.06735464266
loss_valid 9 1.01868190043
loss_train 9 1.05157446672
loss_valid 10 1.00749826988
loss_train 10 1.03823459824
loss_valid 11 1.00023748045
loss_train 11 1.02686274108
loss_valid 12 0.991472069453
loss_train 12 1.01713917804
loss_valid 13 0.983796232999
loss_train 13 1.00859193478
loss_valid 14 0.978769575716
loss_train 14 1.00116779597
loss_valid 15 0.973134439761
loss_train 15 0.994634308811
loss_valid 16 0.968791572749
loss_train 16 0.988935493209
loss_valid 17 0.965111776288
loss_train 17 0.983885510887
loss_valid 18 0.96173332515
loss_train 18 0.979392836407
loss_valid 19 0.958908180116
loss_train 19 0.975303216875
loss_valid 20 0.956560224661
loss_train 20 0.97154454006
loss_valid 21 0.954111251207
loss_train 21 0.968026250672
loss_valid 22 0.951788687282
loss_train 22 0.964740864357
loss_valid 23 0.949437130655
loss_train 23 0.961636524497
loss_valid 24 0.947374142978
loss_train 24 0.958788022775
loss_valid 25 0.946262449242
loss_train 25 0.956222720529
loss_valid 26 0.944494385402
loss_train 26 0.953769801741
loss_valid 27 0.94342920117
loss_train 27 0.951560515111
loss_valid 28 nan
loss_train 28 nan
loss_valid 29 nan
loss_train 29 nan
loss_valid 30 nan
loss_train 30 nan
loss_valid 31 nan
loss_train 31 nan
loss_valid 32 nan
loss_train 32 nan
loss_valid 33 nan
loss_train 33 nan
loss_valid 34 nan
loss_train 34 nan
loss_valid 35 nan
loss_train 35 nan
loss_valid 36 nan
loss_train 36 nan
loss_valid 37 nan
loss_train 37 nan
loss_valid 38 nan
loss_train 38 nan
loss_valid 39 nan
loss_train 39 nan
loss_valid 40 nan
loss_train 40 nan
loss_valid 41 nan
loss_train 41 nan
loss_valid 42 nan
loss_train 42 nan
loss_valid 43 nan
loss_train 43 nan
loss_valid 44 nan
loss_train 44 nan
loss_valid 45 nan
loss_train 45 nan
loss_valid 46 nan
loss_train 46 nan
loss_valid 47 nan
loss_train 47 nan
loss_valid 48 nan
loss_train 48 nan
loss_valid 49 nan
loss_train 49 nan
loss_valid 50 nan
loss_train 50 nan
loss_valid 51 nan
loss_train 51 nan
loss_valid 52 nan
loss_train 52 nan
loss_valid 53 nan
loss_train 53 nan
loss_valid 54 nan
loss_train 54 nan
loss_valid 55 nan
loss_train 55 nan
loss_valid 56 nan
loss_train 56 nan
loss/home/jasonzhang/anaconda2/lib/python2.7/site-packages/numpy/lib/nanfunctions.py:703: RuntimeWarning: Mean of empty slice
  warnings.warn("Mean of empty slice", RuntimeWarning)
/home/jasonzhang/anaconda2/lib/python2.7/site-packages/numpy/lib/nanfunctions.py:1202: RuntimeWarning: Degrees of freedom <= 0 for slice.
  warnings.warn("Degrees of freedom <= 0 for slice.", RuntimeWarning)
_valid 57 nan
loss_train 57 nan
loss_valid 58 nan
loss_train 58 nan
loss_valid 59 nan
loss_train 59 nan
loss_valid 60 nan
loss_train 60 nan
loss_valid 61 nan
loss_train 61 nan
loss_valid 62 nan
loss_train 62 nan
loss_valid 63 nan
loss_train 63 nan
loss_valid 64 nan
loss_train 64 nan
loss_valid 65 nan
loss_train 65 nan
loss_valid 66 nan
loss_train 66 nan
loss_valid 67 nan
loss_train 67 nan
loss_valid 68 nan
loss_train 68 nan
loss_valid 69 nan
loss_train 69 nan
loss_valid 70 nan
loss_train 70 nan
loss_valid 71 nan
loss_train 71 nan
loss_valid 72 nan
loss_train 72 nan
loss_valid 73 nan
loss_train 73 nan
loss_valid 74 nan
loss_train 74 nan
loss_valid 75 nan
loss_train 75 nan
loss_valid 76 nan
loss_train 76 nan
loss_valid 77 nan
loss_train 77 nan
loss_valid 78 nan
loss_train 78 nan
loss_valid 79 nan
loss_train 79 nan
loss_valid 80 nan
loss_train 80 nan
loss_valid 81 nan
loss_train 81 nan
loss_valid 82 nan
loss_train 82 nan
loss_valid 83 nan
loss_train 83 nan
loss_valid 84 nan
loss_train 84 nan
loss_valid 85 nan
loss_train 85 nan
loss_valid 86 nan
loss_train 86 nan
loss_valid 87 nan
loss_train 87 nan
loss_valid 88 nan
loss_train 88 nan
loss_valid 89 nan
loss_train 89 nan
loss_valid 90 nan
loss_train 90 nan
loss_valid 91 nan
loss_train 91 nan
loss_valid 92 nan
loss_train 92 nan
loss_valid 93 nan
loss_train 93 nan
loss_valid 94 nan
loss_train 94 nan
loss_valid 95 nan
loss_train 95 nan
loss_valid 96 nan
loss_train 96 nan
loss_valid 97 nan
loss_train 97 nan
loss_valid 98 nan
loss_train 98 nan
loss_valid 99 nan
loss_train 99 nan
[0.04, 0, 0, 0.059999999999999998, 0.40000000000000002, 0.46000000000000002, 0.54000000000000004, 0.71999999999999997, 0.81999999999999995, 1.0, 0.06, 0.4, 0.46, 0.54, 0.72, 0.82, 1.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 0, 0.44617097216874874, 0.080000000000000002, nan, 0.029411764705882353, 0.038461538461538464, 0.037037037037037035, 0.037037037037037035, 0.043478260869565216, 0.041666666666666664, 0.95, nan, 0.041666666666666664, 0.05263157894736842, 0.047619047619047616, 0.047619047619047616, 0.05555555555555555, 0.9642857142857143, 0.9666666666666667, nan, 0.03333333333333333, 0.037037037037037035, 0.043478260869565216, 0.05263157894736842, 0.9629629629629629, 0.9714285714285714, 0.9583333333333334, nan, 0.03571428571428571, 0.03333333333333333, 0.04, 0.9629629629629629, 0.9696969696969697, 0.9629629629629629, 0.9615384615384616, nan, 0.041666666666666664, 0.037037037037037035, 0.967741935483871, 0.9696969696969697, 0.967741935483871, 0.9583333333333334, 0.9666666666666667, nan, 0.03571428571428571, 0.9666666666666667, 0.9722222222222222, 0.9714285714285714, 0.9655172413793104, 0.9629629629629629, 0.9615384615384616, nan, 0, 32, 24, 25, 25, 21, 22, 18, 0, 22, 17, 19, 19, 16, 26, 28, 0, 28, 25, 21, 17, 25, 33, 22, 0, 26, 28, 23, 25, 31, 25, 24, 0, 22, 25, 29, 31, 29, 22, 28, 0, 26, 28, 34, 33, 27, 25, 24, 0, 0.19771419068986754]
{'perf': [array([  1.59215686e-02,   0.00000000e+00,   0.00000000e+00,
         7.12941176e-02,   2.74352941e-01,   4.24549020e-01,
         5.95137255e-01,   7.06431373e-01,   8.53176471e-01,
         1.00000000e+00,   7.14941176e-02,   2.74352941e-01,
         4.24549020e-01,   5.95137255e-01,   7.06431373e-01,
         8.53176471e-01,   1.00000000e+00,   5.04000000e+01,
         5.04000000e+01,   5.04000000e+01,   5.04000000e+01,
         5.04000000e+01,   5.04000000e+01,   5.04000000e+01,
         0.00000000e+00,   4.01841942e-01,   2.73389356e-03,
                    nan,   3.65245900e-02,   3.88950824e-02,
         3.61848799e-02,   3.53444315e-02,   4.16956522e-02,
         3.74724573e-02,   9.60939783e-01,              nan,
         3.77573576e-02,   3.71958962e-02,   3.61952862e-02,
         3.92555261e-02,   3.90873016e-02,   9.64958714e-01,
         9.63909309e-01,              nan,   3.50312319e-02,
         3.65465314e-02,   3.81196764e-02,   3.94355547e-02,
         9.62054867e-01,   9.61022963e-01,   9.60411141e-01,
                    nan,   3.80264550e-02,   3.71941392e-02,
         3.72436357e-02,   9.61308081e-01,   9.60489510e-01,
         9.62899471e-01,   9.64139194e-01,              nan,
         3.82845116e-02,   3.63882305e-02,   9.66625562e-01,
         9.64761675e-01,   9.64124363e-01,   9.63268283e-01,
         9.63959528e-01,              nan,   3.41394238e-02,
         9.63621967e-01,   9.62583809e-01,   9.64307692e-01,
         9.63341544e-01,   9.60111625e-01,   9.59809376e-01,
                    nan,   0.00000000e+00,   2.60000000e+01,
         2.38000000e+01,   2.58000000e+01,   2.64000000e+01,
         2.22000000e+01,   2.48000000e+01,   2.44000000e+01,
         0.00000000e+00,   2.46000000e+01,   2.60000000e+01,
         2.64000000e+01,   2.38000000e+01,   2.48000000e+01,
         2.66000000e+01,   2.58000000e+01,   0.00000000e+00,
         2.68000000e+01,   2.54000000e+01,   2.44000000e+01,
         2.40000000e+01,   2.46000000e+01,   2.44000000e+01,
         2.36000000e+01,   0.00000000e+00,   2.44000000e+01,
         2.50000000e+01,   2.50000000e+01,   2.40000000e+01,
         2.40000000e+01,   2.50000000e+01,   2.60000000e+01,
         0.00000000e+00,   2.44000000e+01,   2.64000000e+01,
         2.82000000e+01,   2.66000000e+01,   2.60000000e+01,
         2.54000000e+01,   2.60000000e+01,   0.00000000e+00,
         2.74000000e+01,   2.56000000e+01,   2.56000000e+01,
         2.64000000e+01,   2.54000000e+01,   2.40000000e+01,
         2.30000000e+01,   0.00000000e+00,   1.89156580e-01]), array([  1.49464763e-02,   0.00000000e+00,   0.00000000e+00,
         4.44674758e-02,   7.56566436e-02,   5.03500091e-02,
         3.34807352e-02,   3.29538724e-02,   2.68214653e-02,
         0.00000000e+00,   4.41474660e-02,   7.56566436e-02,
         5.03500091e-02,   3.34807352e-02,   3.29538724e-02,
         2.68214653e-02,   0.00000000e+00,   4.89897949e-01,
         4.89897949e-01,   4.89897949e-01,   4.89897949e-01,
         4.89897949e-01,   4.89897949e-01,   4.89897949e-01,
         0.00000000e+00,   8.53244067e-02,   5.37091764e-02,
                    nan,   5.52365819e-03,   2.37849157e-03,
         2.67084326e-03,   2.17218281e-03,   3.92217105e-03,
         2.48148061e-03,   6.94165204e-03,              nan,
         2.49686123e-03,   8.21762528e-03,   6.33844760e-03,
         4.61417041e-03,   9.07832960e-03,   1.60917085e-03,
         2.07711262e-03,              nan,   3.29397886e-03,
         1.34909114e-03,   3.09713552e-03,   6.81060348e-03,
         3.87311936e-03,   6.13800411e-03,   4.71501102e-03,
                    nan,   2.40071749e-03,   2.37407855e-03,
         2.66318311e-03,   3.00429309e-03,   6.34616864e-03,
         1.56578002e-03,   2.29457286e-03,              nan,
         3.93478932e-03,   6.75563408e-03,   3.01006063e-03,
         3.04227234e-03,   2.38439711e-03,   2.99960005e-03,
         3.50933915e-03,              nan,   1.99569801e-03,
         2.31730355e-03,   6.49732756e-03,   3.92256899e-03,
         2.54912557e-03,   7.63201034e-03,   2.85364299e-03,
                    nan,   0.00000000e+00,   4.14728827e+00,
         1.46969385e+00,   2.22710575e+00,   1.74355958e+00,
         2.31516738e+00,   1.72046505e+00,   4.54312668e+00,
         0.00000000e+00,   1.74355958e+00,   5.09901951e+00,
         4.45421149e+00,   2.78567766e+00,   5.30659966e+00,
         1.35646600e+00,   1.60000000e+00,   0.00000000e+00,
         2.71293199e+00,   1.01980390e+00,   2.05912603e+00,
         3.74165739e+00,   2.41660919e+00,   4.75815090e+00,
         2.87054002e+00,   0.00000000e+00,   1.62480768e+00,
         1.78885438e+00,   2.09761770e+00,   2.00000000e+00,
         4.33589668e+00,   1.09544512e+00,   1.78885438e+00,
         0.00000000e+00,   2.72763634e+00,   5.00399840e+00,
         2.63818119e+00,   2.57681975e+00,   1.89736660e+00,
         2.15406592e+00,   2.60768096e+00,   0.00000000e+00,
         1.85472370e+00,   1.74355958e+00,   5.08330601e+00,
         3.49857114e+00,   1.74355958e+00,   4.89897949e+00,
         1.67332005e+00,   0.00000000e+00,   5.37059537e-03])]}
SMPrank.py:298: RuntimeWarning: invalid value encountered in double_scalars
  self.probfeature[k] = self.probfeature[k] / sum_probf
/home/jasonzhang/anaconda2/lib/python2.7/site-packages/numpy/ma/core.py:867: RuntimeWarning: invalid value encountered in less_equal
  return umath.less_equal(x, self.critical_value)
dataset, N,d,L,K bodyfat 252 7 7 5
N, d, L:  181 7 7
loss_valid 0 1.61692357103
loss_train 0 1.53756731065
loss_valid 1 1.5085902621
loss_train 1 1.4195968612
loss_valid 2 1.43019940802
loss_train 2 1.33489015376
loss_valid 3 1.37089569016
loss_train 3 1.2719979911
loss_valid 4 1.32548865751
loss_train 4 1.22377474173
loss_valid 5 1.29015931822
loss_train 5 1.18539940964
loss_valid 6 1.26306761381
loss_train 6 1.15411769861
loss_valid 7 1.24187021513
loss_train 7 1.12821660741
loss_valid 8 1.22541723122
loss_train 8 1.10652177388
loss_valid 9 1.21223757796
loss_train 9 1.08826994113
loss_valid 10 1.20148349645
loss_train 10 1.07280239816
loss_valid 11 1.19278366476
loss_train 11 1.05944335509
loss_valid 12 1.18556192067
loss_train 12 1.04788955219
loss_valid 13 1.17946245199
loss_train 13 1.03782263301
loss_valid 14 1.17403403362
loss_train 14 1.02902933157
loss_valid 15 1.16901311316
loss_train 15 1.02130632008
loss_valid 16 1.16437246832
loss_train 16 1.01451928336
loss_valid 17 1.16007865363
loss_train 17 1.00845831871
loss_valid 18 1.15625490122
loss_train 18 1.00303211364
loss_valid 19 1.15267933154
loss_train 19 0.99820042161
loss_valid 20 1.14949544017
loss_train 20 0.993805094124
loss_valid 21 1.14677699087
loss_train 21 0.98978586666
loss_valid 22 1.14407331249
loss_train 22 0.986123519218
loss_valid 23 1.14172006085
loss_train 23 0.982728103404
loss_valid 24 1.13967788343
loss_train 24 0.979568476537
loss_valid 25 1.13801548439
loss_train 25 nan
nan train loss
early stop at epoch 25
[0.0196078431372549, 0, 0, 0.84313725490196079, 0.60784313725490191, 0.84313725490196079, 0.45098039215686275, 0.47058823529411764, 0.47058823529411764, 0.52941176470588236, 0.8431372549019608, 0.6078431372549019, 0.8431372549019608, 0.45098039215686275, 0.47058823529411764, 0.47058823529411764, 0.5294117647058824, 51.0, 51.0, 51.0, 51.0, 51.0, 51.0, 51.0, 0, 0.58283540051853211, 0.14285714285714285, nan, 0.9090909090909091, 0.37037037037037035, 0.8571428571428571, 0.8076923076923077, 0.875, 0.9285714285714286, 0.18181818181818182, nan, 0.1724137931034483, 0.7575757575757576, 0.8148148148148148, 0.8275862068965517, 0.8125, 0.75, 0.8461538461538461, nan, 0.8518518518518519, 0.7857142857142857, 0.8846153846153846, 0.8666666666666667, 0.4074074074074074, 0.5, 0.39285714285714285, nan, 0.4, 0.3076923076923077, 0.45161290322580644, 0.1724137931034483, 0.17857142857142858, 0.25925925925925924, 0.8333333333333334, nan, 0.7586206896551724, 0.8235294117647058, 0.25806451612903225, 0.23076923076923078, 0.20689655172413793, 0.7586206896551724, 0.3076923076923077, nan, 0.7575757575757576, 0.3333333333333333, 0.5652173913043478, 0.08, 0.75, 0.23809523809523808, 0.36363636363636365, nan, 0, 20, 25, 26, 24, 22, 26, 31, 0, 27, 31, 25, 27, 30, 26, 24, 0, 25, 26, 24, 28, 25, 20, 26, 0, 23, 24, 29, 27, 26, 25, 28, 0, 27, 32, 29, 24, 27, 27, 24, 0, 31, 25, 21, 23, 22, 19, 20, 0, 0.4770622088528429]
N, d, L:  181 7 7
loss_valid 0 1.84021920057
loss_train 0 1.64047580451
loss_valid 1 1.70542241385
loss_train 1 1.51112844931
loss_valid 2 1.60624261841
loss_train 2 1.41591866823
loss_valid 3 1.53132068656
loss_train 3 1.34348090163
loss_valid 4 1.47319194601
loss_train 4 1.28678904861
loss_valid 5 1.42701429583
loss_train 5 1.24155752425
loss_valid 6 1.38941827842
loss_train 6 1.20505771943
loss_valid 7 1.35841229708
loss_train 7 1.17512777762
loss_valid 8 1.33145240202
loss_train 8 1.15028636518
loss_valid 9 1.30775999756
loss_train 9 1.12928540584
loss_valid 10 1.28680177521
loss_train 10 1.11130808937
loss_valid 11 1.26806355998
loss_train 11 1.09570568411
loss_valid 12 1.25125871308
loss_train 12 1.08197145064
loss_valid 13 1.23645782158
loss_train 13 1.06981655932
loss_valid 14 1.22271381855
loss_train 14 1.05892638965
loss_valid 15 1.21038247169
loss_train 15 1.04905589131
loss_valid 16 1.20004394157
loss_train 16 1.0399822823
loss_valid 17 1.19123240077
loss_train 17 1.03161747423
loss_valid 18 1.18368892585
loss_train 18 1.02380763846
loss_valid 19 1.17728876683
loss_train 19 1.01645322875
loss_valid 20 1.17195341835
loss_train 20 1.00992312131
loss_valid 21 1.168087953
loss_train 21 1.00394302132
loss_valid 22 1.16486156536
loss_train 22 0.998555226305
loss_valid 23 1.16136924921
loss_train 23 0.993425003234
loss_valid 24 1.15936236432
loss_train 24 nan
nan train loss
early stop at epoch 24
[0.0, 0, 0, 0.68627450980392157, 0.80392156862745101, 0.6470588235294118, 0.80392156862745101, 0.49019607843137253, 0.31372549019607843, 0.43137254901960786, 0.6862745098039216, 0.803921568627451, 0.6470588235294118, 0.803921568627451, 0.49019607843137253, 0.3137254901960784, 0.43137254901960786, 51.0, 51.0, 51.0, 51.0, 51.0, 51.0, 51.0, 0, 0.56785106960909992, 0.075630252100840345, nan, 0.19230769230769232, 0.34615384615384615, 0.5769230769230769, 0.7931034482758621, 0.9047619047619048, 0.8076923076923077, 0.6206896551724138, nan, 0.8846153846153846, 0.65625, 0.8484848484848485, 0.9615384615384616, 0.8214285714285714, 0.7586206896551724, 0.3103448275862069, nan, 0.41379310344827586, 0.6296296296296297, 0.7241379310344828, 0.48148148148148145, 0.41379310344827586, 0.6086956521739131, 0.6923076923076923, nan, 0.8148148148148148, 0.88, 0.7407407407407407, 0.38461538461538464, 0.36363636363636365, 0.6785714285714286, 0.39285714285714285, nan, 0.5, 0.3076923076923077, 0.11764705882352941, 0.034482758620689655, 0.38461538461538464, 0.06666666666666667, 0.5172413793103449, nan, 0.35714285714285715, 0.27586206896551724, 0.2962962962962963, 0.42857142857142855, 0.32142857142857145, 0.6896551724137931, 0.6666666666666666, nan, 0, 24, 24, 24, 27, 19, 24, 27, 0, 24, 30, 31, 24, 26, 27, 27, 0, 27, 25, 27, 25, 27, 21, 24, 0, 25, 23, 25, 24, 20, 26, 26, 0, 24, 24, 32, 27, 24, 28, 27, 0, 26, 27, 25, 26, 26, 27, 25, 0, 0.45868304285507611]
N, d, L:  182 7 7
loss_valid 0 1.47932984191
loss_train 0 1.49826386644
loss_valid 1 1.37405147958
loss_train 1 1.38913392863
loss_valid 2 1.29191782743
loss_train 2 1.30368488087
loss_valid 3 1.22791795468
loss_train 3 1.23629119802
loss_valid 4 1.17731185771
loss_train 4 1.18428655726
loss_valid 5 1.13698183318
loss_train 5 1.14373295691
loss_valid 6 1.10522444587
loss_train 6 1.11233561946
loss_valid 7 1.07966185685
loss_train 7 1.08739736505
loss_valid 8 1.05790650369
loss_train 8 1.0673032048
loss_valid 9 1.04286152094
loss_train 9 1.05105279338
loss_valid 10 1.03166706294
loss_train 10 1.03769611814
loss_valid 11 1.02263674457
loss_train 11 1.02655585773
loss_valid 12 1.0148062395
loss_train 12 1.01714447469
loss_valid 13 1.00873682512
loss_train 13 1.00895725922
loss_valid 14 1.00476773827
loss_train 14 1.00174588443
loss_valid 15 1.00149889533
loss_train 15 0.99505411247
loss_valid 16 0.998917289018
loss_train 16 0.989257428167
loss_valid 17 0.999263770375
loss_train 17 0.98350700154
early stop at the end of epoch:  17
[0.02, 0, 0, 0.62, 0.46000000000000002, 0.69999999999999996, 0.66000000000000003, 0.62, 0.5, 0.71999999999999997, 0.62, 0.46, 0.7, 0.66, 0.62, 0.5, 0.72, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 0, 0.60424385299363759, 0.12380952380952381, nan, 0.8888888888888888, 0.391304347826087, 0.4230769230769231, 0.6896551724137931, 0.68, 0.4074074074074074, 0.1111111111111111, nan, 0.3076923076923077, 0.38461538461538464, 0.6060606060606061, 0.6, 0.4166666666666667, 0.6129032258064516, 0.8214285714285714, nan, 0.84, 0.7241379310344828, 0.75, 0.23076923076923078, 0.7142857142857143, 0.8928571428571429, 0.3103448275862069, nan, 0.5357142857142857, 0.7692307692307693, 0.4230769230769231, 0.44, 0.3333333333333333, 0.48, 0.5769230769230769, nan, 0.43478260869565216, 0.5238095238095238, 0.4482758620689655, 0.3448275862068966, 0.5, 0.21428571428571427, 0.6451612903225806, nan, 0.4827586206896552, 0.8888888888888888, 0.8666666666666667, 0.8214285714285714, 0.6071428571428571, 0.6363636363636364, 0.4, nan, 0, 25, 21, 24, 27, 23, 25, 25, 0, 24, 24, 31, 23, 22, 29, 26, 0, 23, 27, 26, 24, 26, 26, 27, 0, 26, 24, 24, 23, 19, 23, 24, 0, 21, 19, 27, 27, 24, 26, 29, 0, 27, 25, 28, 26, 26, 31, 23, 0, 0.50892580814682342]
N, d, L:  182 7 7
loss_valid 0 1.54595608212
loss_train 0 1.54514947063
loss_valid 1 1.40234913149
loss_train 1 1.41513528749
loss_valid 2 1.30334114102
loss_train 2 1.32086349896
loss_valid 3 1.23831403519
loss_train 3 1.25238731585
loss_valid 4 1.19170803944
loss_train 4 1.20114868038
loss_valid 5 1.15771313054
loss_train 5 1.1628588584
loss_valid 6 1.13200954704
loss_train 6 1.13375802851
loss_valid 7 1.11202712336
loss_train 7 1.11136364787
loss_valid 8 1.09658527976
loss_train 8 1.09394546807
loss_valid 9 1.08448735567
loss_train 9 1.08015112763
loss_valid 10 1.07492311379
loss_train 10 1.06878983246
loss_valid 11 1.0672276549
loss_train 11 1.05928546278
loss_valid 12 1.06119517122
loss_train 12 1.0510001394
loss_valid 13 1.05701385165
loss_train 13 1.04357242656
loss_valid 14 1.05309954662
loss_train 14 1.03685290076
loss_valid 15 1.05030713842
loss_train 15 1.03053743965
loss_valid 16 1.04945816447
loss_train 16 1.02461591427
loss_valid 17 1.0496594798
loss_train 17 1.01909422214
early stop at the end of epoch:  17
[0.06, 0, 0, 0.64000000000000001, 0.56000000000000005, 0.38, 0.62, 0.62, 0.76000000000000001, 0.62, 0.64, 0.56, 0.38, 0.62, 0.62, 0.76, 0.62, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 0, 0.58925835793812831, 0.23238095238095238, nan, 0.8064516129032258, 0.8518518518518519, 0.78125, 0.5483870967741935, 0.42857142857142855, 0.41379310344827586, 0.391304347826087, nan, 0.8571428571428571, 0.9, 0.5357142857142857, 0.4642857142857143, 0.375, 0.25925925925925924, 0.2692307692307692, nan, 0.696969696969697, 0.6153846153846154, 0.5384615384615384, 0.39285714285714285, 0.7272727272727273, 0.3333333333333333, 0.5714285714285714, nan, 0.6666666666666666, 0.75, 0.5384615384615384, 0.5652173913043478, 0.5384615384615384, 0.6071428571428571, 0.6333333333333333, nan, 0.5333333333333333, 0.7647058823529411, 0.7692307692307693, 0.8076923076923077, 0.5357142857142857, 0.7692307692307693, 0.5, nan, 0.9310344827586207, 0.76, 0.7272727272727273, 0.6923076923076923, 0.8214285714285714, 0.3, 0.32, nan, 0, 29, 25, 30, 29, 26, 27, 21, 0, 26, 28, 26, 26, 30, 25, 24, 0, 31, 24, 24, 26, 20, 22, 19, 0, 22, 26, 24, 21, 24, 26, 28, 0, 28, 32, 24, 24, 26, 24, 22, 0, 27, 23, 20, 24, 26, 18, 23, 0, 0.5708875423125398]
N, d, L:  182 7 7
loss_valid 0 1.77360474055
loss_train 0 1.53957106021
loss_valid 1 1.64669303319
loss_train 1 1.39370476551
loss_valid 2 1.5580936825
loss_train 2 1.29356949675
loss_valid 3 1.49464734387
loss_train 3 1.22039944947
loss_valid 4 1.44722247535
loss_train 4 1.16473250232
loss_valid 5 1.41000850504
loss_train 5 1.1219098308
loss_valid 6 1.37912989856
loss_train 6 1.08859709529
loss_valid 7 1.35222596011
loss_train 7 1.06322746992
loss_valid 8 1.32885354357
loss_train 8 1.04353584987
loss_valid 9 1.30843143326
loss_train 9 1.02841421424
loss_valid 10 1.29135420363
loss_train 10 1.01670358231
loss_valid 11 1.27582431064
loss_train 11 1.00756949927
loss_valid 12 1.26255548156
loss_train 12 1.000310894
loss_valid 13 1.25089178649
loss_train 13 0.994453588825
loss_valid 14 1.24096874725
loss_train 14 0.989628853808
loss_valid 15 1.23242755009
loss_train 15 0.98552013841
loss_valid 16 1.22552888826
loss_train 16 0.981755577774
loss_valid 17 1.21907178442
loss_train 17 0.97798928588
loss_valid 18 1.21329750691
loss_train 18 0.973977201534
loss_valid 19 1.20879766286
loss_train 19 0.970553190703
loss_valid 20 1.20473684613
loss_train 20 0.967996792558
loss_valid 21 1.20053527298
loss_train 21 0.96595896377
loss_valid 22 1.19842737039
loss_train 22 0.964395688434
loss_valid 23 1.19469061088
loss_train 23 0.962528770459
loss_valid 24 nan
loss_train 24 0.961281300586
nan valid loss
early stop at epoch 24
[0.0, 0, 0, 0.35999999999999999, 0.68000000000000005, 0.54000000000000004, 0.57999999999999996, 0.62, 0.56000000000000005, 0.64000000000000001, 0.36, 0.68, 0.54, 0.58, 0.62, 0.56, 0.64, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 0, 0.5589099442697345, 0.11999999999999998, nan, 0.29411764705882354, 0.5384615384615384, 0.3333333333333333, 0.2962962962962963, 0.17391304347826086, 0.7083333333333334, 0.75, nan, 0.5416666666666666, 0.7368421052631579, 0.47619047619047616, 0.3333333333333333, 0.6666666666666666, 0.4642857142857143, 0.5666666666666667, nan, 0.4, 0.44/home/jasonzhang/anaconda2/lib/python2.7/site-packages/numpy/lib/nanfunctions.py:703: RuntimeWarning: Mean of empty slice
  warnings.warn("Mean of empty slice", RuntimeWarning)
/home/jasonzhang/anaconda2/lib/python2.7/site-packages/numpy/lib/nanfunctions.py:1202: RuntimeWarning: Degrees of freedom <= 0 for slice.
  warnings.warn("Degrees of freedom <= 0 for slice.", RuntimeWarning)
44444444444444, 0.4782608695652174, 0.7894736842105263, 0.7777777777777778, 0.5142857142857142, 0.875, nan, 0.32142857142857145, 0.4666666666666667, 0.6, 0.7777777777777778, 0.7575757575757576, 0.4444444444444444, 0.46153846153846156, nan, 0.375, 0.5925925925925926, 0.9354838709677419, 0.6666666666666666, 0.45161290322580644, 0.625, 0.5333333333333333, nan, 0.6428571428571429, 0.5666666666666667, 0.6111111111111112, 0.5428571428571428, 0.6206896551724138, 0.5925925925925926, 0.6923076923076923, nan, 0, 32, 24, 25, 25, 21, 22, 18, 0, 22, 17, 19, 19, 16, 26, 28, 0, 28, 25, 21, 17, 25, 33, 22, 0, 26, 28, 23, 25, 31, 25, 24, 0, 22, 25, 29, 31, 29, 22, 28, 0, 26, 28, 34, 33, 27, 25, 24, 0, 0.53003387885314557]
{'perf': [array([  1.99215686e-02,   0.00000000e+00,   0.00000000e+00,
         6.29882353e-01,   6.22352941e-01,   6.22039216e-01,
         6.22980392e-01,   5.64156863e-01,   5.20862745e-01,
         5.88156863e-01,   6.29882353e-01,   6.22352941e-01,
         6.22039216e-01,   6.22980392e-01,   5.64156863e-01,
         5.20862745e-01,   5.88156863e-01,   5.04000000e+01,
         5.04000000e+01,   5.04000000e+01,   5.04000000e+01,
         5.04000000e+01,   5.04000000e+01,   5.04000000e+01,
         0.00000000e+00,   5.80619725e-01,   1.38935574e-01,
                    nan,   6.18171350e-01,   4.99628391e-01,
         5.94345238e-01,   6.27026864e-01,   6.12449275e-01,
         6.53159516e-01,   4.10984659e-01,              nan,
         5.52706202e-01,   6.87056649e-01,   6.56253006e-01,
         6.37348743e-01,   6.18452381e-01,   5.69013778e-01,
         5.62764936e-01,              nan,   6.40522930e-01,
         6.39862181e-01,   6.75095145e-01,   5.52249641e-01,
         6.08107346e-01,   5.69834369e-01,   5.68387647e-01,
                    nan,   5.47724868e-01,   6.34717949e-01,
         5.50778421e-01,   4.68004869e-01,   4.34315684e-01,
         4.93883598e-01,   5.79597070e-01,              nan,
         5.20347326e-01,   6.02465944e-01,   5.05740415e-01,
         4.16887710e-01,   4.15767825e-01,   4.86760768e-01,
         5.00685662e-01,              nan,   6.34273772e-01,
         5.64950192e-01,   6.13312839e-01,   5.13032967e-01,
         6.24137931e-01,   4.91341328e-01,   4.88522145e-01,
                    nan,   0.00000000e+00,   2.60000000e+01,
         2.38000000e+01,   2.58000000e+01,   2.64000000e+01,
         2.22000000e+01,   2.48000000e+01,   2.44000000e+01,
         0.00000000e+00,   2.46000000e+01,   2.60000000e+01,
         2.64000000e+01,   2.38000000e+01,   2.48000000e+01,
         2.66000000e+01,   2.58000000e+01,   0.00000000e+00,
         2.68000000e+01,   2.54000000e+01,   2.44000000e+01,
         2.40000000e+01,   2.46000000e+01,   2.44000000e+01,
         2.36000000e+01,   0.00000000e+00,   2.44000000e+01,
         2.50000000e+01,   2.50000000e+01,   2.40000000e+01,
         2.40000000e+01,   2.50000000e+01,   2.60000000e+01,
         0.00000000e+00,   2.44000000e+01,   2.64000000e+01,
         2.82000000e+01,   2.66000000e+01,   2.60000000e+01,
         2.54000000e+01,   2.60000000e+01,   0.00000000e+00,
         2.74000000e+01,   2.56000000e+01,   2.56000000e+01,
         2.64000000e+01,   2.54000000e+01,   2.40000000e+01,
         2.30000000e+01,   0.00000000e+00,   5.09118496e-01]), array([ 0.02190946,  0.        ,  0.        ,  0.15600913,  0.11553982,
        0.15550024,  0.11449922,  0.06867409,  0.14462847,  0.09911268,
        0.15600913,  0.11553982,  0.15550024,  0.11449922,  0.06867409,
        0.14462847,  0.09911268,  0.48989795,  0.48989795,  0.48989795,
        0.48989795,  0.48989795,  0.48989795,  0.48989795,  0.        ,
        0.01594984,  0.05165721,         nan,  0.30975548,  0.18844922,
        0.20087785,  0.1895841 ,  0.27751315,  0.20998411,  0.2456568 ,
               nan,  0.28552456,  0.17042076,  0.14937624,  0.23024351,
        0.19034966,  0.18844039,  0.24376611,         nan,  0.1984414 ,
        0.11598895,  0.14786963,  0.24037349,  0.16266578,  0.18425479,
        0.20335861,         nan,  0.1781354 ,  0.21294785,  0.11384342,
        0.20026068,  0.19796038,  0.14465325,  0.15237222,         nan,
        0.13099925,  0.18350295,  0.30638818,  0.28326103,  0.11607305,
        0.29104258,  0.10905052,         nan,  0.20154521,  0.23672852,
        0.18973272,  0.2540819 ,  0.17128837,  0.18512523,  0.15817482,
               nan,  0.        ,  4.14728827,  1.46969385,  2.22710575,
        1.74355958,  2.31516738,  1.72046505,  4.54312668,  0.        ,
        1.74355958,  5.09901951,  4.45421149,  2.78567766,  5.30659966,
        1.356466  ,  1.6       ,  0.        ,  2.71293199,  1.0198039 ,
        2.05912603,  3.74165739,  2.41660919,  4.7581509 ,  2.87054002,
        0.        ,  1.62480768,  1.78885438,  2.0976177 ,  2.        ,
        4.33589668,  1.09544512,  1.78885438,  0.        ,  2.72763634,
        5.0039984 ,  2.63818119,  2.57681975,  1.8973666 ,  2.15406592,
        2.60768096,  0.        ,  1.8547237 ,  1.74355958,  5.08330601,
        3.49857114,  1.74355958,  4.89897949,  1.67332005,  0.        ,
        0.03955817])]}
