/home/jasonzhang/anaconda2/lib/python2.7/site-packages/numpy/ma/core.py:867: RuntimeWarning: invalid value encountered in less_equal
  return umath.less_equal(x, self.critical_value)
N, d, L:  181 7 7
loss_valid 0 1.74865429218
loss_train 0 1.72344418268
loss_valid 1 1.75032981592
loss_train 1 1.71261708092
early stop at the end of epoch:  1
[0.0, 0, 0, 0.70588235294117652, 0.37254901960784315, 0.52941176470588236, 0.52941176470588236, 0.49019607843137253, 0.62745098039215685, 0.74509803921568629, 0.7058823529411765, 0.37254901960784315, 0.5294117647058824, 0.5294117647058824, 0.49019607843137253, 0.6274509803921569, 0.7450980392156863, 51.0, 51.0, 51.0, 51.0, 51.0, 51.0, 51.0, 0, 0.55822518440819291, 0.053221288515406168, nan, 0.8181818181818182, 0.4444444444444444, 0.7142857142857143, 0.6923076923076923, 0.4166666666666667, 0.6071428571428571, 0.15151515151515152, nan, 0.3103448275862069, 0.42424242424242425, 0.6296296296296297, 0.4482758620689655, 0.46875, 0.42857142857142855, 0.5, nan, 0.6296296296296297, 0.5, 0.38461538461538464, 0.5666666666666667, 0.48148148148148145, 0.5909090909090909, 0.8214285714285714, nan, 0.36, 0.4230769230769231, 0.45161290322580644, 0.4482758620689655, 0.4642857142857143, 0.5185185185185185, 0.4666666666666667, nan, 0.5517241379310345, 0.5588235294117647, 0.5161290322580645, 0.5384615384615384, 0.5862068965517241, 0.5517241379310345, 0.6923076923076923, nan, 0.5454545454545454, 0.6666666666666666, 0.6521739130434783, 0.72, 0.5416666666666666, 0.42857142857142855, 0.5, nan, 0, 20, 25, 26, 24, 22, 26, 31, 0, 27, 31, 25, 27, 30, 26, 24, 0, 25, 26, 24, 28, 25, 20, 26, 0, 23, 24, 29, 27, 26, 25, 28, 0, 27, 32, 29, 24, 27, 27, 24, 0, 31, 25, 21, 23, 22, 19, 20, 0, 0.51048086056169306]
N, d, L:  181 7 7
loss_valid 0 1.73384048207
loss_train 0 1.57399469731
loss_valid 1 1.7180548862
loss_train 1 1.51794714812
loss_valid 2 1.7041857045
loss_train 2 1.45850834355
loss_valid 3 1.69289255203
loss_train 3 1.39653199691
loss_valid 4 1.68442101835
loss_train 4 1.33364245589
loss_valid 5 1.67855776567
loss_train 5 1.27190113904
loss_valid 6 1.67495264136
loss_train 6 1.2133031845
loss_valid 7 1.6732399049
loss_train 7 1.15926122199
loss_valid 8 1.67311464933
loss_train 8 1.11052298728
loss_valid 9 1.67427061279
loss_train 9 1.06722194322
early stop at the end of epoch:  9
[0.0, 0, 0, 0.47058823529411764, 0.5490196078431373, 0.70588235294117652, 0.58823529411764708, 0.5490196078431373, 0.5490196078431373, 0.58823529411764708, 0.47058823529411764, 0.5490196078431373, 0.7058823529411765, 0.5882352941176471, 0.5490196078431373, 0.5490196078431373, 0.5882352941176471, 51.0, 51.0, 51.0, 51.0, 51.0, 51.0, 51.0, 0, 0.56777598079866565, 0.15966386554621848, nan, 0.5384615384615384, 0.23076923076923078, 0.5769230769230769, 0.5517241379310345, 0.47619047619047616, 0.46153846153846156, 0.7586206896551724, nan, 0.6923076923076923, 0.59375, 0.6666666666666666, 0.5769230769230769, 0.5714285714285714, 0.6896551724137931, 0.6896551724137931, nan, 0.6896551724137931, 0.6666666666666666, 0.5862068965517241, 0.5185185185185185, 0.7586206896551724, 0.6521739130434783, 0.4230769230769231, nan, 0.6666666666666666, 0.48, 0.6296296296296297, 0.5384615384615384, 0.45454545454545453, 0.4642857142857143, 0.5, nan, 0.5, 0.5769230769230769, 0.6764705882352942, 0.5517241379310345, 0.4230769230769231, 0.5, 0.6551724137931034, nan, 0.4642857142857143, 0.6896551724137931, 0.6666666666666666, 0.5, 0.42857142857142855, 0.6896551724137931, 0.5185185185185185, nan, 0, 24, 24, 24, 27, 19, 24, 27, 0, 24, 30, 31, 24, 26, 27, 27, 0, 27, 25, 27, 25, 27, 21, 24, 0, 25, 23, 25, 24, 20, 26, 26, 0, 24, 24, 32, 27, 24, 28, 27, 0, 26, 27, 25, 26, 26, 27, 25, 0, 0.55818694382353451]
N, d, L:  182 7 7
loss_valid 0 1.76104030481
loss_train 0 1.7321415245
loss_valid 1 1.75351834523
loss_train 1 1.72037975988
loss_valid 2 1.74743918074
loss_train 2 1.70980668548
loss_valid 3 1.74312236894
loss_train 3 1.70107840528
loss_valid 4 1.74084566178
loss_train 4 1.69435164899
loss_valid 5 1.740738776
loss_train 5 1.68916490383
loss_valid 6 1.7427460342
loss_train 6 1.68476300534
early stop at the end of epoch:  6
[0.0, 0, 0, 0.57999999999999996, 0.5, 0.66000000000000003, 0.56000000000000005, 0.52000000000000002, 0.69999999999999996, 0.56000000000000005, 0.58, 0.5, 0.66, 0.56, 0.52, 0.7, 0.56, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 0, 0.57912751050426736, 0.10095238095238095, nan, 0.5555555555555556, 0.30434782608695654, 0.6153846153846154, 0.7931034482758621, 0.4, 0.6666666666666666, 0.3333333333333333, nan, 0.5, 0.6538461538461539, 0.6060606060606061, 0.48, 0.6666666666666666, 0.6451612903225806, 0.6071428571428571, nan, 0.68, 0.7586206896551724, 0.6071428571428571, 0.5769230769230769, 0.5714285714285714, 0.42857142857142855, 0.2413793103448276, nan, 0.6071428571428571, 0.5769230769230769, 0.6153846153846154, 0.36, 0.3333333333333333, 0.4, 0.4230769230769231, nan, 0.2608695652173913, 0.7142857142857143, 0.5862068965517241, 0.6551724137931034, 0.5, 0.6785714285714286, 0.8709677419354839, nan, 0.7241379310344828, 0.5555555555555556, 0.4666666666666667, 0.35714285714285715, 0.4642857142857143, 0.5151515151515151, 0.4, nan, 0, 25, 21, 24, 27, 23, 25, 25, 0, 24, 24, 31, 23, 22, 29, 26, 0, 23, 27, 26, 24, 26, 26, 27, 0, 26, 24, 24, 23, 19, 23, 24, 0, 21, 19, 27, 27, 24, 26, 29, 0, 27, 25, 28, 26, 26, 31, 23, 0, 0.51973331778940957]
N, d, L:  182 7 7
loss_valid 0 1.67680200904
loss_train 0 1.61894538567
loss_valid 1 1.67557549839
loss_train 1 1.57055474838
loss_valid 2 1.68287475889
loss_train 2 1.517894385
early stop at the end of epoch:  2
[0.02, 0, 0, 0.52000000000000002, 0.38, 0.40000000000000002, 0.62, 0.54000000000000004, 0.85999999999999999, 0.83999999999999997, 0.52, 0.38, 0.4, 0.62, 0.54, 0.86, 0.84, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 0, 0.56818159257810652, 0.15238095238095239, nan, 0.7741935483870968, 0.5185185185185185, 0.78125, 0.5161290322580645, 0.14285714285714285, 0.3448275862068966, 0.2608695652173913, nan, 0.6428571428571429, 0.6333333333333333, 0.4642857142857143, 0.07142857142857142, 0.25, 0.4444444444444444, 0.6153846153846154, nan, 0.48484848484848486, 0.38461538461538464, 0.07692307692307693, 0.21428571428571427, 0.8181818181818182, 0.6666666666666666, 0.6666666666666666, nan, 0.7083333333333334, 0.42857142857142855, 0.5384615384615384, 0.6521739130434783, 0.5384615384615384, 0.7142857142857143, 0.7333333333333333, nan, 0.16666666666666666, 0.4117647058823529, 0.9615384615384616, 0.9615384615384616, 0.9285714285714286, 0.7692307692307693, 0.8333333333333334, nan, 0.6206896551724138, 0.8, 0.8181818181818182, 0.8461538461538461, 0.75, 0.75, 0.6, nan, 0, 29, 25, 30, 29, 26, 27, 21, 0, 26, 28, 26, 26, 30, 25, 24, 0, 31, 24, 24, 26, 20, 22, 19, 0, 22, 26, 24, 21, 24, 26, 28, 0, 28, 32, 24, 24, 26, 24, 22, 0, 27, 23, 20, 24, 26, 18, 23, 0, 0.502604675820155]
N, d, L:  182 7 7
loss_valid 0 1.74399346848
loss_train 0 1.73234142052
loss_valid 1 1.73965893657
loss_train 1 1.72301903432
loss_valid 2 1.73673231452
loss_train 2 1.71549113017
loss_valid 3 1.73553127403
loss_train 3 1.70983758513
loss_valid 4 1.73626430479
loss_train 4 1.70592148689
early stop at the end of epoch:  4
[0.02, 0, 0, 0.64000000000000001, 0.85999999999999999, 0.68000000000000005, 0.44, 0.69999999999999996, 0.46000000000000002, 0.47999999999999998, 0.64, 0.86, 0.68, 0.44, 0.7, 0.46, 0.48, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 0, 0.59189975539614581, 0.051428571428571414, nan, 0.5294117647058824, 0.4230769230769231, 0.5555555555555556, 0.6296296296296297, 0.5652173913043478, 0.5416666666666666, 0.6, nan, 0.5416666666666666, 0.8421052631578947, 0.8095238095238095, 0.8095238095238095, 0.6666666666666666, 0.7142857142857143, 0.4666666666666667, nan, 0.6666666666666666, 0.5185185185185185, 0.6956521739130435, 0.5789473684210527, 0.25925925925925924, 0.22857142857142856, 0.16666666666666666, nan, 0.32142857142857145, 0.5333333333333333, 0.52, 0.5925925925925926, 0.42424242424242425, 0.5555555555555556, 0.7307692307692307, nan, 0.5833333333333334, 0.7407407407407407, 0.5483870967741935, 0.30303030303030304, 0.4838709677419355, 0.5416666666666666, 0.3333333333333333, nan, 0.6428571428571429, 0.5666666666666667, 0.3888888888888889, 0.5714285714285714, 0.5517241379310345, 0.37037037037037035, 0.38461538461538464, nan, 0, 32, 24, 25, 25, 21, 22, 18, 0, 22, 17, 19, 19, 16, 26, /home/jasonzhang/anaconda2/lib/python2.7/site-packages/numpy/lib/nanfunctions.py:703: RuntimeWarning: Mean of empty slice
  warnings.warn("Mean of empty slice", RuntimeWarning)
/home/jasonzhang/anaconda2/lib/python2.7/site-packages/numpy/lib/nanfunctions.py:1202: RuntimeWarning: Degrees of freedom <= 0 for slice.
  warnings.warn("Degrees of freedom <= 0 for slice.", RuntimeWarning)
28, 0, 28, 25, 21, 17, 25, 33, 22, 0, 26, 28, 23, 25, 31, 25, 24, 0, 22, 25, 29, 31, 29, 22, 28, 0, 26, 28, 34, 33, 27, 25, 24, 0, 0.50890814136713147]
{'perf': [array([  8.00000000e-03,   0.00000000e+00,   0.00000000e+00,
         5.83294118e-01,   5.32313725e-01,   5.95058824e-01,
         5.47529412e-01,   5.59843137e-01,   6.39294118e-01,
         6.42666667e-01,   5.83294118e-01,   5.32313725e-01,
         5.95058824e-01,   5.47529412e-01,   5.59843137e-01,
         6.39294118e-01,   6.42666667e-01,   5.04000000e+01,
         5.04000000e+01,   5.04000000e+01,   5.04000000e+01,
         5.04000000e+01,   5.04000000e+01,   5.04000000e+01,
         0.00000000e+00,   5.73042005e-01,   1.03529412e-01,
                    nan,   6.43160845e-01,   3.84231389e-01,
         6.48679792e-01,   6.36578788e-01,   4.00186335e-01,
         5.24368448e-01,   4.20867748e-01,              nan,
         5.37435266e-01,   6.29455435e-01,   6.35233285e-01,
         4.77230264e-01,   5.24702381e-01,   5.84423610e-01,
         5.75769862e-01,              nan,   6.30159991e-01,
         5.65684252e-01,   4.70108078e-01,   4.91068269e-01,
         5.77794364e-01,   5.13378506e-01,   4.63843628e-01,
                    nan,   5.32714286e-01,   4.88380952e-01,
         5.51017737e-01,   5.18300781e-01,   4.42973693e-01,
         5.30529101e-01,   5.70769231e-01,              nan,
         4.12518741e-01,   6.00507553e-01,   6.57746415e-01,
         6.01985371e-01,   5.84345243e-01,   6.08238600e-01,
         6.77022903e-01,              nan,   5.99484998e-01,
         6.55708812e-01,   5.98515591e-01,   5.98945055e-01,
         5.47249589e-01,   5.50749697e-01,   4.80626781e-01,
                    nan,   0.00000000e+00,   2.60000000e+01,
         2.38000000e+01,   2.58000000e+01,   2.64000000e+01,
         2.22000000e+01,   2.48000000e+01,   2.44000000e+01,
         0.00000000e+00,   2.46000000e+01,   2.60000000e+01,
         2.64000000e+01,   2.38000000e+01,   2.48000000e+01,
         2.66000000e+01,   2.58000000e+01,   0.00000000e+00,
         2.68000000e+01,   2.54000000e+01,   2.44000000e+01,
         2.40000000e+01,   2.46000000e+01,   2.44000000e+01,
         2.36000000e+01,   0.00000000e+00,   2.44000000e+01,
         2.50000000e+01,   2.50000000e+01,   2.40000000e+01,
         2.40000000e+01,   2.50000000e+01,   2.60000000e+01,
         0.00000000e+00,   2.44000000e+01,   2.64000000e+01,
         2.82000000e+01,   2.66000000e+01,   2.60000000e+01,
         2.54000000e+01,   2.60000000e+01,   0.00000000e+00,
         2.74000000e+01,   2.56000000e+01,   2.56000000e+01,
         2.64000000e+01,   2.54000000e+01,   2.40000000e+01,
         2.30000000e+01,   0.00000000e+00,   5.19982788e-01]), array([ 0.00979796,  0.        ,  0.        ,  0.08362182,  0.17744058,
        0.11495364,  0.06157157,  0.07292878,  0.13625888,  0.1309119 ,
        0.08362182,  0.17744058,  0.11495364,  0.06157157,  0.07292878,
        0.13625888,  0.1309119 ,  0.48989795,  0.48989795,  0.48989795,
        0.48989795,  0.48989795,  0.48989795,  0.48989795,  0.        ,
        0.0115199 ,  0.04645341,         nan,  0.12599793,  0.10305303,
        0.0858006 ,  0.09936038,  0.14106866,  0.11276714,  0.22448961,
               nan,  0.13273177,  0.1335793 ,  0.110879  ,  0.23911449,
        0.15561691,  0.12289099,  0.08144696,         nan,  0.07546145,
        0.13171703,  0.22137346,  0.14012209,  0.2006318 ,  0.16557761,
        0.24838302,         nan,  0.16049721,  0.05961956,  0.06530544,
        0.10374042,  0.06644984,  0.10579518,  0.13392963,         nan,
        0.16712238,  0.11876739,  0.16110434,  0.21356165,  0.17983982,
        0.10020627,  0.19020049,         nan,  0.08839374,  0.08950034,
        0.1530389 ,  0.17016584,  0.11146877,  0.14678343,  0.07972445,
               nan,  0.        ,  4.14728827,  1.46969385,  2.22710575,
        1.74355958,  2.31516738,  1.72046505,  4.54312668,  0.        ,
        1.74355958,  5.09901951,  4.45421149,  2.78567766,  5.30659966,
        1.356466  ,  1.6       ,  0.        ,  2.71293199,  1.0198039 ,
        2.05912603,  3.74165739,  2.41660919,  4.7581509 ,  2.87054002,
        0.        ,  1.62480768,  1.78885438,  2.0976177 ,  2.        ,
        4.33589668,  1.09544512,  1.78885438,  0.        ,  2.72763634,
        5.0039984 ,  2.63818119,  2.57681975,  1.8973666 ,  2.15406592,
        2.60768096,  0.        ,  1.8547237 ,  1.74355958,  5.08330601,
        3.49857114,  1.74355958,  4.89897949,  1.67332005,  0.        ,
        0.01987236])]}
SMPrank.py:290: RuntimeWarning: invalid value encountered in double_scalars
  self.probfeature[k] = self.probfeature[k] / sum_probf
dataset, N,d,L,K bodyfat 252 7 7 5
N, d, L:  181 7 7
loss_valid 0 1.61692357103
loss_train 0 1.53756731065
loss_valid 1 1.5085902621
loss_train 1 1.4195968612
loss_valid 2 1.43019940802
loss_train 2 1.33489015376
loss_valid 3 1.37089569016
loss_train 3 1.2719979911
loss_valid 4 1.32548865751
loss_train 4 1.22377474173
loss_valid 5 1.29015931822
loss_train 5 1.18539940964
loss_valid 6 1.26306761381
loss_train 6 1.15411769861
loss_valid 7 1.24187021513
loss_train 7 1.12821660741
loss_valid 8 1.22541723122
loss_train 8 1.10652177388
loss_valid 9 1.21223757796
loss_train 9 1.08826994113
loss_valid 10 1.20148349645
loss_train 10 1.07280239816
loss_valid 11 1.19278366476
loss_train 11 1.05944335509
loss_valid 12 1.18556192067
loss_train 12 1.04788955219
loss_valid 13 1.17946245199
loss_train 13 1.03782263301
loss_valid 14 1.17403403362
loss_train 14 1.02902933157
loss_valid 15 1.16901311316
loss_train 15 1.02130632008
loss_valid 16 1.16437246832
loss_train 16 1.01451928336
loss_valid 17 1.16007865363
loss_train 17 1.00845831871
loss_valid 18 1.15625490122
loss_train 18 1.00303211364
loss_valid 19 1.15267933154
loss_train 19 0.99820042161
loss_valid 20 1.14949544017
loss_train 20 0.993805094124
loss_valid 21 1.14677699087
loss_train 21 0.98978586666
loss_valid 22 1.14407331249
loss_train 22 0.986123519218
loss_valid 23 1.14172006085
loss_train 23 0.982728103404
loss_valid 24 1.13967788343
loss_train 24 0.979568476537
loss_valid 25 1.13801548439
loss_train 25 nan
loss_valid 26 nan
loss_train 26 nan
loss_valid 27 nan
loss_train 27 nan
loss_valid 28 nan
loss_train 28 nan
loss_valid 29 nan
loss_train 29 nan
loss_valid 30 nan
loss_train 30 nan
loss_valid 31 nan
loss_train 31 nan
loss_valid 32 nan
loss_train 32 nan
loss_valid 33 nan
loss_train 33 nan
loss_valid 34 nan
loss_train 34 nan
loss_valid 35 nan
loss_train 35 nan
loss_valid 36 nan
loss_train 36 nan
loss_valid 37 nan
loss_train 37 nan
loss_valid 38 nan
loss_train 38 nan
loss_valid 39 nan
loss_train 39 nan
loss_valid 40 nan
loss_train 40 nan
loss_valid 41 nan
loss_train 41 nan
loss_valid 42 nan
loss_train 42 nan
loss_valid 43 nan
loss_train 43 nan
loss_valid 44 nan
loss_train 44 nan
loss_valid 45 nan
loss_train 45 nan
loss_valid 46 nan
loss_train 46 nan
loss_valid 47 nan
loss_train 47 nan
loss_valid 48 nan
loss_train 48 nan
loss_valid 49 nan
loss_train 49 nan
loss_valid 50 nan
loss_train 50 nan
loss_valid 51 nan
loss_train 51 nan
loss_valid 52 nan
loss_train 52 nan
loss_valid 53 nan
loss_train 53 nan
loss_valid 54 nan
loss_train 54 nan
loss_valid 55 nan
loss_train 55 nan
loss_valid 56 nan
loss_train 56 nan
loss_valid 57 nan
loss_train 57 nan
loss_valid 58 nan
loss_train 58 nan
loss_valid 59 nan
loss_train 59 nan
loss_valid 60 nan
loss_train 60 nan
loss_valid 61 nan
loss_train 61 nan
loss_valid 62 nan
loss_train 62 nan
loss_valid 63 nan
loss_train 63 nan
loss_valid 64 nan
loss_train 64 nan
loss_valid 65 nan
loss_train 65 nan
loss_valid 66 nan
loss_train 66 nan
loss_valid 67 nan
loss_train 67 nan
loss_valid 68 nan
loss_train 68 nan
loss_valid 69 nan
loss_train 69 nan
loss_valid 70 nan
loss_train 70 nan
loss_valid 71 nan
loss_train 71 nan
loss_valid 72 nan
loss_train 72 nan
loss_valid 73 nan
loss_train 73 nan
loss_valid 74 nan
loss_train 74 nan
loss_valid 75 nan
loss_train 75 nan
loss_valid 76 nan
loss_train 76 nan
loss_valid 77 nan
loss_train 77 nan
loss_valid 78 nan
loss_train 78 nan
loss_valid 79 nan
loss_train 79 nan
loss_valid 80 nan
loss_train 80 nan
loss_valid 81 nan
loss_train 81 nan
loss_valid 82 nan
loss_train 82 nan
loss_valid 83 nan
loss_train 83 nan
loss_valid 84 nan
loss_train 84 nan
loss_valid 85 nan
loss_train 85 nan
loss_valid 86 nan
loss_train 86 nan
loss_valid 87 nan
loss_train 87 nan
loss_valid 88 nan
loss_train 88 nan
loss_valid 89 nan
loss_train 89 nan
loss_valid 90 nan
loss_train 90 nan
loss_valid 91 nan
loss_train 91 nan
loss_valid 92 nan
loss_train 92 nan
loss_valid 93 nan
loss_train 93 nan
loss_valid 94 nan
loss_train 94 nan
loss_valid 95 nan
loss_train 95 nan
loss_valid 96 nan
loss_train 96 nan
loss_valid 97 nan
loss_train 97 nan
loss_valid 98 nan
loss_t/home/jasonzhang/anaconda2/lib/python2.7/site-packages/numpy/ma/core.py:867: RuntimeWarning: invalid value encountered in less_equal
  return umath.less_equal(x, self.critical_value)
rain 98 nan
loss_valid 99 nan
loss_train 99 nan
[0.0, 0, 0, 0.11764705882352941, 0.19607843137254902, 0.49019607843137253, 0.60784313725490191, 0.6470588235294118, 0.82352941176470584, 1.0, 0.11764705882352941, 0.19607843137254902, 0.49019607843137253, 0.6078431372549019, 0.6470588235294118, 0.8235294117647058, 1.0, 51.0, 51.0, 51.0, 51.0, 51.0, 51.0, 51.0, 0, 0.448715149456205, -0.030812324929971987, nan, 0.045454545454545456, 0.037037037037037035, 0.03571428571428571, 0.038461538461538464, 0.041666666666666664, 0.03571428571428571, 0.9696969696969697, nan, 0.034482758620689655, 0.030303030303030304, 0.037037037037037035, 0.034482758620689655, 0.03125, 0.9642857142857143, 0.9615384615384616, nan, 0.037037037037037035, 0.03571428571428571, 0.038461538461538464, 0.03333333333333333, 0.9629629629629629, 0.9545454545454546, 0.9642857142857143, nan, 0.04, 0.038461538461538464, 0.03225806451612903, 0.9655172413793104, 0.9642857142857143, 0.9629629629629629, 0.9666666666666667, nan, 0.034482758620689655, 0.029411764705882353, 0.967741935483871, 0.9615384615384616, 0.9655172413793104, 0.9655172413793104, 0.9615384615384616, nan, 0.030303030303030304, 0.9629629629629629, 0.9565217391304348, 0.96, 0.9583333333333334, 0.9523809523809523, 0.9545454545454546, nan, 0, 20, 25, 26, 24, 22, 26, 31, 0, 27, 31, 25, 27, 30, 26, 24, 0, 25, 26, 24, 28, 25, 20, 26, 0, 23, 24, 29, 27, 26, 25, 28, 0, 27, 32, 29, 24, 27, 27, 24, 0, 31, 25, 21, 23, 22, 19, 20, 0, 0.18495516923296976]
N, d, L:  181 7 7
loss_valid 0 1.55615459918
loss_train 0 1.54529002654
loss_valid 1 1.4289581232
loss_train 1 1.40775514433
loss_valid 2 1.34236072177
loss_train 2 1.31104785336
loss_valid 3 1.28120842258
loss_train 3 1.24271030725
loss_valid 4 1.23670911293
loss_train 4 1.19239095872
loss_valid 5 1.20184107161
loss_train 5 1.15403797744
loss_valid 6 1.17413820929
loss_train 6 1.12364878924
loss_valid 7 1.15136058939
loss_train 7 1.09899152078
loss_valid 8 1.13136837694
loss_train 8 1.07880910712
loss_valid 9 1.11586887187
loss_train 9 1.06185445665
loss_valid 10 1.10382293183
loss_train 10 1.04744501988
loss_valid 11 1.09306721701
loss_train 11 1.03517792836
loss_valid 12 1.08369441751
loss_train 12 1.02457318429
loss_valid 13 1.07522386871
loss_train 13 1.0153926184
loss_valid 14 1.07013403893
loss_train 14 1.00693445449
loss_valid 15 1.06538396579
loss_train 15 0.999437580887
loss_valid 16 1.06106415677
loss_train 16 0.99254924927
loss_valid 17 1.05679147605
loss_train 17 0.986345457242
loss_valid 18 1.05367365626
loss_train 18 0.980869847491
loss_valid 19 1.05062061464
loss_train 19 0.97617419648
loss_valid 20 1.04750122882
loss_train 20 0.971999372207
loss_valid 21 1.04491210697
loss_train 21 0.967902273159
loss_valid 22 1.04300495055
loss_train 22 0.964561183393
loss_valid 23 nan
loss_train 23 nan
loss_valid 24 nan
loss_train 24 nan
loss_valid 25 nan
loss_train 25 nan
loss_valid 26 nan
loss_train 26 nan
loss_valid 27 nan
loss_train 27 nan
loss_valid 28 nan
loss_train 28 nan
loss_valid 29 nan
loss_train 29 nan
loss_valid 30 nan
loss_train 30 nan
loss_valid 31 nan
loss_train 31 nan
loss_valid 32 nan
loss_train 32 nan
loss_valid 33 nan
loss_train 33 nan
loss_valid 34 nan
loss_train 34 nan
loss_valid 35 nan
loss_train 35 nan
loss_valid 36 nan
loss_train 36 nan
loss_valid 37 nan
loss_train 37 nan
loss_valid 38 nan
loss_train 38 nan
loss_valid 39 nan
loss_train 39 nan
loss_valid 40 nan
loss_train 40 nan
loss_valid 41 nan
loss_train 41 nan
loss_valid 42 nan
loss_train 42 nan
loss_valid 43 nan
loss_train 43 nan
loss_valid 44 nan
loss_train 44 nan
loss_valid 45 nan
loss_train 45 nan
loss_valid 46 nan
loss_train 46 nan
loss_valid 47 nan
loss_train 47 nan
loss_valid 48 nan
loss_train 48 nan
loss_valid 49 nan
loss_train 49 nan
loss_valid 50 nan
loss_train 50 nan
loss_valid 51 nan
loss_train 51 nan
loss_valid 52 nan
loss_train 52 nan
loss_valid 53 nan
loss_train 53 nan
loss_valid 54 nan
loss_train 54 nan
loss_valid 55 nan
loss_train 55 nan
loss_valid 56 nan
loss_train 56 nan
loss_valid 57 nan
loss_train 57 nan
loss_valid 58 nan
loss_train 58 nan
loss_valid 59 nan
loss_train 59 nan
loss_valid 60 nan
loss_train 60 nan
loss_valid 61 nan
loss_train 61 nan
loss_valid 62 nan
loss_train 62 nan
loss_valid 63 nan
loss_train 63 nan
loss_valid 64 nan
loss_train 64 nan
loss_valid 65 nan
loss_train 65 nan
loss_valid 66 nan
loss_train 66 nan
loss_valid 67 nan
loss_train 67 nan
loss_valid 68 nan
loss_train 68 nan
loss_valid 69 nan
loss_train 69 nan
loss_valid 70 nan
loss_train 70 nan
loss_valid 71 nan
loss_train 71 nan
loss_valid 72 nan
loss_train 72 nan
loss_valid 73 nan
loss_train 73 nan
loss_valid 74 nan
loss_train 74 nan
loss_valid 75 nan
loss_train 75 nan
loss_valid 76 nan
loss_train 76 nan
loss_valid 77 nan
loss_train 77 nan
loss_valid 78 nan
loss_train 78 nan
loss_valid 79 nan
loss_train 79 nan
loss_valid 80 nan
loss_train 80 nan
loss_valid 81 nan
loss_train 81 nan
loss_valid 82 nan
loss_train 82 nan
loss_valid 83 nan
loss_train 83 nan
loss_valid 84 nan
loss_train 84 nan
loss_valid 85 nan
loss_train 85 nan
loss_valid 86 nan
loss_train 86 nan
loss_valid 87 nan
loss_train 87 nan
loss_valid 88 nan
loss_train 88 nan
loss_valid 89 nan
loss_train 89 nan
loss_valid 90 nan
loss_train 90 nan
loss_valid 91 nan
loss_train 91 nan
loss_valid 92 nan
loss_train 92 nan
loss_valid 93 nan
loss_train 93 nan
loss_valid 94 nan
loss_train 94 nan
loss_valid 95 nan
loss_train 95 nan
loss_valid 96 nan
loss_train 96 nan
loss_valid 97 nan
loss_train 97 nan
loss_valid 98 nan
loss_train 98 nan
loss_valid 99 nan
loss_train 99 nan
[0.0196078431372549, 0, 0, 0.058823529411764705, 0.21568627450980393, 0.37254901960784315, 0.60784313725490191, 0.74509803921568629, 0.88235294117647056, 1.0, 0.058823529411764705, 0.21568627450980393, 0.37254901960784315, 0.6078431372549019, 0.7450980392156863, 0.8823529411764706, 1.0, 51.0, 51.0, 51.0, 51.0, 51.0, 51.0, 51.0, 0, 0.40821252601407365, 0.014005602240896352, nan, 0.038461538461538464, 0.038461538461538464, 0.038461538461538464, 0.034482758620689655, 0.047619047619047616, 0.038461538461538464, 0.9655172413793104, nan, 0.038461538461538464, 0.03125, 0.030303030303030304, 0.038461538461538464, 0.03571428571428571, 0.9655172413793104, 0.9655172413793104, nan, 0.034482758620689655, 0.037037037037037035, 0.034482758620689655, 0.037037037037037035, 0.9655172413793104, 0.9565217391304348, 0.9615384615384616, nan, 0.037037037037037035, 0.04, 0.037037037037037035, 0.9615384615384616, 0.9545454545454546, 0.9642857142857143, 0.9642857142857143, nan, 0.038461538461538464, 0.038461538461538464, 0.9705882352941176, 0.9655172413793104, 0.9615384615384616, 0.9666666666666667, 0.9655172413793104, nan, 0.03571428571428571, 0.9655172413793104, 0.9629629629629629, 0.9642857142857143, 0.9642857142857143, 0.9655172413793104, 0.9629629629629629, nan, 0, 24, 24, 24, 27, 19, 24, 27, 0, 24, 30, 31, 24, 26, 27, 27, 0, 27, 25, 27, 25, 27, 21, 24, 0, 25, 23, 25, 24, 20, 26, 26, 0, 24, 24, 32, 27, 24, 28, 27, 0, 26, 27, 25, 26, 26, 27, 25, 0, 0.18881207833762206]
N, d, L:  182 7 7
loss_valid 0 1.55399801729
loss_train 0 1.54228301151
loss_valid 1 1.43284059442
loss_train 1 1.42354468499
loss_valid 2 1.3455753674
loss_train 2 1.33871078686
loss_valid 3 1.2830644131
loss_train 3 1.27671759673
loss_valid 4 1.23795211231
loss_train 4 1.23023405919
loss_valid 5 1.20444899409
loss_train 5 1.1945047178
loss_valid 6 1.1785014916
loss_train 6 1.16642808441
loss_valid 7 1.15743739586
loss_train 7 1.14383289216
loss_valid 8 1.1400304283
loss_train 8 1.12532173637
loss_valid 9 1.12500277162
loss_train 9 1.10988708352
loss_valid 10 1.11174501384
loss_train 10 1.09675092592
loss_valid 11 1.10009442565
loss_train 11 1.08556817631
loss_valid 12 1.08981480675
loss_train 12 1.07599621492
loss_valid 13 1.08091295352
loss_train 13 1.0677496516
loss_valid 14 1.07301990595
loss_train 14 1.06058678506
loss_valid 15 1.06625789343
loss_train 15 1.05428735296
loss_valid 16 1.06079651735
loss_train 16 1.04857118885
loss_valid 17 1.05613866434
loss_train 17 1.04320768995
loss_valid 18 1.05223230655
loss_train 18 1.03816768537
loss_valid 19 1.04968996658
loss_train 19 1.0333783741
loss_valid 20 1.04739194283
loss_train 20 1.02863732123
loss_valid 21 1.04657132667
loss_train 21 1.02432035506
loss_valid 22 1.04636137258
loss_train 22 1.02033210895
loss_valid 23 1.0461148329
loss_train 23 1.01664413648
loss_valid 24 1.04349760099
loss_train 24 1.01323233988
loss_valid 25 1.04178937491
loss_train 25 1.01022529438
loss_valid 26 1.04124728541
loss_train 26 1.00744005468
loss_valid 27 nan
loss_train 27 nan
loss_valid 28 nan
loss_train 28 nan
loss_valid 29 nan
loss_train 29 nan
loss_valid 30 nan
loss_train 30 nan
loss_valid 31 nan
loss_train 31 nan
loss_valid 32 nan
loss_train 32 nan
loss_valid 33 nan
loss_train 33 nan
loss_valid 34 nan
loss_train 34 nan
loss_valid 35 nan
loss_train 35 nan
loss_valid 36 nan
loss_train 36 nan
loss_valid 37 nan
loss_train 37 nan
loss_valid 38 nan
loss_train 38 nan
loss_valid 39 nan
loss_train 39 nan
loss_valid 40 nan
loss_train 40 nan
loss_valid 41 nan
loss_train 41 nan
loss_valid 42 nan
loss_train 42 nan
loss_valid 43 nan
loss_train 43 nan
loss_valid 44 nan
loss_train 44 nan
loss_valid 45 nan
loss_train 45 nan
loss_valid 46 nan
loss_train 46 nan
loss_valid 47 nan
loss_train 47 nan
loss_valid 48 nan
loss_train 48 nan
loss_valid 49 nan
loss_train 49 nan
loss_valid 50 nan
loss_train 50 nan
loss_valid 51 nan
loss_train 51 nan
loss_valid 52 nan
loss_train 52 nan
loss_valid 53 nan
loss_train 53 nan
loss_valid 54 nan
loss_train 54 nan
loss_valid 55 nan
loss_train 55 nan
loss_valid 56 nan
loss_train 56 nan
loss_valid 57 nan
loss_train 57 nan
loss_valid 58 nan
loss_train 58 nan
loss_valid 59 nan
loss_train 59 nan
loss_valid 60 nan
loss_train 60 nan
loss_valid 61 nan
loss_train 61 nan
loss_valid 62 nan
loss_train 62 nan
loss_valid 63 nan
loss_train 63 nan
loss_valid 64 nan
loss_train 64 nan
loss_valid 65 nan
loss_train 65 nan
loss_valid 66 nan
loss_train 66 nan
loss_valid 67 nan
loss_train 67 nan
loss_valid 68 nan
loss_train 68 nan
loss_valid 69 nan
loss_train 69 nan
loss_valid 70 nan
loss_train 70 nan
loss_valid 71 nan
loss_train 71 nan
loss_valid 72 nan
loss_train 72 nan
loss_valid 73 nan
loss_train 73 nan
loss_valid 74 nan
loss_train 74 nan
loss_valid 75 nan
loss_train 75 nan
loss_valid 76 nan
loss_train 76 nan
loss_valid 77 nan
loss_train 77 nan
loss_valid 78 nan
loss_train 78 nan
loss_valid 79 nan
loss_train 79 nan
loss_valid 80 nan
loss_train 80 nan
loss_valid 81 nan
loss_train 81 nan
loss_valid 82 nan
loss_train 82 nan
loss_valid 83 nan
loss_train 83 nan
loss_valid 84 nan
loss_train 84 nan
loss_valid 85 nan
loss_train 85 nan
loss_valid 86 nan
loss_train 86 nan
loss_valid 87 nan
loss_train 87 nan
loss_valid 88 nan
loss_train 88 nan
loss_valid 89 nan
loss_train 89 nan
loss_valid 90 nan
loss_train 90 nan
loss_valid 91 nan
loss_train 91 nan
loss_valid 92 nan
loss_train 92 nan
loss_valid 93 nan
loss_train 93 nan
loss_valid 94 nan
loss_train 94 nan
loss_valid 95 nan
loss_train 95 nan
loss_valid 96 nan
loss_train 96 nan
loss_valid 97 nan
loss_train 97 nan
loss_valid 98 nan
loss_train 98 nan
loss_valid 99 nan
loss_train 99 nan
[0.02, 0, 0, 0.12, 0.32000000000000001, 0.35999999999999999, 0.57999999999999996, 0.71999999999999997, 0.88, 1.0, 0.12, 0.32, 0.36, 0.58, 0.72, 0.88, 1.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 0, 0.47018430359481994, 0.028571428571428571, nan, 0.037037037037037035, 0.043478260869565216, 0.038461538461538464, 0.034482758620689655, 0.04, 0.037037037037037035, 0.9629629629629629, nan, 0.038461538461538464, 0.038461538461538464, 0.030303030303030304, 0.04, 0.041666666666666664, 0.967741935483871, 0.9642857142857143, nan, 0.04, 0.034482758620689655, 0.03571428571428571, 0.038461538461538464, 0.9642857142857143, 0.9642857142857143, 0.9655172413793104, nan, 0.03571428571428571, 0.038461538461538464, 0.038461538461538464, 0.96, 0.9523809523809523, 0.96, 0.9615384615384616, nan, 0.043478260869565216, 0.047619047619047616, 0.9655172413793104, 0.9655172413793104, 0.9615384615384616, 0.9642857142857143, 0.967741935483871, nan, 0.034482758620689655, 0.9629629629629629, 0.9666666666666667, 0.9642857142857143, 0.9642857142857143, 0.9696969696969697, 0.96, nan, 0, 25, 21, 24, 27, 23, 25, 25, 0, 24, 24, 31, 23, 22, 29, 26, 0, 23, 27, 26, 24, 26, 26, 27, 0, 26, 24, 24, 23, 19, 23, 24, 0, 21, 19, 27, 27, 24, 26, 29, 0, 27, 25, 28, 26, 26, 31, 23, 0, 0.19190290901485918]
N, d, L:  182 7 7
loss_valid 0 1.68262387972
loss_train 0 1.63494994612
loss_valid 1 1.56942035831
loss_train 1 1.52616436173
loss_valid 2 1.4780053896
loss_train 2 1.4408158912
loss_valid 3 1.40039871141
loss_train 3 1.37064214435
loss_valid 4 1.33569231987
loss_train 4 1.31076620253
loss_valid 5 1.2830508235
loss_train 5 1.25931299196
loss_valid 6 1.24129395001
loss_train 6 1.21547552547
loss_valid 7 1.20759229609
loss_train 7 1.17802073365
loss_valid 8 1.17981795391
loss_train 8 1.14581282632
loss_valid 9 1.15620567171
loss_train 9 1.11956525665
loss_valid 10 1.13385399734
loss_train 10 1.0980805433
loss_valid 11 1.11479833423
loss_train 11 1.08032276925
loss_valid 12 1.09873235162
loss_train 12 1.06517841248
loss_valid 13 1.08403044816
loss_train 13 1.0517393325
loss_valid 14 1.07248083826
loss_train 14 1.04046477178
loss_valid 15 1.06471902853
loss_train 15 1.03069787448
loss_valid 16 1.05847553701
loss_train 16 1.02232171024
loss_valid 17 1.05125995238
loss_train 17 1.01471208747
loss_valid 18 1.04363037496
loss_train 18 1.00771871245
loss_valid 19 1.03639522982
loss_train 19 1.00133081658
loss_valid 20 1.03121308934
loss_train 20 0.995106664629
loss_valid 21 1.02479678784
loss_train 21 0.989779214176
loss_valid 22 1.01944270056
loss_train 22 0.984358646654
loss_valid 23 1.01547364793
loss_train 23 0.978499243012
loss_valid 24 1.01079760952
loss_train 24 0.97376377937
loss_valid 25 nan
loss_train 25 nan
loss_valid 26 nan
loss_train 26 nan
loss_valid 27 nan
loss_train 27 nan
loss_valid 28 nan
loss_train 28 nan
loss_valid 29 nan
loss_train 29 nan
loss_valid 30 nan
loss_train 30 nan
loss_valid 31 nan
loss_train 31 nan
loss_valid 32 nan
loss_train 32 nan
loss_valid 33 nan
loss_train 33 nan
loss_valid 34 nan
loss_train 34 nan
loss_valid 35 nan
loss_train 35 nan
loss_valid 36 nan
loss_train 36 nan
loss_valid 37 nan
loss_train 37 nan
loss_valid 38 nan
loss_train 38 nan
loss_valid 39 nan
loss_train 39 nan
loss_valid 40 nan
loss_train 40 nan
loss_valid 41 nan
loss_train 41 nan
loss_valid 42 nan
loss_train 42 nan
loss_valid 43 nan
loss_train 43 nan
loss_valid 44 nan
loss_train 44 nan
loss_valid 45 nan
loss_train 45 nan
loss_valid 46 nan
loss_train 46 nan
loss_valid 47 nan
loss_train 47 nan
loss_valid 48 nan
loss_train 48 nan
loss_valid 49 nan
loss_train 49 nan
loss_valid 50 nan
loss_train 50 nan
loss_valid 51 nan
loss_train 51 nan
loss_valid 52 nan
loss_train 52 nan
loss_valid 53 nan
loss_train 53 nan
loss_valid 54 nan
loss_train 54 nan
loss_valid 55 nan
loss_train 55 nan
loss_valid 56 nan
loss_train 56 nan
loss_valid 57 nan
loss_train 57 nan
loss_valid 58 nan
loss_train 58 nan
loss_valid 59 nan
loss_train 59 nan
loss_valid 60 nan
loss_train 60 nan
loss_valid 61 nan
loss_train 61 nan
loss_valid 62 nan
loss_train 62 nan
loss_valid 63 nan
loss_train 63 nan
loss_valid 64 nan
loss_train 64 nan
loss_valid 65 nan
loss_train 65 nan
loss_valid 66 nan
loss_train 66 nan
loss_valid 67 nan
loss_train 67 nan
loss_valid 68 nan
loss_train 68 nan
loss_valid 69 nan
loss_train 69 nan
loss_valid 70 nan
loss_train 70 nan
loss_valid 71 nan
loss_train 71 nan
loss_valid 72 nan
loss_train 72 nan
loss_valid 73 nan
loss_train 73 nan
loss_valid 74 nan
loss_train 74 nan
loss_valid 75 nan
loss_train 75 nan
loss_valid 76 nan
loss_train 76 nan
loss_valid 77 nan
loss_train 77 nan
loss_valid 78 nan
loss_train 78 nan
loss_valid 79 nan
loss_train 79 nan
loss_valid 80 nan
loss_train 80 nan
loss_valid 81 nan
loss_train 81 nan
loss_valid 82 nan
loss_train 82 nan
loss_valid 83 nan
loss_train 83 nan
loss_valid 84 nan
loss_train 84 nan
loss_valid 85 nan
loss_train 85 nan
loss_valid 86 nan
loss_train 86 nan
loss_valid 87 nan
loss_train 87 nan
loss_valid 88 nan
loss_train 88 nan
loss_valid 89 nan
loss_train 89 nan
loss_valid 90 nan
loss_train 90 nan
loss_valid 91 nan
loss_train 91 nan
loss_valid 92 nan
loss_train 92 nan
loss_valid 93 nan
loss_train 93 nan
loss_valid 94 nan
loss_train 94 nan
loss_valid 95 nan
loss_train 95 nan
loss_valid 96 nan
loss_train 96 nan
loss_valid 97 nan
loss_train 97 nan
loss_valid 98 nan
loss_train 98 nan
loss_valid 99 nan
loss_train 99 nan
NONERECALL: 0.0 for emoticon: 0
[0.0, 0, 0, 0.0, 0.23999999999999999, 0.44, 0.64000000000000001, 0.69999999999999996, 0.85999999999999999, 1.0, 0.001, 0.24, 0.44, 0.64, 0.7, 0.86, 1.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 0, 0.23592675678933842, -0.078095238095238093, nan, 0.03225806451612903, 0.037037037037037035, 0.03125, 0.03225806451612903, 0.03571428571428571, 0.034482758620689655, 0.9565217391304348, nan, 0.03571428571428571, 0.03333333333333333, 0.03571428571428571, 0.03571428571428571, 0.03125, 0.9629629629629629, 0.9615384615384616, nan, 0.030303030303030304, 0.038461538461538464, 0.038461538461538464, 0.03571428571428571, 0.9545454545454546, 0.9583333333333334, 0.9523809523809523, nan, 0.041666666666666664, 0.03571428571428571, 0.038461538461538464, 0.9565217391304348, 0.9615384615384616, 0.9642857142857143, 0.9666666666666667, nan, 0.03333333333333333, 0.029411764705882353, 0.9615384615384616, 0.9615384615384616, 0.9642857142857143, 0.9615384615384616, 0.9583333333333334, nan, 0.034482758620689655, 0.96, 0.9545454545454546, 0.9615384615384616, 0.9642857142857143, 0.95, 0.96, nan, 0, 29, 25, 30, 29, 26, 27, 21, 0, 26, 28, 26, 26, 30, 25, 24, 0, 31, 24, 24, 26, 20, 22, 19, 0, 22, 26, 24, 21, 24, 26, 28, 0, 28, 32, 24, 24, 26, 24, 22, 0, 27, 23, 20, 24, 26, 18, 23, 0, 0.18239855399501095]
N, d, L:  182 7 7
loss_valid 0 1.47973324948
loss_train 0 1.50175769665
loss_valid 1 1.35934061221
loss_train 1 1.37875372844
loss_valid 2 1.2701940301
loss_train 2 1.28956531513
loss_valid 3 1.20222714432
loss_train 3 1.22407122209
loss_valid 4 1.14896262305
loss_train 4 1.17532569596
loss_valid 5 1.10771777329
loss_train 5 1.13842865528
loss_valid 6 1.07620325834
loss_train 6 1.10951334431
loss_valid 7 1.052546153
loss_train 7 1.08637175553
loss_valid 8 1.03280976124
loss_train 8 1.06735464266
loss_valid 9 1.01868190043
loss_train 9 1.05157446672
loss_valid 10 1.00749826988
loss_train 10 1.03823459824
loss_valid 11 1.00023748045
loss_train 11 1.02686274108
loss_valid 12 0.991472069453
loss_train 12 1.01713917804
loss_valid 13 0.983796232999
loss_train 13 1.00859193478
loss_valid 14 0.978769575716
loss_train 14 1.00116779597
loss_valid 15 0.973134439761
loss_train 15 0.994634308811
loss_valid 16 0.968791572749
loss_train 16 0.988935493209
loss_valid 17 0.965111776288
loss_train 17 0.983885510887
loss_valid 18 0.96173332515
loss_train 18 0.979392836407
loss_valid 19 0.958908180116
loss_train 19 0.975303216875
loss_valid 20 0.956560224661
loss_train 20 0.97154454006
loss_valid 21 0.954111251207
loss_train 21 0.968026250672
loss_valid 22 0.951788687282
loss_train 22 0.964740864357
loss_valid 23 0.949437130655
loss_train 23 0.961636524497
loss_valid 24 0.947374142978
loss_train 24 0.958788022775
loss_valid 25 0.946262449242
loss_train 25 0.956222720529
loss_valid 26 0.944494385402
loss_train 26 0.953769801741
loss_valid 27 0.94342920117
loss_train 27 0.951560515111
loss_valid 28 nan
loss_train 28 nan
loss_valid 29 nan
loss_train 29 nan
loss_valid 30 nan
loss_train 30 nan
loss_valid 31 nan
loss_train 31 nan
loss_valid 32 nan
loss_train 32 nan
loss_valid 33 nan
loss_train 33 nan
loss_valid 34 nan
loss_train 34 nan
loss_valid 35 nan
loss_train 35 nan
loss_valid 36 nan
loss_train 36 nan
loss_valid 37 nan
loss_train 37 nan
loss_valid 38 nan
loss_train 38 nan
loss_valid 39 nan
loss_train 39 nan
loss_valid 40 nan
loss_train 40 nan
loss_valid 41 nan
loss_train 41 nan
loss_valid 42 nan
loss_train 42 nan
loss_valid 43 nan
loss_train 43 nan
loss_valid 44 nan
loss_train 44 nan
loss_valid 45 nan
loss_train 45 nan
loss_valid 46 nan
loss_train 46 nan
loss_valid 47 nan
loss_train 47 nan
loss_valid 48 nan
loss_train 48 nan
loss_valid 49 nan
loss_train 49 nan
loss_valid 50 nan
loss_train 50 nan
loss_valid 51 nan
loss_train 51 nan
loss_valid 52 nan
loss_train 52 nan
loss_valid 53 nan
loss_train 53 nan
loss_valid 54 nan
loss_train 54 nan
loss_valid 55 nan
loss_train 55 nan
loss_valid 56 nan
loss_train 56 nan
loss/home/jasonzhang/anaconda2/lib/python2.7/site-packages/numpy/lib/nanfunctions.py:703: RuntimeWarning: Mean of empty slice
  warnings.warn("Mean of empty slice", RuntimeWarning)
/home/jasonzhang/anaconda2/lib/python2.7/site-packages/numpy/lib/nanfunctions.py:1202: RuntimeWarning: Degrees of freedom <= 0 for slice.
  warnings.warn("Degrees of freedom <= 0 for slice.", RuntimeWarning)
_valid 57 nan
loss_train 57 nan
loss_valid 58 nan
loss_train 58 nan
loss_valid 59 nan
loss_train 59 nan
loss_valid 60 nan
loss_train 60 nan
loss_valid 61 nan
loss_train 61 nan
loss_valid 62 nan
loss_train 62 nan
loss_valid 63 nan
loss_train 63 nan
loss_valid 64 nan
loss_train 64 nan
loss_valid 65 nan
loss_train 65 nan
loss_valid 66 nan
loss_train 66 nan
loss_valid 67 nan
loss_train 67 nan
loss_valid 68 nan
loss_train 68 nan
loss_valid 69 nan
loss_train 69 nan
loss_valid 70 nan
loss_train 70 nan
loss_valid 71 nan
loss_train 71 nan
loss_valid 72 nan
loss_train 72 nan
loss_valid 73 nan
loss_train 73 nan
loss_valid 74 nan
loss_train 74 nan
loss_valid 75 nan
loss_train 75 nan
loss_valid 76 nan
loss_train 76 nan
loss_valid 77 nan
loss_train 77 nan
loss_valid 78 nan
loss_train 78 nan
loss_valid 79 nan
loss_train 79 nan
loss_valid 80 nan
loss_train 80 nan
loss_valid 81 nan
loss_train 81 nan
loss_valid 82 nan
loss_train 82 nan
loss_valid 83 nan
loss_train 83 nan
loss_valid 84 nan
loss_train 84 nan
loss_valid 85 nan
loss_train 85 nan
loss_valid 86 nan
loss_train 86 nan
loss_valid 87 nan
loss_train 87 nan
loss_valid 88 nan
loss_train 88 nan
loss_valid 89 nan
loss_train 89 nan
loss_valid 90 nan
loss_train 90 nan
loss_valid 91 nan
loss_train 91 nan
loss_valid 92 nan
loss_train 92 nan
loss_valid 93 nan
loss_train 93 nan
loss_valid 94 nan
loss_train 94 nan
loss_valid 95 nan
loss_train 95 nan
loss_valid 96 nan
loss_train 96 nan
loss_valid 97 nan
loss_train 97 nan
loss_valid 98 nan
loss_train 98 nan
loss_valid 99 nan
loss_train 99 nan
[0.04, 0, 0, 0.059999999999999998, 0.40000000000000002, 0.46000000000000002, 0.54000000000000004, 0.71999999999999997, 0.81999999999999995, 1.0, 0.06, 0.4, 0.46, 0.54, 0.72, 0.82, 1.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 0, 0.44617097216874874, 0.080000000000000002, nan, 0.029411764705882353, 0.038461538461538464, 0.037037037037037035, 0.037037037037037035, 0.043478260869565216, 0.041666666666666664, 0.95, nan, 0.041666666666666664, 0.05263157894736842, 0.047619047619047616, 0.047619047619047616, 0.05555555555555555, 0.9642857142857143, 0.9666666666666667, nan, 0.03333333333333333, 0.037037037037037035, 0.043478260869565216, 0.05263157894736842, 0.9629629629629629, 0.9714285714285714, 0.9583333333333334, nan, 0.03571428571428571, 0.03333333333333333, 0.04, 0.9629629629629629, 0.9696969696969697, 0.9629629629629629, 0.9615384615384616, nan, 0.041666666666666664, 0.037037037037037035, 0.967741935483871, 0.9696969696969697, 0.967741935483871, 0.9583333333333334, 0.9666666666666667, nan, 0.03571428571428571, 0.9666666666666667, 0.9722222222222222, 0.9714285714285714, 0.9655172413793104, 0.9629629629629629, 0.9615384615384616, nan, 0, 32, 24, 25, 25, 21, 22, 18, 0, 22, 17, 19, 19, 16, 26, 28, 0, 28, 25, 21, 17, 25, 33, 22, 0, 26, 28, 23, 25, 31, 25, 24, 0, 22, 25, 29, 31, 29, 22, 28, 0, 26, 28, 34, 33, 27, 25, 24, 0, 0.19771419068986754]
{'perf': [array([  1.59215686e-02,   0.00000000e+00,   0.00000000e+00,
         7.12941176e-02,   2.74352941e-01,   4.24549020e-01,
         5.95137255e-01,   7.06431373e-01,   8.53176471e-01,
         1.00000000e+00,   7.14941176e-02,   2.74352941e-01,
         4.24549020e-01,   5.95137255e-01,   7.06431373e-01,
         8.53176471e-01,   1.00000000e+00,   5.04000000e+01,
         5.04000000e+01,   5.04000000e+01,   5.04000000e+01,
         5.04000000e+01,   5.04000000e+01,   5.04000000e+01,
         0.00000000e+00,   4.01841942e-01,   2.73389356e-03,
                    nan,   3.65245900e-02,   3.88950824e-02,
         3.61848799e-02,   3.53444315e-02,   4.16956522e-02,
         3.74724573e-02,   9.60939783e-01,              nan,
         3.77573576e-02,   3.71958962e-02,   3.61952862e-02,
         3.92555261e-02,   3.90873016e-02,   9.64958714e-01,
         9.63909309e-01,              nan,   3.50312319e-02,
         3.65465314e-02,   3.81196764e-02,   3.94355547e-02,
         9.62054867e-01,   9.61022963e-01,   9.60411141e-01,
                    nan,   3.80264550e-02,   3.71941392e-02,
         3.72436357e-02,   9.61308081e-01,   9.60489510e-01,
         9.62899471e-01,   9.64139194e-01,              nan,
         3.82845116e-02,   3.63882305e-02,   9.66625562e-01,
         9.64761675e-01,   9.64124363e-01,   9.63268283e-01,
         9.63959528e-01,              nan,   3.41394238e-02,
         9.63621967e-01,   9.62583809e-01,   9.64307692e-01,
         9.63341544e-01,   9.60111625e-01,   9.59809376e-01,
                    nan,   0.00000000e+00,   2.60000000e+01,
         2.38000000e+01,   2.58000000e+01,   2.64000000e+01,
         2.22000000e+01,   2.48000000e+01,   2.44000000e+01,
         0.00000000e+00,   2.46000000e+01,   2.60000000e+01,
         2.64000000e+01,   2.38000000e+01,   2.48000000e+01,
         2.66000000e+01,   2.58000000e+01,   0.00000000e+00,
         2.68000000e+01,   2.54000000e+01,   2.44000000e+01,
         2.40000000e+01,   2.46000000e+01,   2.44000000e+01,
         2.36000000e+01,   0.00000000e+00,   2.44000000e+01,
         2.50000000e+01,   2.50000000e+01,   2.40000000e+01,
         2.40000000e+01,   2.50000000e+01,   2.60000000e+01,
         0.00000000e+00,   2.44000000e+01,   2.64000000e+01,
         2.82000000e+01,   2.66000000e+01,   2.60000000e+01,
         2.54000000e+01,   2.60000000e+01,   0.00000000e+00,
         2.74000000e+01,   2.56000000e+01,   2.56000000e+01,
         2.64000000e+01,   2.54000000e+01,   2.40000000e+01,
         2.30000000e+01,   0.00000000e+00,   1.89156580e-01]), array([  1.49464763e-02,   0.00000000e+00,   0.00000000e+00,
         4.44674758e-02,   7.56566436e-02,   5.03500091e-02,
         3.34807352e-02,   3.29538724e-02,   2.68214653e-02,
         0.00000000e+00,   4.41474660e-02,   7.56566436e-02,
         5.03500091e-02,   3.34807352e-02,   3.29538724e-02,
         2.68214653e-02,   0.00000000e+00,   4.89897949e-01,
         4.89897949e-01,   4.89897949e-01,   4.89897949e-01,
         4.89897949e-01,   4.89897949e-01,   4.89897949e-01,
         0.00000000e+00,   8.53244067e-02,   5.37091764e-02,
                    nan,   5.52365819e-03,   2.37849157e-03,
         2.67084326e-03,   2.17218281e-03,   3.92217105e-03,
         2.48148061e-03,   6.94165204e-03,              nan,
         2.49686123e-03,   8.21762528e-03,   6.33844760e-03,
         4.61417041e-03,   9.07832960e-03,   1.60917085e-03,
         2.07711262e-03,              nan,   3.29397886e-03,
         1.34909114e-03,   3.09713552e-03,   6.81060348e-03,
         3.87311936e-03,   6.13800411e-03,   4.71501102e-03,
                    nan,   2.40071749e-03,   2.37407855e-03,
         2.66318311e-03,   3.00429309e-03,   6.34616864e-03,
         1.56578002e-03,   2.29457286e-03,              nan,
         3.93478932e-03,   6.75563408e-03,   3.01006063e-03,
         3.04227234e-03,   2.38439711e-03,   2.99960005e-03,
         3.50933915e-03,              nan,   1.99569801e-03,
         2.31730355e-03,   6.49732756e-03,   3.92256899e-03,
         2.54912557e-03,   7.63201034e-03,   2.85364299e-03,
                    nan,   0.00000000e+00,   4.14728827e+00,
         1.46969385e+00,   2.22710575e+00,   1.74355958e+00,
         2.31516738e+00,   1.72046505e+00,   4.54312668e+00,
         0.00000000e+00,   1.74355958e+00,   5.09901951e+00,
         4.45421149e+00,   2.78567766e+00,   5.30659966e+00,
         1.35646600e+00,   1.60000000e+00,   0.00000000e+00,
         2.71293199e+00,   1.01980390e+00,   2.05912603e+00,
         3.74165739e+00,   2.41660919e+00,   4.75815090e+00,
         2.87054002e+00,   0.00000000e+00,   1.62480768e+00,
         1.78885438e+00,   2.09761770e+00,   2.00000000e+00,
         4.33589668e+00,   1.09544512e+00,   1.78885438e+00,
         0.00000000e+00,   2.72763634e+00,   5.00399840e+00,
         2.63818119e+00,   2.57681975e+00,   1.89736660e+00,
         2.15406592e+00,   2.60768096e+00,   0.00000000e+00,
         1.85472370e+00,   1.74355958e+00,   5.08330601e+00,
         3.49857114e+00,   1.74355958e+00,   4.89897949e+00,
         1.67332005e+00,   0.00000000e+00,   5.37059537e-03])]}
