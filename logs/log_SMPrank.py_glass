N, d, L:  154 9 6
distinct features:  100 distinct pairlabel:  27
Traceback (most recent call last):
  File "SMPrank.py", line 366, in <module>
    results = crossValidate(x,y,K=100)
  File "SMPrank.py", line 342, in crossValidate
    y_pred = SmpRank(K=K).fit(x_train, y_train).predict(x_test)
  File "SMPrank.py", line 70, in fit
    self.initialize(N,d,L, x_train, y_train)
  File "SMPrank.py", line 229, in initialize
    raise ValueError("too many prototypes")
ValueError: too many prototypes
/home/jasonzhang/anaconda2/lib/python2.7/site-packages/numpy/ma/core.py:867: RuntimeWarning: invalid value encountered in less_equal
  return umath.less_equal(x, self.critical_value)
dataset, N,d,L,K glass 214 9 6 4
N, d, L:  154 9 6
loss_valid 0 1.00421190617
loss_train 0 1.04819557782
loss_valid 1 0.848228228372
loss_train 1 0.892457747949
loss_valid 2 0.747584661187
loss_train 2 0.799911076592
loss_valid 3 0.690222617843
loss_train 3 0.737787231871
loss_valid 4 0.653683484435
loss_train 4 0.685411708065
loss_valid 5 0.611325167945
loss_train 5 0.644023325063
loss_valid 6 0.587973535124
loss_train 6 0.616458266941
loss_valid 7 0.61234966734
loss_train 7 0.599744140912
early stop at the end of epoch:  7
[0.627906976744186, 0, 0, 1.0, 0.97674418604651159, 0.7441860465116279, 0.86046511627906974, 0.88372093023255816, 0.97674418604651159, 1.0, 0.9767441860465116, 0.7441860465116279, 0.8604651162790697, 0.8837209302325582, 0.9767441860465116, 43.0, 43.0, 43.0, 43.0, 43.0, 43.0, 0, 0.90236435931781789, 0.84806201550387605, nan, 0.95, 0.9743589743589743, 0.9743589743589743, 0.975609756097561, 0.9473684210526315, 0.8571428571428571, nan, 0.875, 0.975, 0.9761904761904762, 0.9473684210526315, 0.125, 0.967741935483871, nan, 0.8157894736842105, 0.975609756097561, 0.9473684210526315, 0.125, 0.14285714285714285, 0.7777777777777778, nan, 0.9761904761904762, 0.9487179487179487, 0.16666666666666666, 0.2, 0.16666666666666666, 0.2, nan, 0.9473684210526315, 0.7777777777777778, 0.7777777777777778, 0.7777777777777778, 0.875, 0.7777777777777778, nan, 0, 38, 37, 37, 39, 36, 5, 0, 14, 38, 40, 36, 6, 29, 0, 36, 39, 36, 6, 5, 7, 0, 40, 37, 4, 3, 4, 3, 0, 36, 7, 7, 7, 6, 7, 0, 0.6001670494625192]
N, d, L:  154 9 6
loss_valid 0 0.988324416855
loss_train 0 0.716880615006
loss_valid 1 0.838421736592
loss_train 1 0.56861278554
loss_valid 2 0.756038265923
loss_train 2 0.51249420005
loss_valid 3 0.69654814643
loss_train 3 0.48267563674
loss_valid 4 0.649915979932
loss_train 4 0.464038631898
loss_valid 5 0.626035457033
loss_train 5 0.452394280737
loss_valid 6 0.601791848425
loss_train 6 0.443891550619
loss_valid 7 0.588458276906
loss_train 7 0.437655155902
loss_valid 8 0.585202709548
loss_train 8 0.433931484352
loss_valid 9 0.585425991302
loss_train 9 0.431377695482
early stop at the end of epoch:  9
[0.4883720930232558, 0, 0, 0.93023255813953487, 0.88372093023255816, 0.79069767441860461, 0.86046511627906974, 0.90697674418604646, 0.93023255813953487, 0.9302325581395349, 0.8837209302325582, 0.7906976744186046, 0.8604651162790697, 0.9069767441860465, 0.9302325581395349, 43.0, 43.0, 43.0, 43.0, 43.0, 43.0, 0, 0.88234754632633849, 0.82635658914728694, nan, 0.918918918918919, 0.9722222222222222, 0.9459459459459459, 0.95, 0.975, 0.7, nan, 0.7058823529411765, 0.975, 0.9512195121951219, 0.9761904761904762, 0.09090909090909091, 0.9666666666666667, nan, 0.9714285714285714, 0.95, 0.975, 0.5, 0.14285714285714285, 0.5, nan, 0.95, 0.975, 0.42857142857142855, 0.5, 0.42857142857142855, 0.42857142857142855, nan, 0.9743589743589743, 0.5714285714285714, 0.8, 0.5714285714285714, 0.5714285714285714, 0.5, nan, 0, 35, 34, 35, 38, 38, 8, 0, 15, 38, 39, 40, 9, 28, 0, 33, 38, 38, 8, 5, 10, 0, 38, 38, 5, 4, 5, 5, 0, 37, 5, 3, 5, 5, 6, 0, 0.65155764760863843]
N, d, L:  154 9 6
loss_valid 0 1.07592894984
loss_train 0 1.07176176218
loss_valid 1 0.956621510459
loss_train 1 0.867563542946
loss_valid 2 0.937134356679
loss_train 2 0.781004882492
loss_valid 3 0.934822570425
loss_train 3 0.731719863576
loss_valid 4 0.90519651944
loss_train 4 0.690769396615
loss_valid 5 0.872050426081
loss_train 5 0.663716301555
loss_valid 6 0.850323202441
loss_train 6 0.646239198113
loss_valid 7 0.831866250208
loss_train 7 0.639064238709
loss_valid 8 0.812226949575
loss_train 8 0.633636756105
loss_valid 9 0.800152308656
loss_train 9 0.619226839543
loss_valid 10 0.792655025441
loss_train 10 0.612453091927
loss_valid 11 0.787974552418
loss_train 11 0.609060071019
loss_valid 12 0.78426234243
loss_train 12 0.607138956671
loss_valid 13 0.781553286559
loss_train 13 0.606858660732
loss_valid 14 0.780159914269
loss_train 14 0.605187854712
loss_valid 15 0.778474557111
loss_train 15 0.603915091512
loss_valid 16 0.777171191375
loss_train 16 0.603324095691
loss_valid 17 0.77598813157
loss_train 1SMPrank.py:290: RuntimeWarning: invalid value encountered in double_scalars
  self.probfeature[k] = self.probfeature[k] / sum_probf
7 0.602864138391
loss_valid 18 0.775200894712
loss_train 18 0.602351282626
loss_valid 19 0.774282278427
loss_train 19 0.601941344375
loss_valid 20 0.77362819774
loss_train 20 0.601678679415
loss_valid 21 0.773025622127
loss_train 21 0.601462174425
loss_valid 22 0.772461229158
loss_train 22 0.601290459762
loss_valid 23 0.772086666961
loss_train 23 0.601147971109
loss_valid 24 0.771665328552
loss_train 24 0.601029450079
loss_valid 25 0.771302520135
loss_train 25 0.600928711107
loss_valid 26 0.770881271519
loss_train 26 0.600844498845
loss_valid 27 0.770638479822
loss_train 27 0.600770972107
loss_valid 28 0.770403411348
loss_train 28 0.600708142664
loss_valid 29 0.770168410714
loss_train 29 0.600652731004
loss_valid 30 0.769927653802
loss_train 30 0.600605931797
loss_valid 31 0.76971111809
loss_train 31 0.600564414974
loss_valid 32 0.769479359409
loss_train 32 0.600528596603
loss_valid 33 0.769397331194
loss_train 33 0.600496613105
loss_valid 34 0.769219824612
loss_train 34 0.600468563781
loss_valid 35 0.769047687481
loss_train 35 0.600443164463
loss_valid 36 0.768875171687
loss_train 36 0.600420678028
loss_valid 37 0.768695960515
loss_train 37 0.600400989265
loss_valid 38 0.768532305694
loss_train 38 0.600382631902
loss_valid 39 0.768460759321
loss_train 39 0.600366383548
loss_valid 40 0.768344363415
loss_train 40 0.600351571328
loss_valid 41 0.768200118149
loss_train 41 0.600338290931
loss_valid 42 nan
loss_train 42 nan
loss_valid 43 nan
loss_train 43 nan
loss_valid 44 nan
loss_train 44 nan
loss_valid 45 nan
loss_train 45 nan
loss_valid 46 nan
loss_train 46 nan
loss_valid 47 nan
loss_train 47 nan
loss_valid 48 nan
loss_train 48 nan
loss_valid 49 nan
loss_train 49 nan
loss_valid 50 nan
loss_train 50 nan
loss_valid 51 nan
loss_train 51 nan
loss_valid 52 nan
loss_train 52 nan
loss_valid 53 nan
loss_train 53 nan
loss_valid 54 nan
loss_train 54 nan
loss_valid 55 nan
loss_train 55 nan
loss_valid 56 nan
loss_train 56 nan
loss_valid 57 nan
loss_train 57 nan
loss_valid 58 nan
loss_train 58 nan
loss_valid 59 nan
loss_train 59 nan
loss_valid 60 nan
loss_train 60 nan
loss_valid 61 nan
loss_train 61 nan
loss_valid 62 nan
loss_train 62 nan
loss_valid 63 nan
loss_train 63 nan
loss_valid 64 nan
loss_train 64 nan
loss_valid 65 nan
loss_train 65 nan
loss_valid 66 nan
loss_train 66 nan
loss_valid 67 nan
loss_train 67 nan
loss_valid 68 nan
loss_train 68 nan
loss_valid 69 nan
loss_train 69 nan
loss_valid 70 nan
loss_train 70 nan
loss_valid 71 nan
loss_train 71 nan
loss_valid 72 nan
loss_train 72 nan
loss_valid 73 nan
loss_train 73 nan
loss_valid 74 nan
loss_train 74 nan
loss_valid 75 nan
loss_train 75 nan
loss_valid 76 nan
loss_train 76 nan
loss_valid 77 nan
loss_train 77 nan
loss_valid 78 nan
loss_train 78 nan
loss_valid 79 nan
loss_train 79 nan
loss_valid 80 nan
loss_train 80 nan
loss_valid 81 nan
loss_train 81 nan
loss_valid 82 nan
loss_train 82 nan
loss_valid 83 nan
loss_train 83 nan
loss_valid 84 nan
loss_train 84 nan
loss_valid 85 nan
loss_train 85 nan
loss_valid 86 nan
loss_train 86 nan
loss_valid 87 nan
loss_train 87 nan
loss_valid 88 nan
loss_train 88 nan
loss_valid 89 nan
loss_train 89 nan
loss_valid 90 nan
loss_train 90 nan
loss_valid 91 nan
loss_train 91 nan
loss_valid 92 nan
loss_train 92 nan
loss_valid 93 nan
loss_train 93 nan
loss_valid 94 nan
loss_train 94 nan
loss_valid 95 nan
loss_train 95 nan
loss_valid 96 nan
loss_train 96 nan
loss_valid 97 nan
loss_train 97 nan
loss_valid 98 nan
loss_train 98 nan
loss_valid 99 nan
loss_train 99 nan
NONERECALL: 0.0 for emoticon: 0
NONERECALL: 0.0 for emoticon: 1
[0.0, 0, 0, 0.0, 0.0, 0.23255813953488372, 0.93023255813953487, 1.0, 1.0, 0.001, 0.001, 0.23255813953488372, 0.9302325581395349, 1.0, 1.0, 43.0, 43.0, 43.0, 43.0, 43.0, 43.0, 0, 0.077479566118594981, -0.71162790697674416, nan, 0.025, 0.02702702702702703, 0.024390243902439025, 0.022222222222222223, 0.02702702702702703, 0.8571428571428571, nan, 0.0625, 0.022727272727272728, 0.022222222222222223, 0.02631578947368421, 0.9, 0.967741935483871, nan, 0.024390243902439025, 0.022727272727272728, 0.02702702702702703, 0.8333333333333334, 0.666/home/jasonzhang/anaconda2/lib/python2.7/site-packages/numpy/lib/nanfunctions.py:703: RuntimeWarning: Mean of empty slice
  warnings.warn("Mean of empty slice", RuntimeWarning)
/home/jasonzhang/anaconda2/lib/python2.7/site-packages/numpy/lib/nanfunctions.py:1202: RuntimeWarning: Degrees of freedom <= 0 for slice.
  warnings.warn("Degrees of freedom <= 0 for slice.", RuntimeWarning)
6666666666666, 0.8333333333333334, nan, 0.023255813953488372, 0.02631578947368421, nan, nan, 0.6666666666666666, 0.75, nan, 0.027777777777777776, 0.9, 0.8888888888888888, 0.9, 0.8888888888888888, 0.9090909090909091, nan, 0, 38, 35, 39, 43, 35, 5, 0, 14, 42, 43, 36, 8, 29, 0, 39, 42, 35, 4, 1, 4, 0, 41, 36, 0, 0, 1, 2, 0, 34, 8, 7, 8, 7, 9, 0, 0.13142882420430993]
N, d, L:  154 9 6
loss_valid 0 1.33883316649
loss_train 0 1.04899826593
loss_valid 1 1.19251607128
loss_train 1 0.864276780439
loss_valid 2 1.13447497871
loss_train 2 0.762498812193
loss_valid 3 1.0898686578
loss_train 3 0.707962139163
loss_valid 4 1.06126631107
loss_train 4 0.67007415201
loss_valid 5 1.04144561251
loss_train 5 0.638381752664
loss_valid 6 0.990449646955
loss_train 6 0.617112887242
loss_valid 7 0.983497489281
loss_train 7 0.598197359351
loss_valid 8 0.92512957852
loss_train 8 0.583239839516
loss_valid 9 0.91619938657
loss_train 9 0.574320511441
loss_valid 10 0.910528669806
loss_train 10 0.566430496945
loss_valid 11 0.918899241444
loss_train 11 0.557870215251
early stop at the end of epoch:  11
[0.6511627906976745, 0, 0, 0.93023255813953487, 0.93023255813953487, 1.0, 0.88372093023255816, 0.88372093023255816, 0.90697674418604646, 0.9302325581395349, 0.9302325581395349, 1.0, 0.8837209302325582, 0.8837209302325582, 0.9069767441860465, 43.0, 43.0, 43.0, 43.0, 43.0, 43.0, 0, 0.92165471848868485, 0.81395348837209303, nan, 0.8888888888888888, 0.9777777777777777, 0.8571428571428571, 0.975609756097561, 0.9714285714285714, 0.7272727272727273, nan, 0.8181818181818182, 0.975609756097561, 0.975, 0.972972972972973, nan, 0.96, nan, 0.8823529411764706, 0.975609756097561, 0.9714285714285714, 0.5833333333333334, 0.16666666666666666, 0.6153846153846154, nan, 0.975, 0.972972972972973, 0.16666666666666666, 0.14285714285714285, 0.16666666666666666, 0.14285714285714285, nan, 0.9722222222222222, 0.5833333333333334, 0.7, 0.5833333333333334, 0.7, 0.6363636363636364, nan, 0, 34, 43, 33, 39, 33, 9, 0, 20, 39, 38, 35, 0, 23, 0, 32, 39, 33, 10, 4, 11, 0, 38, 35, 4, 5, 4, 5, 0, 34, 10, 8, 10, 8, 9, 0, 0.62177560309968882]
N, d, L:  155 9 6
loss_valid 0 1.27238305573
loss_train 0 1.05788598376
loss_valid 1 1.00769354555
loss_train 1 0.822377399708
loss_valid 2 0.898883994998
loss_train 2 0.747985084333
loss_valid 3 0.882006014899
loss_train 3 0.71376956693
loss_valid 4 0.835469657855
loss_train 4 0.676939889287
loss_valid 5 0.835112352899
loss_train 5 0.653103648831
loss_valid 6 0.861593572391
loss_train 6 0.632871600298
early stop at the end of epoch:  6
[0.5, 0, 0, 0.88095238095238093, 0.69047619047619047, 0.90476190476190477, 0.97619047619047616, 0.8571428571428571, 0.9285714285714286, 0.8809523809523809, 0.6904761904761905, 0.9047619047619048, 0.9761904761904762, 0.8571428571428571, 0.9285714285714286, 42.0, 42.0, 42.0, 42.0, 42.0, 42.0, 0, 0.8679734149960977, 0.79047619047619055, nan, 0.8857142857142857, 0.9761904761904762, 0.8787878787878788, 0.9743589743589743, 0.972972972972973, 0.5454545454545454, nan, 0.6363636363636364, 0.7692307692307693, 0.9743589743589743, 0.9736842105263158, 0.25, 0.9583333333333334, nan, 0.8787878787878788, 0.9743589743589743, 0.972972972972973, 0.8461538461538461, 0.8571428571428571, 0.8461538461538461, nan, 0.9736842105263158, 0.9736842105263158, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.125, nan, 0.972972972972973, 0.5555555555555556, 0.625, 0.5555555555555556, 0.625, 0.5555555555555556, nan, 0, 33, 40, 31, 37, 35, 9, 0, 20, 37, 37, 36, 2, 22, 0, 31, 37, 35, 11, 5, 11, 0, 36, 36, 5, 5, 5, 6, 0, 35, 7, 6, 7, 6, 7, 0, 0.61668007384988599]
{'perf': [array([  0.45348837,   0.        ,   0.        ,   0.7482835 ,
         0.69623477,   0.73444075,   0.90221484,   0.90631229,
         0.94850498,   0.7484835 ,   0.69643477,   0.73444075,
         0.90221484,   0.90631229,   0.94850498,  42.8       ,
        42.8       ,  42.8       ,  42.8       ,  42.8       ,
        42.8       ,   0.        ,   0.73036392,   0.51344408,
                nan,   0.73370442,   0.7855153 ,   0.73612518,
         0.77956014,   0.7787594 ,   0.7374026 ,          nan,
         0.61958556,   0.74351356,   0.77979824,   0.77930637,
         0.34147727,   0.96409677,          nan,   0.71454982,
         0.77966115,   0.7787594 ,   0.5775641 ,   0.3952381 ,
         0.71452991,          nan,   0.7796261 ,   0.77933818,
         0.22619048,   0.24642857,   0.31428571,   0.32928571,
                nan,   0.77894007,   0.67761905,   0.75833333,
         0.67761905,   0.73206349,   0.67575758,          nan,
         0.        ,  35.6       ,  37.8       ,  35.        ,
        39.2       ,  35.4       ,   7.2       ,   0.        ,
        16.6       ,  38.8       ,  39.4       ,  36.6       ,
         5.        ,  26.2       ,   0.        ,  34.2       ,
        39.        ,  35.4       ,   7.8       ,   4.        ,
         8.6       ,   0.        ,  38.6       ,  36.4       ,
         3.6       ,   3.4       ,   3.8       ,   4.2       ,
         0.        ,  35.2       ,   7.4       ,   6.2       ,
         7.4       ,   6.4       ,   7.6       ,   0.        ,   0.52432184]), array([ 0.23602073,  0.        ,  0.        ,  0.37605886,  0.36148792,
        0.266337  ,  0.04491212,  0.04942914,  0.03436572,  0.3756609 ,
        0.36110273,  0.266337  ,  0.04491212,  0.04942914,  0.03436572,
        0.4       ,  0.4       ,  0.4       ,  0.4       ,  0.4       ,
        0.4       ,  0.        ,  0.32694639,  0.6128193 ,         nan,
        0.3551167 ,  0.37924867,  0.35842562,  0.37879492,  0.37600033,
        0.11577658,         nan,  0.29077482,  0.36911647,  0.37890215,
        0.37664108,  0.32785765,  0.00407871,         nan,  0.34862255,
        0.37859297,  0.37600033,  0.26397806,  0.30550505,  0.13512385,
               nan,  0.37830949,  0.37663791,  0.11724831,  0.14824655,
        0.20503857,  0.23666906,         nan,  0.37571484,  0.13751419,
        0.0898215 ,  0.13751419,  0.12908856,  0.14949495,         nan,
        0.        ,  2.05912603,  3.31058907,  2.82842712,  2.03960781,
        1.62480768,  1.83303028,  0.        ,  2.8       ,  1.72046505,
        2.05912603,  1.74355958,  3.46410162,  3.05941171,  0.        ,
        2.92574777,  1.67332005,  1.62480768,  2.56124969,  1.54919334,
        2.72763634,  0.        ,  1.74355958,  1.0198039 ,  1.8547237 ,
        1.8547237 ,  1.46969385,  1.46969385,  0.        ,  1.16619038,
        1.62480768,  1.72046505,  1.62480768,  1.0198039 ,  1.2       ,
        0.        ,  0.19714647])]}
