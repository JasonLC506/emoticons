/home/jasonzhang/anaconda2/lib/python2.7/site-packages/numpy/ma/core.py:867: RuntimeWarning: invalid value encountered in less_equal
  return umath.less_equal(x, self.critical_value)
N, d, L:  29353 9 5
loss_valid 0 0.945685105989
loss_train 0 0.944543718203
loss_valid 1 0.832475380688
loss_train 1 0.830369348842
loss_valid 2 0.73278733561
loss_train 2 0.729854647097
loss_valid 3 0.647586676672
loss_train 3 0.644057143547
loss_valid 4 0.582186933724
loss_train 4 0.578503781997
loss_valid 5 0.532772089962
loss_train 5 0.52970997867
loss_valid 6 0.494763829561
loss_train 6 0.492313435259
loss_valid 7 0.463881826149
loss_train 7 0.461793768618
loss_valid 8 0.43679473197
loss_train 8 0.434772622638
loss_valid 9 0.412421970773
loss_train 9 0.409416593577
loss_valid 10 0.389322472235
loss_train 10 0.385234646884
loss_valid 11 0.367374611129
loss_train 11 0.362728849697
loss_valid 12 0.347270882737
loss_train 12 0.342114177052
loss_valid 13 0.330366914363
loss_train 13 0.325101766443
loss_valid 14 0.316877966035
loss_train 14 0.311100328348
loss_valid 15 0.304211488391
loss_train 15 0.298369738203
loss_valid 16 0.292977131304
loss_train 16 0.287467554892
loss_valid 17 0.284366926261
loss_train 17 0.278753872847
loss_valid 18 0.276719409736
loss_train 18 0.271271397708
loss_valid 19 0.270702597705
loss_train 19 0.264208694828
loss_valid 20 0.266098106939
loss_train 20 0.259260526252
loss_valid 21 0.261122270926
loss_train 21 0.254551694313
loss_valid 22 0.256281777854
loss_train 22 0.250097742602
loss_valid 23 0.252366903637
loss_train 23 0.246285632621
loss_valid 24 0.249217131626
loss_train 24 0.243032920984
loss_valid 25 0.246051020928
loss_train 25 0.240177414129
loss_valid 26 0.243373402732
loss_train 26 0.237293235379
loss_valid 27 0.241133880417
loss_train 27 0.234629710568
loss_valid 28 0.23891026512
loss_train 28 0.231942504525
loss_valid 29 0.235183394833
loss_train 29 0.229492636892
loss_valid 30 0.233302002979
loss_train 30 0.227005058135
loss_valid 31 0.23004113544
loss_train 31 0.224254262257
loss_valid 32 0.228589567785
loss_train 32 0.222778245955
loss_valid 33 0.226104216236
loss_train 33 0.220323908836
loss_valid 34 0.22421179394
loss_train 34 0.218644120659
loss_valid 35 0.221933472225
loss_train 35 0.21671985039
loss_valid 36 0.221185510991
loss_train 36 0.215581903548
loss_valid 37 0.219892344023
loss_train 37 0.213371986335
loss_valid 38 0.217505797177
loss_train 38 0.211337692724
loss_valid 39 0.214689610056
loss_train 39 0.20898024476
loss_valid 40 0.214439840447
loss_train 40 0.208296541677
loss_valid 41 0.213536299025
loss_train 41 0.207668864239
loss_valid 42 0.210920543425
loss_train 42 0.206907598001
loss_valid 43 0.210455761005
loss_train 43 0.205303277675
loss_valid 44 0.208946782319
loss_train 44 0.203554246201
loss_valid 45 0.208710414997
loss_train 45 0.203423290244
loss_valid 46 0.209039845657
loss_train 46 0.203334545605
early stop at the end of epoch:  46
[0.19487368162864852, 0, 0, 0.95425558008339462, 0.80340936963453524, 0.67696835908756436, 0.55984792739759626, 0.40679421142997302, 0.9542555800833946, 0.8034093696345352, 0.6769683590875644, 0.5598479273975963, 0.406794211429973, 8154.0, 8154.0, 8154.0, 8154.0, 8154.0, 0, 0.65241353455371187, 0.26620063772381652, nan, 0.9625304136253041, 0.9815782922579039, 0.9701419481155164, 0.9923588858762632, 0.28631422924901184, nan, 0.9636319257993654, 0.9645286686103013, 0.9659627329192546, 0.2779521854624487, 0.29623245506032997, nan, 0.9777288301517376, 0.9862946647087616, 0.2843811394891945, 0.30059376546264227, 0.2973968565815324, nan, 0.9930503847108464, 0.2779809802487198, 0.30220179046697315, 0.293713163064833, 0.30031484620973603, nan, 0, 4108, 4015, 4084, 4055, 4046, 0, 4095, 4114, 4023, 4139, 4059, 0, 4084, 4084, 4070, 4040, 4070, 0, 4027, 4099, 4131, 4070, 4127, 0, 0.53337798072934284]
N, d, L:  29353 9 5
loss_valid 0 0.945517157507
loss_train 0 0.945632077841
loss_valid 1 0.831727902293
loss_train 1 0.8322527096
loss_valid 2 0.73154784052
loss_train 2 0.732358244413
loss_valid 3 0.646653189215
loss_train 3 0.647217829743
loss_valid 4 0.581437231437
loss_train 4 0.581092063963
loss_valid 5 0.532736297378
loss_train 5 0.530850508865
loss_valid 6 0.495135348727
loss_train 6 0.491732948679
loss_valid 7 0.464409603952
loss_train 7 0.459641714067
loss_valid 8 0.437421145834
loss_train 8 0.431444757734
loss_valid 9 0.411648574073
loss_train 9 0.405039529487
loss_valid 10 0.386544722779
loss_train 10 0.379412147699
loss_valid 11 0.362705886837
loss_train 11 0.355803758552
loss_valid 12 0.343153712577
loss_train 12 0.335784110029
loss_valid 13 0.325697951914
loss_train 13 0.318194980929
loss_valid 14 0.311013850171
loss_train 14 0.303518665792
loss_valid 15 0.30016010283
loss_train 15 0.292782652451
loss_valid 16 0.290975131153
loss_train 16 0.283661667722
loss_valid 17 0.284543088381
loss_train 17 0.276829448853
loss_valid 18 0.277787447071
loss_train 18 0.2702173004
loss_valid 19 0.271391704515
loss_train 19 0.263754276107
loss_valid 20 0.265935265542
loss_train 20 0.258356938372
loss_valid 21 0.260990249872
loss_train 21 0.253879951021
loss_valid 22 0.257282551531
loss_train 22 0.25041400888
loss_valid 23 0.254435300171
loss_train 23 0.24690928788
loss_valid 24 0.250735093849
loss_train 24 0.24321681828
loss_valid 25 0.247344310204
loss_train 25 0.239717481581
loss_valid 26 0.24502321683
loss_train 26 0.236700321546
loss_valid 27 0.241731046614
loss_train 27 0.233897322865
loss_valid 28 0.240283355667
loss_train 28 0.231900686725
loss_valid 29 0.23762785881
loss_train 29 0.229790334907
loss_valid 30 0.236682600718
loss_train 30 0.227653834287
loss_valid 31 0.233520243257
loss_train 31 0.224991426015
loss_valid 32 0.231454613612
loss_train 32 0.223313423436
loss_valid 33 0.23114101789
loss_train 33 0.221913913902
loss_valid 34 0.228587670815
loss_train 34 0.219888329158
loss_valid 35 0.227464931078
loss_train 35 0.218656754267
loss_valid 36 0.225178028377
loss_train 36 0.217382832596
loss_valid 37 0.225340298634
loss_train 37 0.216110163129
early stop at the end of epoch:  37
[0.37110620554329166, 0, 0, 0.87821927888153051, 0.85172921265636492, 0.79617365710080945, 0.78403237674760851, 0.67120431689968119, 0.8782192788815305, 0.8517292126563649, 0.7961736571008095, 0.7840323767476085, 0.6712043168996812, 8154.0, 8154.0, 8154.0, 8154.0, 8154.0, 0, 0.79290327267984773, 0.6617120431689969, nan, 0.9600389863547758, 0.9202153695545766, 0.9193820902727492, 0.9540865384615385, 0.7035027133695116, nan, 0.9434008294706026, 0.9529612478674141, 0.9583837406242439, 0.7212671905697446, 0.7006651884700665, nan, 0.9543467702768335, 0.9475204622051036, 0.696637608966376, 0.7491985203452528, 0.7324257425742574, nan, 0.9783411272458774, 0.6945972986493246, 0.7118012422360248, 0.704045954045954, 0.6893772893772894, nan, 0, 4102, 4084, 4141, 4158, 4052, 0, 4097, 4101, 4131, 4070, 4057, 0, 4116, 4152, 4013, 4053, 4038, 0, 4061, 3996, 4023, 4002, 4093, 0, 0.82080009312744318]
N, d, L:  29353 9 5
loss_valid 0 0.938592152307
loss_train 0 0.938206731736
loss_valid 1 0.825510865279
loss_train 1 0.824217216804
loss_valid 2 0.726454903132
loss_train 2 0.724005667639
loss_valid 3 0.64264958328
loss_train 3 0.639303041172
loss_valid 4 0.578256514157
loss_train 4 0.574412432318
loss_valid 5 0.530099552968
loss_train 5 0.525974002675
loss_valid 6 0.493265320964
loss_train 6 0.488850376273
loss_valid 7 0.463421367253
loss_train 7 0.458543622451
loss_valid 8 0.437566884042
loss_train 8 0.431852330447
loss_valid 9 0.412692091042
loss_train 9 0.406939631951
loss_valid 10 0.388598617393
loss_train 10 0.383566441195
loss_valid 11 0.36744903749
loss_train 11 0.362737987744
loss_valid 12 0.349690338704
loss_train 12 0.345041433307
loss_valid 13 0.335443028729
loss_train 13 0.329998695227
loss_valid 14 0.323178192219
loss_train 14 0.318090783911
loss_valid 15 0.313947194373
loss_train 15 0.308818114165
loss_valid 16 0.305816356823
loss_train 16 0.3005597611
loss_valid 17 0.29848748518
loss_train 17 0.293206197895
loss_valid 18 0.290518082222
loss_train 18 0.286043687648
loss_valid 19 0.283724032666
loss_train 19 0.279174212862
loss_valid 20 0.278504968152
loss_train 20 0.2738757298
loss_valid 21 0.273809991544
loss_train 21 0.269041481035
loss_valid 22 0.268852110428
loss_train 22 0.263759015684
loss_valid 23 0.265217148706
loss_train 23 0.259743629057
loss_valid 24 0.26138651992SMPrank.py:89: UserWarning: training set loss increase
  warnings.warn("training set loss increase")
8
loss_train 24 0.255692841845
loss_valid 25 0.257998827036
loss_train 25 0.251959784105
loss_valid 26 0.254467746468
loss_train 26 0.248560452394
loss_valid 27 0.251426538796
loss_train 27 0.245831579734
loss_valid 28 0.248971288745
loss_train 28 0.243407942343
loss_valid 29 0.247222022166
loss_train 29 0.241523052971
loss_valid 30 0.244934516718
loss_train 30 0.238737209953
loss_valid 31 0.24262651139
loss_train 31 0.2373787731
loss_valid 32 0.240791012704
loss_train 32 0.234699971475
loss_valid 33 0.240231657762
loss_train 33 0.232730154535
loss_valid 34 0.237259079573
loss_train 34 0.231365122686
loss_valid 35 0.237910279247
loss_train 35 0.230874060844
early stop at the end of epoch:  35
[0.5165562913907285, 0, 0, 0.86411577140053963, 0.83541819965661024, 0.82622025999509441, 0.75472160902624474, 0.7512877115526122, 0.8641157714005396, 0.8354181996566102, 0.8262202599950944, 0.7547216090262447, 0.7512877115526122, 8154.0, 8154.0, 8154.0, 8154.0, 8154.0, 0, 0.80506872350372538, 0.71179789060583776, nan, 0.956436612824278, 0.942878156410885, 0.9042709867452136, 0.9162303664921466, 0.7595776031434185, nan, 0.9136015798568254, 0.9492663516538175, 0.9500499500499501, 0.7862221132630547, 0.7832968103238374, nan, 0.9643559488692232, 0.9332509270704573, 0.7752203721841332, 0.7715736040609137, 0.7618581907090465, nan, 0.9171554252199413, 0.8037135278514589, 0.7628791526239769, 0.7850717238025772, 0.7875061485489424, nan, 0, 4084, 4077, 4072, 4009, 4070, 0, 4049, 4019, 4002, 4077, 4105, 0, 4066, 4043, 4082, 4135, 4088, 0, 4090, 4145, 4152, 4111, 4064, 0, 0.85245467180535384]
N, d, L:  29354 9 5
loss_valid 0 0.945969933013
loss_train 0 0.945838382801
loss_valid 1 0.830904156718
loss_train 1 0.830284254122
loss_valid 2 0.727608519886
loss_train 2 0.726379908449
loss_valid 3 0.640996545773
loss_train 3 0.639292977681
loss_valid 4 0.575909256217
loss_train 4 0.57362949454
loss_valid 5 0.528061299598
loss_train 5 0.524965913807
loss_valid 6 0.492008669296
loss_train 6 0.487717726149
loss_valid 7 0.462985278044
loss_train 7 0.457339080917
loss_valid 8 0.437922332233
loss_train 8 0.430544546851
loss_valid 9 0.414070144488
loss_train 9 0.405367845115
loss_valid 10 0.390822249649
loss_train 10 0.380851780104
loss_valid 11 0.367885577237
loss_train 11 0.358346320235
loss_valid 12 0.348552638206
loss_train 12 0.339152107438
loss_valid 13 0.331244712057
loss_train 13 0.322252695881
loss_valid 14 0.315684393572
loss_train 14 0.307863601806
loss_valid 15 0.303933342969
loss_train 15 0.296657922443
loss_valid 16 0.29476674698
loss_train 16 0.287998833194
loss_valid 17 0.288116321133
loss_train 17 0.28132659181
loss_valid 18 0.281839154047
loss_train 18 0.275131579647
loss_valid 19 0.275739203753
loss_train 19 0.269010710341
loss_valid 20 0.269898474128
loss_train 20 0.263883805747
loss_valid 21 0.26411071578
loss_train 21 0.258981273545
loss_valid 22 0.259150061517
loss_train 22 0.254630594266
loss_valid 23 0.25459677268
loss_train 23 0.249602223404
loss_valid 24 0.24980407006
loss_train 24 0.245425808489
loss_valid 25 0.246311242286
loss_train 25 0.242295229867
loss_valid 26 0.242585462207
loss_train 26 0.238990102705
loss_valid 27 0.239497707618
loss_train 27 0.235860366179
loss_valid 28 0.236712322638
loss_train 28 0.233152234095
loss_valid 29 0.232197371909
loss_train 29 0.230041775967
loss_valid 30 0.230076520235
loss_train 30 0.227240676321
loss_valid 31 0.225302827138
loss_train 31 0.22362067201
loss_valid 32 0.22159577818
loss_train 32 0.22010064674
loss_valid 33 0.218537324231
loss_train 33 0.217504891888
loss_valid 34 0.216974743282
loss_train 34 0.215145692664
loss_valid 35 0.2169507821
loss_train 35 0.214419667686
loss_valid 36 0.215525176203
loss_train 36 0.212705539015
loss_valid 37 0.212260241975
loss_train 37 0.210836738697
loss_valid 38 0.211902321892
loss_train 38 0.210983712525
last epoch loss:  0.210836738697
current epoch loss:  0.210983712525
loss_valid 39 0.210345914013
loss_train 39 0.209957053425
loss_valid 40 0.209833241955
loss_train 40 0.208461824302
loss_valid 41 0.208654265505
loss_train 41 0.207523091247
loss_valid 42 0.210155933942
loss_train 42 0.208372336577
last epoch loss:  0.207523091247
current epoch loss:  0.208372336577
early stop at the end of epoch:  42
[0.2955967128664295, 0, 0, 0.95155157610695451, 0.79676192812461666, 0.7010916227155648, 0.65117134797007237, 0.50042928983196366, 0.9515515761069545, 0.7967619281246167, 0.7010916227155648, 0.6511713479700724, 0.5004292898319637, 8153.0, 8153.0, 8153.0, 8153.0, 8153.0, 0, 0.70423034982398103, 0.42354961363915122, nan, 0.9799237611181703, 0.9486097318768619, 0.9931456548347614, 0.9785418190685199, 0.45405021316911415, nan, 0.9674154529745742, 0.9362056653177815, 0.941346850108617, 0.4608864131751029, 0.45104724792985873, nan, 0.9386176897051716, 0.9651468130818811, 0.47961689587426326, 0.4595551061678463, 0.46205523762129885, nan, 0.9765133171912833, 0.42480276134122286, 0.4828101644245142, 0.43170362903225806, 0.47628507573876333, nan, 0, 3933, 4026, 4083, 4099, 4220, 0, 4049, 4199, 4141, 4127, 4104, 0, 4136, 4187, 4070, 3954, 4017, 0, 4128, 4054, 4012, 3966, 4025, 0, 0.66383696580425278]
N, d, L:  29354 9 5
loss_valid 0 0.948573915679
loss_train 0 0.947633556202
loss_valid 1 0.83259729851
loss_train 1 0.830673237637
loss_valid 2 0.733198984274
loss_train 2 0.730105682195
loss_valid 3 0.649868118072
loss_train 3 0.645461873042
loss_valid 4 0.586309243908
loss_train 4 0.580948701581
loss_valid 5 0.538496604065
loss_train 5 0.532612740503
loss_valid 6 0.500801043277
loss_train 6 0.494925340429
loss_valid 7 0.469368727962
loss_train 7 0.463789257125
loss_valid 8 0.442025303633
loss_train 8 0.436487970723
loss_valid 9 0.417034096238
loss_train 9 0.411091491331
loss_valid 10 0.393494677556
loss_train 10 0.387148527022
loss_valid 11 0.37228198907
loss_train 11 0.365948239578
loss_valid 12 0.352955557773
loss_train 12 0.346633866433
loss_valid 13 0.335647089903
loss_train 13 0.329783049806
loss_valid 14 0.321870309084
loss_train 14 0.316233127915
loss_valid 15 0.309654367919
loss_train 15 0.30468934944
loss_valid 16 0.29970052753
loss_train 16 0.294920762052
loss_valid 17 0.291616011199
loss_train 17 0.287166928754
loss_valid 18 0.285156629887
loss_train 18 0.280695989329
loss_valid 19 0.27957129889
loss_train 19 0.274807204593
loss_valid 20 0.272918872438
loss_train 20 0.269096845243
loss_valid 21 0.267711580515
loss_train 21 0.263383147012
loss_valid 22 0.261040408002
loss_train 22 0.257405086661
loss_valid 23 0.255382220621
loss_train 23 0.251873695656
loss_valid 24 0.250073740061
loss_train 24 0.24717077324
loss_valid 25 0.246696162599
loss_train 25 0.243602079334
loss_valid 26 0.24284350445
loss_train 26 0.239568833208
loss_valid 27 0.240241627926
loss_train 27 0.236764426124
loss_valid 28 0.236824405075
loss_train 28 0.233742708562
loss_valid 29 0.234190538558
loss_train 29 0.230656502487
loss_valid 30 0.231387308476
loss_train 30 0.227870934493
loss_valid 31 0.229755475715
loss_train 31 0.226023711634
loss_valid 32 0.226451461188
loss_train 32 0.222823216291
loss_valid 33 0.22473061296
loss_train 33 0.220839740722
loss_valid 34 0.22355390717
loss_train 34 0.219399621241
loss_valid 35 0.223293257231
loss_train 35 0.217977396403
loss_valid 36 0.221202511968
loss_train 36 0.216270221744
loss_valid 37 0.220705682025
loss_train 37 0.214880302666
loss_valid 38 0.218187742361
loss_train 38 0.212426004002
loss_valid 39 0.217251747703
loss_train 39 0.21170535559
loss_valid 40 0.214349495999
loss_train 40 0.210291247341
loss_valid 41 0.213524478432
loss_train 41 0.207893645395
loss_valid 42 0.213722124393
loss_train 42 0.206682119293
early stop at the end of epoch:  42
[0.33840304182509506, 0, 0, 0.94345639641849621, 0.8114804366490862, 0.72881148043664912, 0.63056543603581505, 0.54029191708573532, 0.9434563964184962, 0.8114804366490862, 0.7288114804366491, 0.630565436035815, 0.5402919170857353, 8153.0, 8153.0, 8153.0, 8153.0, 8153.0, 0, 0.71745512942489997, 0.43603581503740957, nan, 0.9820594740722536, 0.9548185532323961, 0.9667798254122212, 0.9832964873495456, 0.46208414872798437, nan, 0.9339622641509434, 0.9765316205533597, 0.9546673165976115, 0.46096096096096095, 0.47377/home/jasonzhang/anaconda2/lib/python2.7/site-packages/numpy/lib/nanfunctions.py:703: RuntimeWarning: Mean of empty slice
  warnings.warn("Mean of empty slice", RuntimeWarning)
/home/jasonzhang/anaconda2/lib/python2.7/site-packages/numpy/lib/nanfunctions.py:1202: RuntimeWarning: Degrees of freedom <= 0 for slice.
  warnings.warn("Degrees of freedom <= 0 for slice.", RuntimeWarning)
578921203084, nan, 0.9346835443037975, 0.9830296980284502, 0.43937515497148527, 0.46653687028474083, 0.4865700023769907, nan, 0.9776264591439688, 0.4711209006363191, 0.4930932412432166, 0.46289156626506023, 0.4946847960444994, nan, 0, 4067, 4159, 4122, 4069, 4086, 0, 4132, 4046, 4101, 3994, 4021, 0, 3948, 4005, 4031, 4107, 4205, 0, 4110, 4084, 4052, 4148, 4043, 0, 0.67390989888768216]
{'perf': [array([  3.43307187e-01,   0.00000000e+00,   0.00000000e+00,
         9.18319721e-01,   8.19759829e-01,   7.45853076e-01,
         6.76067739e-01,   5.74001489e-01,   9.18319721e-01,
         8.19759829e-01,   7.45853076e-01,   6.76067739e-01,
         5.74001489e-01,   8.15360000e+03,   8.15360000e+03,
         8.15360000e+03,   8.15360000e+03,   8.15360000e+03,
         0.00000000e+00,   7.34414202e-01,   4.99859200e-01,
                    nan,   9.68197850e-01,   9.49620021e-01,
         9.50744101e-01,   9.64902819e-01,   5.33105782e-01,
                    nan,   9.44402410e-01,   9.55898711e-01,
         9.54082118e-01,   5.41457773e-01,   5.41003498e-01,
                    nan,   9.53946557e-01,   9.63048513e-01,
         5.35046234e-01,   5.49491573e-01,   5.48061206e-01,
                    nan,   9.68537343e-01,   5.34443094e-01,
         5.50557118e-01,   5.35485207e-01,   5.49633631e-01,
                    nan,   0.00000000e+00,   4.05880000e+03,
         4.07220000e+03,   4.10040000e+03,   4.07800000e+03,
         4.09480000e+03,   0.00000000e+00,   4.08440000e+03,
         4.09580000e+03,   4.07960000e+03,   4.08140000e+03,
         4.06920000e+03,   0.00000000e+00,   4.07000000e+03,
         4.09420000e+03,   4.05320000e+03,   4.05780000e+03,
         4.08360000e+03,   0.00000000e+00,   4.08320000e+03,
         4.07560000e+03,   4.07400000e+03,   4.05940000e+03,
         4.07040000e+03,   0.00000000e+00,   7.08875922e-01]), array([  1.04996079e-01,   0.00000000e+00,   0.00000000e+00,
         3.89196820e-02,   2.06495296e-02,   5.66218210e-02,
         8.25107614e-02,   1.22789472e-01,   3.89196820e-02,
         2.06495296e-02,   5.66218210e-02,   8.25107614e-02,
         1.22789472e-01,   4.89897949e-01,   4.89897949e-01,
         4.89897949e-01,   4.89897949e-01,   4.89897949e-01,
         0.00000000e+00,   5.71591066e-02,   1.64685915e-01,
                    nan,   1.06457053e-02,   1.97962764e-02,
         3.33905339e-02,   2.74378735e-02,   1.74656535e-01,
                    nan,   1.97892161e-02,   1.37158744e-02,
         8.22728208e-03,   1.86894454e-01,   1.77043757e-01,
                    nan,   1.60009372e-02,   2.03619777e-02,
         1.78243511e-01,   1.82276418e-01,   1.75334106e-01,
                    nan,   2.63944429e-02,   1.89715289e-01,
         1.67721453e-01,   1.81769265e-01,   1.71272220e-01,
                    nan,   0.00000000e+00,   6.45210043e+01,
         5.11914055e+01,   2.64317990e+01,   4.94206435e+01,
         6.41573067e+01,   0.00000000e+00,   3.17590932e+01,
         6.22684511e+01,   5.67365843e+01,   5.13521178e+01,
         3.18395980e+01,   0.00000000e+00,   6.56780024e+01,
         6.72826872e+01,   2.64680940e+01,   6.24416528e+01,
         6.55090833e+01,   0.00000000e+00,   3.58407589e+01,
         4.94716080e+01,   5.70298168e+01,   6.72951707e+01,
         3.62414128e+01,   0.00000000e+00,   1.15924575e-01])]}
SMPrank.py:89: UserWarning: training set loss increase
  warnings.warn("training set loss increase")
/home/jasonzhang/anaconda2/lib/python2.7/site-packages/numpy/ma/core.py:867: RuntimeWarning: invalid value encountered in less_equal
  return umath.less_equal(x, self.critical_value)
dataset, N,d,L,K fried 40768 9 5 60
N, d, L:  29353 9 5
loss_valid 0 0.902751597743
loss_train 0 0.901648275352
loss_valid 1 0.78978264937
loss_train 1 0.787712997899
loss_valid 2 0.682115530264
loss_train 2 0.679053433996
loss_valid 3 0.598183980815
loss_train 3 0.594940300513
loss_valid 4 0.536744207326
loss_train 4 0.533815428887
loss_valid 5 0.49013270297
loss_train 5 0.488172829978
loss_valid 6 0.452981610063
loss_train 6 0.451741625607
loss_valid 7 0.421031093102
loss_train 7 0.420118490355
loss_valid 8 0.393020076608
loss_train 8 0.392278703365
loss_valid 9 0.371565962458
loss_train 9 0.370160259094
loss_valid 10 0.352719393755
loss_train 10 0.350883662871
loss_valid 11 0.334521367067
loss_train 11 0.33314644321
loss_valid 12 0.320574197027
loss_train 12 0.318913332507
loss_valid 13 0.308513929535
loss_train 13 0.306945297157
loss_valid 14 0.299903442288
loss_train 14 0.297509358813
loss_valid 15 0.293454496762
loss_train 15 0.290603870191
loss_valid 16 0.288832854459
loss_train 16 0.285382756442
loss_valid 17 0.284317898215
loss_train 17 0.280377656541
loss_valid 18 0.280257661126
loss_train 18 0.276683620693
loss_valid 19 0.276386405692
loss_train 19 0.272397412729
loss_valid 20 0.272666520895
loss_train 20 0.269147935626
loss_valid 21 0.269499931109
loss_train 21 0.265415057295
loss_valid 22 0.265941705349
loss_train 22 0.262368688111
loss_valid 23 0.264116141581
loss_train 23 0.259544666806
loss_valid 24 0.261759459632
loss_train 24 0.2569388499
loss_valid 25 0.258774575163
loss_train 25 0.25463019508
loss_valid 26 0.256373590581
loss_train 26 0.252473991262
loss_valid 27 0.255527511315
loss_train 27 0.251030098021
loss_valid 28 0.252679604064
loss_train 28 0.248364962884
loss_valid 29 0.250861157595
loss_train 29 0.247709728087
loss_valid 30 0.249094736586
loss_train 30 0.245796781988
loss_valid 31 0.247999166942
loss_train 31 0.245005321535
loss_valid 32 0.247247837966
loss_train 32 0.243411420968
loss_valid 33 0.243577661619
loss_train 33 0.241971318818
loss_valid 34 0.240987522669
loss_train 34 0.239224124036
loss_valid 35 0.239886318149
loss_train 35 0.238873043534
loss_valid 36 0.240201780454
loss_train 36 0.238947397132
last epoch loss:  0.238873043534
current epoch loss:  0.238947397132
early stop at the end of epoch:  36
[0.17279862644101054, 0, 0, 0.94346333088054946, 0.8003433897473633, 0.67561932793720869, 0.57027225901398082, 0.40814324258032869, 0.9434633308805495, 0.8003433897473633, 0.6756193279372087, 0.5702722590139808, 0.4081432425803287, 8154.0, 8154.0, 8154.0, 8154.0, 8154.0, 0, 0.65300974417003943, 0.29050772626931565, nan, 0.9659367396593674, 0.9798356982823002, 0.9637787567302986, 0.9763371949716539, 0.33473320158102765, nan, 0.9628996826946546, 0.9426627793974732, 0.9570186335403726, 0.31489978266119295, 0.3380940655011081, nan, 0.9647577092511013, 0.9747919725893294, 0.33079567779960706, 0.34685799109351806, 0.3300589390962672, nan, 0.9712087366592207, 0.2996830041453304, 0.3348657149770143, 0.3106581532416503, 0.31751029304916445, nan, 0, 4108, 4015, 4084, 4055, 4046, 0, 4095, 4114, 4023, 4139, 4059, 0, 4084, 4084, 4070, 4040, 4070, 0, 4027, 4099, 4131, 4070, 4127, 0, 0.56072283142850143]
N, d, L:  29353 9 5
loss_valid 0 0.900473119433
loss_train 0 0.899475639645
loss_valid 1 0.785509421853
loss_train 1 0.784882638403
loss_valid 2 0.676793941835
loss_train 2 0.677305297566
loss_valid 3 0.593618725407
loss_train 3 0.5953300862
loss_valid 4 0.535621889085
loss_train 4 0.537758282654
loss_valid 5 0.494513701658
loss_train 5 0.496321306992
loss_valid 6 0.462626434188
loss_train 6 0.463528320198
loss_valid 7 0.43446958602
loss_train 7 0.43446659138
loss_valid 8 0.409682798495
loss_train 8 0.408621085103
loss_valid 9 0.389300526574
loss_train 9 0.387245067236
loss_valid 10 0.37035043194
loss_train 10 0.368269249254
loss_valid 11 0.353123747973
loss_train 11 0.351457049208
loss_valid 12 0.337894946827
loss_train 12 0.337135973876
loss_valid 13 0.325986117649
loss_train 13 0.325766209607
loss_valid 14 0.315761447752
loss_train 14 0.316364536201
loss_valid 15 0.307001065813
loss_train 15 0.307384733683
loss_valid 16 0.298761343813
loss_train 16 0.299946109587
loss_valid 17 0.292491133466
loss_train 17 0.293609835912
loss_valid 18 0.286834274153
loss_train 18 0.288115403274
loss_valid 19 0.281906850852
loss_train 19 0.282786255125
loss_valid 20 0.277331155021
loss_train 20 0.277222973969
loss_valid 21 0.272004452407
loss_train 21 0.272126795958
loss_valid 22 0.270146039349
loss_train 22 0.268846181218
loss_valid 23 0.266112019696
loss_train 23 0.265141217435
loss_valid 24 0.263647966312
loss_train 24 0.262187399648
loss_valid 25 0.261030582978
loss_train 25 0.259952784143
loss_valid 26 0.257654642707
loss_train 26 0.25691151353
loss_valid 27 0.255948348635
loss_train 27 0.25411651389
loss_valid 28 0.254464542131
loss_train 28 0.252832620042
loss_valid 29 0.252330952994
loss_train 29 0.250363419605
loss_valid 30 0.251780759815
loss_train 30 0.249364928842
loss_valid 31 0.248739976645
loss_train 31 0.246144737161
loss_valid 32 0.24912506826
loss_train 32 0.246710797819
last epoch loss:  0.246144737161
current epoch loss:  0.246710797819
early stop at the end of epoch:  32
[0.3019376992886927, 0, 0, 0.86166298749080206, 0.79212656364974243, 0.74736325729703212, 0.72185430463576161, 0.5930831493745401, 0.8616629874908021, 0.7921265636497424, 0.7473632572970321, 0.7218543046357616, 0.5930831493745401, 8154.0, 8154.0, 8154.0, 8154.0, 8154.0, 0, 0.73764224145710588, 0.53075791022810892, nan, 0.9432261208576999, 0.9361233480176211, 0.9191407192855419, 0.9259615384615385, 0.5740009866798224, nan, 0.9175408636252744, 0.938337801608579, 0.9305589160416162, 0.5911100196463654, 0.5875831485587583, nan, 0.9344341913550267, 0.9629272989889264, 0.6189290161892902, 0.5926017262638718, 0.6074257425742574, nan, 0.9780950036918533, 0.6018009004502252, 0.5870807453416149, 0.5611888111888111, 0.5658119658119658, nan, 0, 4102, 4084, 4141, 4158, 4052, 0, 4097, 4101, 4131, 4070, 4057, 0, 4116, 4152, 4013, 4053, 4038, 0, 4061, 3996, 4023, 4002, 4093, 0, 0.74316069601067181]
N, d, L:  29353 9 5
loss_valid 0 0.908077665757
loss_train 0 0.907593782171
loss_valid 1 0.793433861585
loss_train 1 0.79274078723
loss_valid 2 0.682704952027
loss_train 2 0.681945544096
loss_valid 3 0.597145766857
loss_train 3 0.596404390266
loss_valid 4 0.535136483575
loss_train 4 0.534381472975
loss_valid 5 0.489495560473
loss_train 5 0.488261159412
loss_valid 6 0.454033352944
loss_train 6 0.452030781681
loss_valid 7 0.424621621598
loss_train 7 0.422050913678
loss_valid 8 0.3983603846
loss_train 8 0.395516122095
loss_valid 9 0.37460145361
loss_train 9 0.372069972303
loss_valid 10 0.354074490033
loss_train 10 0.351719299093
loss_valid 11 0.337572316643
loss_train 11 0.334492648647
loss_valid 12 0.323441434191
loss_train 12 0.319672058456
loss_valid 13 0.312739028491
loss_train 13 0.309344250661
loss_valid 14 0.304506745202
loss_train 14 0.300510523942
loss_valid 15 0.295451535723
loss_train 15 0.292778799508
loss_valid 16 0.290084589578
loss_train 16 0.286548671234
loss_valid 17 0.285051861872
loss_train 17 0.282026574405
loss_valid 18 0.280300718556
loss_train 18 0.277279266018
loss_valid 19 0.275831709775
loss_train 19 0.273270649562
loss_valid 20 0.272875465764
loss_train 20 0.269995788919
loss_valid 21 0.269891667034
loss_train 21 0.267043835542
loss_valid 22 0.265827185955
loss_train 22 0.263268603971
loss_valid 23 0.264068774017
loss_train 23 0.26089706719
loss_valid 24 0.262435534753
loss_train 24 0.259021838532
loss_valid 25 0.258725641829
loss_train 25 0.255812224939
loss_valid 26 0.258635134224
loss_train 26 0.255160117626
loss_valid 27 0.255215597769
loss_train 27 0.252276834242
loss_valid 28 0.252691275493
loss_train 28 0.250127158867
loss_valid 29 0.250884147995
loss_train 29 0.248470347838
loss_valid 30 0.250051516214
loss_train 30 0.247391816289
loss_valid 31 0.249034934696
loss_train 31 0.246094742258
loss_valid 32 0.247920890695
loss_train 32 0.245019248684
loss_valid 33 0.247989863122
loss_train 33 0.243920411282
early stop at the end of epoch:  33
[0.2858719646799117, 0, 0, 0.91182241844493506, 0.81996566102526369, 0.71731665440274717, 0.67046848172675988, 0.53053715967623249, 0.9118224184449351, 0.8199656610252637, 0.7173166544027472, 0.6704684817267599, 0.5305371596762325, 8154.0, 8154.0, 8154.0, 8154.0, 8154.0, 0, 0.7179632320894942, 0.46553838606818737, nan, 0.9336759667156143, 0.9740132385388576, 0.9614629356897398, 0.9466467215158315, 0.5348722986247544, nan, 0.9195260429523574, 0.9694105943795076, 0.9400599400599401, 0.4682520225545477, 0.5237399561723886, nan, 0.9353490658800393, 0.9812113720642769, 0.517384916748286, 0.5160744500846024, 0.5481662591687042, nan, 0.9824046920821115, 0.5196527610320714, 0.5151661049590756, 0.4660831509846827, 0.5277914412198721, nan, 0, 4084, 4077, 4072, 4009, 4070, 0, 4049, 4019, 4002, 4077, 4105, 0, 4066, 4043, 4082, 4135, 4088, 0, 4090, 4145, 4152, 4111, 4064, 0, 0.69968133284457745]
N, d, L:  29354 9 5
loss_valid 0 0.918905061161
loss_train 0 0.917303184267
loss_valid 1 0.807872886305
loss_train 1 0.80562653821
loss_valid 2 0.697560739339
loss_train 2 0.695975932149
loss_valid 3 0.611522402301
loss_train 3 0.610337273776
loss_valid 4 0.548238233032
loss_train 4 0.546893517761
loss_valid 5 0.500661439379
loss_train 5 0.498494003174
loss_valid 6 0.463744045833
loss_train 6 0.460926940868
loss_valid 7 0.433241247985
loss_train 7 0.430361704787
loss_valid 8 0.406856353557
loss_train 8 0.403875933849
loss_valid 9 0.38315554106
loss_train 9 0.380421110397
loss_valid 10 0.360699087553
loss_train 10 0.35866865266
loss_valid 11 0.342124619062
loss_train 11 0.339511012679
loss_valid 12 0.323804783594
loss_train 12 0.321361077031
loss_valid 13 0.311024212643
loss_train 13 0.307818009825
loss_valid 14 0.301733827569
loss_train 14 0.298339440871
loss_valid 15 0.295118131637
loss_train 15 0.291019523103
loss_valid 16 0.286831695261
loss_train 16 0.284414325402
loss_valid 17 0.28090794098
loss_train 17 0.277714838036
loss_valid 18 0.275904462922
loss_train 18 0.272880795194
loss_valid 19 0.271998314364
loss_train 19 0.268592511026
loss_valid 20 0.267690933892
loss_train 20 0.263993681058
loss_valid 21 0.264628150068
loss_train 21 0.260765884929
loss_valid 22 0.261886218309
loss_train 22 0.257659381466
loss_valid 23 0.259877866581
loss_train 23 0.255274092311
loss_valid 24 0.2573396703
loss_train 24 0.252311551887
loss_valid 25 0.256242051639
loss_train 25 0.250111993128
loss_valid 26 0.253601873205
loss_train 26 0.247289834133
loss_valid 27 0.250937207438
loss_train 27 0.244918023893
loss_valid 28 0.249584409901
loss_train 28 0.242843545542
loss_valid 29 0.249140342305
loss_train 29 0.241097705382
loss_valid 30 0.245212214564
loss_train 30 0.239643996887
loss_valid 31 0.244283441898
loss_train 31 0.237878119461
loss_valid 32 0.243684501962
loss_train 32 0.236801354976
loss_valid 33 0.242891236643
loss_train 33 0.234470467202
loss_valid 34 0.243659926219
loss_train 34 0.234156036837
early stop at the end of epoch:  34
[0.24628970930945665, 0, 0, 0.9125475285171103, 0.78363792469029803, 0.7149515515761069, 0.664295351404391, 0.5119587881761315, 0.9125475285171103, 0.783637924690298, 0.7149515515761069, 0.664295351404391, 0.5119587881761315, 8153.0, 8153.0, 8153.0, 8153.0, 8153.0, 0, 0.70477218169698841, 0.42843125229976692, nan, 0.9644218551461246, 0.9473684210526315, 0.9400244798041616, 0.9700073152889539, 0.4597347228801516, nan, 0.9526042952357443, 0.9212092358962152, 0.942795076031861, 0.4669411479777186, 0.47613248904042865, nan, 0.9420009666505558, 0.9720697063738363, 0.48330058939096265, 0.5005055611729019, 0.47997014182632497, nan, 0.9661016949152542, 0.4477317554240631, 0.48405580468360737, 0.4571572580645161, 0.48472808542339213, nan, 0, 3933, 4026, 4083, 4099, 4220, 0, 4049, 4199, 4141, 4127, 4104, 0, 4136, 4187, 4070, 3954, 4017, 0, 4128, 4054, 4012, 3966, 4025, 0, 0.67150420279367573]
N, d, L:  29354 9 5
loss_valid 0 0.905226163607
loss_train 0 0.90359676013
loss_valid 1 0.794136619397
loss_train 1 0.791309854692
loss_valid 2 0.686588647598
loss_train 2 0.682762169039
loss_valid 3 0.602233928115
loss_train 3 0.597600784068
loss_valid 4 0.540709379446
loss_train 4 0.536006445049
loss_valid 5 0.495058297473
loss_train 5 0./home/jasonzhang/anaconda2/lib/python2.7/site-packages/numpy/lib/nanfunctions.py:703: RuntimeWarning: Mean of empty slice
  warnings.warn("Mean of empty slice", RuntimeWarning)
/home/jasonzhang/anaconda2/lib/python2.7/site-packages/numpy/lib/nanfunctions.py:1202: RuntimeWarning: Degrees of freedom <= 0 for slice.
  warnings.warn("Degrees of freedom <= 0 for slice.", RuntimeWarning)
490702028578
loss_valid 6 0.457526164809
loss_train 6 0.453887386869
loss_valid 7 0.424901540347
loss_train 7 0.42143773918
loss_valid 8 0.395696632757
loss_train 8 0.392701668302
loss_valid 9 0.373534145698
loss_train 9 0.370381932625
loss_valid 10 0.356495606822
loss_train 10 0.352916526878
loss_valid 11 0.342385908464
loss_train 11 0.338871900471
loss_valid 12 0.331462016831
loss_train 12 0.327766379718
loss_valid 13 0.321011049183
loss_train 13 0.317795820925
loss_valid 14 0.313376310028
loss_train 14 0.309714152861
loss_valid 15 0.307981580415
loss_train 15 0.303392045864
loss_valid 16 0.301846081896
loss_train 16 0.297672622093
loss_valid 17 0.296984870317
loss_train 17 0.292711404179
loss_valid 18 0.293061333075
loss_train 18 0.287945873005
loss_valid 19 0.287364180024
loss_train 19 0.282593345034
loss_valid 20 0.281934079383
loss_train 20 0.27710768011
loss_valid 21 0.276758948208
loss_train 21 0.272742571604
loss_valid 22 0.273032048603
loss_train 22 0.268737916588
loss_valid 23 0.270913720649
loss_train 23 0.266668766785
loss_valid 24 0.267981593098
loss_train 24 0.263850419623
loss_valid 25 0.267303963406
loss_train 25 0.262728599762
loss_valid 26 0.264258374234
loss_train 26 0.260343953096
loss_valid 27 0.263482653216
loss_train 27 0.2591369757
loss_valid 28 0.261230326769
loss_train 28 0.256784180233
loss_valid 29 0.259675480326
loss_train 29 0.255806630199
loss_valid 30 0.259927618061
loss_train 30 0.254872106305
early stop at the end of epoch:  30
[0.37213295719367107, 0, 0, 0.86483503005028828, 0.83944560284557834, 0.75285171102661597, 0.75763522629706859, 0.66135164969949711, 0.8648350300502883, 0.8394456028455783, 0.752851711026616, 0.7576352262970686, 0.6613516496994971, 8153.0, 8153.0, 8153.0, 8153.0, 8153.0, 0, 0.7718014223171682, 0.63490739605053348, nan, 0.8940771688375522, 0.9641913001682287, 0.9262851600387972, 0.9425202652910832, 0.7311643835616438, nan, 0.9172714078374455, 0.916501976284585, 0.9629539361442847, 0.6736736736736737, 0.7133979617201094, nan, 0.9318987341772151, 0.8966808085849763, 0.7118770146293082, 0.7157459235823801, 0.7176135013073449, nan, 0.9591439688715954, 0.6938325991189427, 0.6677355698075974, 0.7269879518072289, 0.6793572311495674, nan, 0, 4067, 4159, 4122, 4069, 4086, 0, 4132, 4046, 4101, 3994, 4021, 0, 3948, 4005, 4031, 4107, 4205, 0, 4110, 4084, 4052, 4148, 4043, 0, 0.80881987948034517]
{'perf': [array([  2.75806191e-01,   0.00000000e+00,   0.00000000e+00,
         8.98866259e-01,   8.07103828e-01,   7.21620500e-01,
         6.76905125e-01,   5.41014798e-01,   8.98866259e-01,
         8.07103828e-01,   7.21620500e-01,   6.76905125e-01,
         5.41014798e-01,   8.15360000e+03,   8.15360000e+03,
         8.15360000e+03,   8.15360000e+03,   8.15360000e+03,
         0.00000000e+00,   7.17037764e-01,   4.70028534e-01,
                    nan,   9.40267570e-01,   9.60306401e-01,
         9.42138410e-01,   9.52294607e-01,   5.26901119e-01,
                    nan,   9.33968458e-01,   9.37624478e-01,
         9.46677300e-01,   5.02975329e-01,   5.27789524e-01,
                    nan,   9.41688133e-01,   9.57536232e-01,
         5.32457443e-01,   5.34357130e-01,   5.36646917e-01,
                    nan,   9.71390819e-01,   5.12540204e-01,
         5.17780788e-01,   5.04415065e-01,   5.15039803e-01,
                    nan,   0.00000000e+00,   4.05880000e+03,
         4.07220000e+03,   4.10040000e+03,   4.07800000e+03,
         4.09480000e+03,   0.00000000e+00,   4.08440000e+03,
         4.09580000e+03,   4.07960000e+03,   4.08140000e+03,
         4.06920000e+03,   0.00000000e+00,   4.07000000e+03,
         4.09420000e+03,   4.05320000e+03,   4.05780000e+03,
         4.08360000e+03,   0.00000000e+00,   4.08320000e+03,
         4.07560000e+03,   4.07400000e+03,   4.05940000e+03,
         4.07040000e+03,   0.00000000e+00,   6.96777789e-01]), array([  6.56426728e-02,   0.00000000e+00,   0.00000000e+00,
         3.12606653e-02,   2.01598003e-02,   2.76303597e-02,
         6.34173208e-02,   8.46179106e-02,   3.12606653e-02,
         2.01598003e-02,   2.76303597e-02,   6.34173208e-02,
         8.46179106e-02,   4.89897949e-01,   4.89897949e-01,
         4.89897949e-01,   4.89897949e-01,   4.89897949e-01,
         0.00000000e+00,   3.91777771e-02,   1.13887060e-01,
                    nan,   2.61843128e-02,   1.63497686e-02,
         1.80356864e-02,   1.85075819e-02,   1.30725095e-01,
                    nan,   1.97055875e-02,   1.87182839e-02,
         1.17518976e-02,   1.22296549e-01,   1.23850704e-01,
                    nan,   1.20078223e-02,   3.09901303e-02,
         1.28825277e-01,   1.20772877e-01,   1.29446585e-01,
                    nan,   8.29752756e-03,   1.34456919e-01,
         1.11257409e-01,   1.37117705e-01,   1.18048584e-01,
                    nan,   0.00000000e+00,   6.45210043e+01,
         5.11914055e+01,   2.64317990e+01,   4.94206435e+01,
         6.41573067e+01,   0.00000000e+00,   3.17590932e+01,
         6.22684511e+01,   5.67365843e+01,   5.13521178e+01,
         3.18395980e+01,   0.00000000e+00,   6.56780024e+01,
         6.72826872e+01,   2.64680940e+01,   6.24416528e+01,
         6.55090833e+01,   0.00000000e+00,   3.58407589e+01,
         4.94716080e+01,   5.70298168e+01,   6.72951707e+01,
         3.62414128e+01,   0.00000000e+00,   8.22957354e-02])]}
