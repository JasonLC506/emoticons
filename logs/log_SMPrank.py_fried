/home/jasonzhang/anaconda2/lib/python2.7/site-packages/numpy/ma/core.py:867: RuntimeWarning: invalid value encountered in less_equal
  return umath.less_equal(x, self.critical_value)
N, d, L:  29353 9 5
loss_valid 0 0.945685105989
loss_train 0 0.944543718203
loss_valid 1 0.832475380688
loss_train 1 0.830369348842
loss_valid 2 0.73278733561
loss_train 2 0.729854647097
loss_valid 3 0.647586676672
loss_train 3 0.644057143547
loss_valid 4 0.582186933724
loss_train 4 0.578503781997
loss_valid 5 0.532772089962
loss_train 5 0.52970997867
loss_valid 6 0.494763829561
loss_train 6 0.492313435259
loss_valid 7 0.463881826149
loss_train 7 0.461793768618
loss_valid 8 0.43679473197
loss_train 8 0.434772622638
loss_valid 9 0.412421970773
loss_train 9 0.409416593577
loss_valid 10 0.389322472235
loss_train 10 0.385234646884
loss_valid 11 0.367374611129
loss_train 11 0.362728849697
loss_valid 12 0.347270882737
loss_train 12 0.342114177052
loss_valid 13 0.330366914363
loss_train 13 0.325101766443
loss_valid 14 0.316877966035
loss_train 14 0.311100328348
loss_valid 15 0.304211488391
loss_train 15 0.298369738203
loss_valid 16 0.292977131304
loss_train 16 0.287467554892
loss_valid 17 0.284366926261
loss_train 17 0.278753872847
loss_valid 18 0.276719409736
loss_train 18 0.271271397708
loss_valid 19 0.270702597705
loss_train 19 0.264208694828
loss_valid 20 0.266098106939
loss_train 20 0.259260526252
loss_valid 21 0.261122270926
loss_train 21 0.254551694313
loss_valid 22 0.256281777854
loss_train 22 0.250097742602
loss_valid 23 0.252366903637
loss_train 23 0.246285632621
loss_valid 24 0.249217131626
loss_train 24 0.243032920984
loss_valid 25 0.246051020928
loss_train 25 0.240177414129
loss_valid 26 0.243373402732
loss_train 26 0.237293235379
loss_valid 27 0.241133880417
loss_train 27 0.234629710568
loss_valid 28 0.23891026512
loss_train 28 0.231942504525
loss_valid 29 0.235183394833
loss_train 29 0.229492636892
loss_valid 30 0.233302002979
loss_train 30 0.227005058135
loss_valid 31 0.23004113544
loss_train 31 0.224254262257
loss_valid 32 0.228589567785
loss_train 32 0.222778245955
loss_valid 33 0.226104216236
loss_train 33 0.220323908836
loss_valid 34 0.22421179394
loss_train 34 0.218644120659
loss_valid 35 0.221933472225
loss_train 35 0.21671985039
loss_valid 36 0.221185510991
loss_train 36 0.215581903548
loss_valid 37 0.219892344023
loss_train 37 0.213371986335
loss_valid 38 0.217505797177
loss_train 38 0.211337692724
loss_valid 39 0.214689610056
loss_train 39 0.20898024476
loss_valid 40 0.214439840447
loss_train 40 0.208296541677
loss_valid 41 0.213536299025
loss_train 41 0.207668864239
loss_valid 42 0.210920543425
loss_train 42 0.206907598001
loss_valid 43 0.210455761005
loss_train 43 0.205303277675
loss_valid 44 0.208946782319
loss_train 44 0.203554246201
loss_valid 45 0.208710414997
loss_train 45 0.203423290244
loss_valid 46 0.209039845657
loss_train 46 0.203334545605
early stop at the end of epoch:  46
[0.19487368162864852, 0, 0, 0.95425558008339462, 0.80340936963453524, 0.67696835908756436, 0.55984792739759626, 0.40679421142997302, 0.9542555800833946, 0.8034093696345352, 0.6769683590875644, 0.5598479273975963, 0.406794211429973, 8154.0, 8154.0, 8154.0, 8154.0, 8154.0, 0, 0.65241353455371187, 0.26620063772381652, nan, 0.9625304136253041, 0.9815782922579039, 0.9701419481155164, 0.9923588858762632, 0.28631422924901184, nan, 0.9636319257993654, 0.9645286686103013, 0.9659627329192546, 0.2779521854624487, 0.29623245506032997, nan, 0.9777288301517376, 0.9862946647087616, 0.2843811394891945, 0.30059376546264227, 0.2973968565815324, nan, 0.9930503847108464, 0.2779809802487198, 0.30220179046697315, 0.293713163064833, 0.30031484620973603, nan, 0, 4108, 4015, 4084, 4055, 4046, 0, 4095, 4114, 4023, 4139, 4059, 0, 4084, 4084, 4070, 4040, 4070, 0, 4027, 4099, 4131, 4070, 4127, 0, 0.53337798072934284]
N, d, L:  29353 9 5
loss_valid 0 0.945517157507
loss_train 0 0.945632077841
loss_valid 1 0.831727902293
loss_train 1 0.8322527096
loss_valid 2 0.73154784052
loss_train 2 0.732358244413
loss_valid 3 0.646653189215
loss_train 3 0.647217829743
loss_valid 4 0.581437231437
loss_train 4 0.581092063963
loss_valid 5 0.532736297378
loss_train 5 0.530850508865
loss_valid 6 0.495135348727
loss_train 6 0.491732948679
loss_valid 7 0.464409603952
loss_train 7 0.459641714067
loss_valid 8 0.437421145834
loss_train 8 0.431444757734
loss_valid 9 0.411648574073
loss_train 9 0.405039529487
loss_valid 10 0.386544722779
loss_train 10 0.379412147699
loss_valid 11 0.362705886837
loss_train 11 0.355803758552
loss_valid 12 0.343153712577
loss_train 12 0.335784110029
loss_valid 13 0.325697951914
loss_train 13 0.318194980929
loss_valid 14 0.311013850171
loss_train 14 0.303518665792
loss_valid 15 0.30016010283
loss_train 15 0.292782652451
loss_valid 16 0.290975131153
loss_train 16 0.283661667722
loss_valid 17 0.284543088381
loss_train 17 0.276829448853
loss_valid 18 0.277787447071
loss_train 18 0.2702173004
loss_valid 19 0.271391704515
loss_train 19 0.263754276107
loss_valid 20 0.265935265542
loss_train 20 0.258356938372
loss_valid 21 0.260990249872
loss_train 21 0.253879951021
loss_valid 22 0.257282551531
loss_train 22 0.25041400888
loss_valid 23 0.254435300171
loss_train 23 0.24690928788
loss_valid 24 0.250735093849
loss_train 24 0.24321681828
loss_valid 25 0.247344310204
loss_train 25 0.239717481581
loss_valid 26 0.24502321683
loss_train 26 0.236700321546
loss_valid 27 0.241731046614
loss_train 27 0.233897322865
loss_valid 28 0.240283355667
loss_train 28 0.231900686725
loss_valid 29 0.23762785881
loss_train 29 0.229790334907
loss_valid 30 0.236682600718
loss_train 30 0.227653834287
loss_valid 31 0.233520243257
loss_train 31 0.224991426015
loss_valid 32 0.231454613612
loss_train 32 0.223313423436
loss_valid 33 0.23114101789
loss_train 33 0.221913913902
loss_valid 34 0.228587670815
loss_train 34 0.219888329158
loss_valid 35 0.227464931078
loss_train 35 0.218656754267
loss_valid 36 0.225178028377
loss_train 36 0.217382832596
loss_valid 37 0.225340298634
loss_train 37 0.216110163129
early stop at the end of epoch:  37
[0.37110620554329166, 0, 0, 0.87821927888153051, 0.85172921265636492, 0.79617365710080945, 0.78403237674760851, 0.67120431689968119, 0.8782192788815305, 0.8517292126563649, 0.7961736571008095, 0.7840323767476085, 0.6712043168996812, 8154.0, 8154.0, 8154.0, 8154.0, 8154.0, 0, 0.79290327267984773, 0.6617120431689969, nan, 0.9600389863547758, 0.9202153695545766, 0.9193820902727492, 0.9540865384615385, 0.7035027133695116, nan, 0.9434008294706026, 0.9529612478674141, 0.9583837406242439, 0.7212671905697446, 0.7006651884700665, nan, 0.9543467702768335, 0.9475204622051036, 0.696637608966376, 0.7491985203452528, 0.7324257425742574, nan, 0.9783411272458774, 0.6945972986493246, 0.7118012422360248, 0.704045954045954, 0.6893772893772894, nan, 0, 4102, 4084, 4141, 4158, 4052, 0, 4097, 4101, 4131, 4070, 4057, 0, 4116, 4152, 4013, 4053, 4038, 0, 4061, 3996, 4023, 4002, 4093, 0, 0.82080009312744318]
N, d, L:  29353 9 5
loss_valid 0 0.938592152307
loss_train 0 0.938206731736
loss_valid 1 0.825510865279
loss_train 1 0.824217216804
loss_valid 2 0.726454903132
loss_train 2 0.724005667639
loss_valid 3 0.64264958328
loss_train 3 0.639303041172
loss_valid 4 0.578256514157
loss_train 4 0.574412432318
loss_valid 5 0.530099552968
loss_train 5 0.525974002675
loss_valid 6 0.493265320964
loss_train 6 0.488850376273
loss_valid 7 0.463421367253
loss_train 7 0.458543622451
loss_valid 8 0.437566884042
loss_train 8 0.431852330447
loss_valid 9 0.412692091042
loss_train 9 0.406939631951
loss_valid 10 0.388598617393
loss_train 10 0.383566441195
loss_valid 11 0.36744903749
loss_train 11 0.362737987744
loss_valid 12 0.349690338704
loss_train 12 0.345041433307
loss_valid 13 0.335443028729
loss_train 13 0.329998695227
loss_valid 14 0.323178192219
loss_train 14 0.318090783911
loss_valid 15 0.313947194373
loss_train 15 0.308818114165
loss_valid 16 0.305816356823
loss_train 16 0.3005597611
loss_valid 17 0.29848748518
loss_train 17 0.293206197895
loss_valid 18 0.290518082222
loss_train 18 0.286043687648
loss_valid 19 0.283724032666
loss_train 19 0.279174212862
loss_valid 20 0.278504968152
loss_train 20 0.2738757298
loss_valid 21 0.273809991544
loss_train 21 0.269041481035
loss_valid 22 0.268852110428
loss_train 22 0.263759015684
loss_valid 23 0.265217148706
loss_train 23 0.259743629057
loss_valid 24 0.26138651992SMPrank.py:89: UserWarning: training set loss increase
  warnings.warn("training set loss increase")
8
loss_train 24 0.255692841845
loss_valid 25 0.257998827036
loss_train 25 0.251959784105
loss_valid 26 0.254467746468
loss_train 26 0.248560452394
loss_valid 27 0.251426538796
loss_train 27 0.245831579734
loss_valid 28 0.248971288745
loss_train 28 0.243407942343
loss_valid 29 0.247222022166
loss_train 29 0.241523052971
loss_valid 30 0.244934516718
loss_train 30 0.238737209953
loss_valid 31 0.24262651139
loss_train 31 0.2373787731
loss_valid 32 0.240791012704
loss_train 32 0.234699971475
loss_valid 33 0.240231657762
loss_train 33 0.232730154535
loss_valid 34 0.237259079573
loss_train 34 0.231365122686
loss_valid 35 0.237910279247
loss_train 35 0.230874060844
early stop at the end of epoch:  35
[0.5165562913907285, 0, 0, 0.86411577140053963, 0.83541819965661024, 0.82622025999509441, 0.75472160902624474, 0.7512877115526122, 0.8641157714005396, 0.8354181996566102, 0.8262202599950944, 0.7547216090262447, 0.7512877115526122, 8154.0, 8154.0, 8154.0, 8154.0, 8154.0, 0, 0.80506872350372538, 0.71179789060583776, nan, 0.956436612824278, 0.942878156410885, 0.9042709867452136, 0.9162303664921466, 0.7595776031434185, nan, 0.9136015798568254, 0.9492663516538175, 0.9500499500499501, 0.7862221132630547, 0.7832968103238374, nan, 0.9643559488692232, 0.9332509270704573, 0.7752203721841332, 0.7715736040609137, 0.7618581907090465, nan, 0.9171554252199413, 0.8037135278514589, 0.7628791526239769, 0.7850717238025772, 0.7875061485489424, nan, 0, 4084, 4077, 4072, 4009, 4070, 0, 4049, 4019, 4002, 4077, 4105, 0, 4066, 4043, 4082, 4135, 4088, 0, 4090, 4145, 4152, 4111, 4064, 0, 0.85245467180535384]
N, d, L:  29354 9 5
loss_valid 0 0.945969933013
loss_train 0 0.945838382801
loss_valid 1 0.830904156718
loss_train 1 0.830284254122
loss_valid 2 0.727608519886
loss_train 2 0.726379908449
loss_valid 3 0.640996545773
loss_train 3 0.639292977681
loss_valid 4 0.575909256217
loss_train 4 0.57362949454
loss_valid 5 0.528061299598
loss_train 5 0.524965913807
loss_valid 6 0.492008669296
loss_train 6 0.487717726149
loss_valid 7 0.462985278044
loss_train 7 0.457339080917
loss_valid 8 0.437922332233
loss_train 8 0.430544546851
loss_valid 9 0.414070144488
loss_train 9 0.405367845115
loss_valid 10 0.390822249649
loss_train 10 0.380851780104
loss_valid 11 0.367885577237
loss_train 11 0.358346320235
loss_valid 12 0.348552638206
loss_train 12 0.339152107438
loss_valid 13 0.331244712057
loss_train 13 0.322252695881
loss_valid 14 0.315684393572
loss_train 14 0.307863601806
loss_valid 15 0.303933342969
loss_train 15 0.296657922443
loss_valid 16 0.29476674698
loss_train 16 0.287998833194
loss_valid 17 0.288116321133
loss_train 17 0.28132659181
loss_valid 18 0.281839154047
loss_train 18 0.275131579647
loss_valid 19 0.275739203753
loss_train 19 0.269010710341
loss_valid 20 0.269898474128
loss_train 20 0.263883805747
loss_valid 21 0.26411071578
loss_train 21 0.258981273545
loss_valid 22 0.259150061517
loss_train 22 0.254630594266
loss_valid 23 0.25459677268
loss_train 23 0.249602223404
loss_valid 24 0.24980407006
loss_train 24 0.245425808489
loss_valid 25 0.246311242286
loss_train 25 0.242295229867
loss_valid 26 0.242585462207
loss_train 26 0.238990102705
loss_valid 27 0.239497707618
loss_train 27 0.235860366179
loss_valid 28 0.236712322638
loss_train 28 0.233152234095
loss_valid 29 0.232197371909
loss_train 29 0.230041775967
loss_valid 30 0.230076520235
loss_train 30 0.227240676321
loss_valid 31 0.225302827138
loss_train 31 0.22362067201
loss_valid 32 0.22159577818
loss_train 32 0.22010064674
loss_valid 33 0.218537324231
loss_train 33 0.217504891888
loss_valid 34 0.216974743282
loss_train 34 0.215145692664
loss_valid 35 0.2169507821
loss_train 35 0.214419667686
loss_valid 36 0.215525176203
loss_train 36 0.212705539015
loss_valid 37 0.212260241975
loss_train 37 0.210836738697
loss_valid 38 0.211902321892
loss_train 38 0.210983712525
last epoch loss:  0.210836738697
current epoch loss:  0.210983712525
loss_valid 39 0.210345914013
loss_train 39 0.209957053425
loss_valid 40 0.209833241955
loss_train 40 0.208461824302
loss_valid 41 0.208654265505
loss_train 41 0.207523091247
loss_valid 42 0.210155933942
loss_train 42 0.208372336577
last epoch loss:  0.207523091247
current epoch loss:  0.208372336577
early stop at the end of epoch:  42
[0.2955967128664295, 0, 0, 0.95155157610695451, 0.79676192812461666, 0.7010916227155648, 0.65117134797007237, 0.50042928983196366, 0.9515515761069545, 0.7967619281246167, 0.7010916227155648, 0.6511713479700724, 0.5004292898319637, 8153.0, 8153.0, 8153.0, 8153.0, 8153.0, 0, 0.70423034982398103, 0.42354961363915122, nan, 0.9799237611181703, 0.9486097318768619, 0.9931456548347614, 0.9785418190685199, 0.45405021316911415, nan, 0.9674154529745742, 0.9362056653177815, 0.941346850108617, 0.4608864131751029, 0.45104724792985873, nan, 0.9386176897051716, 0.9651468130818811, 0.47961689587426326, 0.4595551061678463, 0.46205523762129885, nan, 0.9765133171912833, 0.42480276134122286, 0.4828101644245142, 0.43170362903225806, 0.47628507573876333, nan, 0, 3933, 4026, 4083, 4099, 4220, 0, 4049, 4199, 4141, 4127, 4104, 0, 4136, 4187, 4070, 3954, 4017, 0, 4128, 4054, 4012, 3966, 4025, 0, 0.66383696580425278]
N, d, L:  29354 9 5
loss_valid 0 0.948573915679
loss_train 0 0.947633556202
loss_valid 1 0.83259729851
loss_train 1 0.830673237637
loss_valid 2 0.733198984274
loss_train 2 0.730105682195
loss_valid 3 0.649868118072
loss_train 3 0.645461873042
loss_valid 4 0.586309243908
loss_train 4 0.580948701581
loss_valid 5 0.538496604065
loss_train 5 0.532612740503
loss_valid 6 0.500801043277
loss_train 6 0.494925340429
loss_valid 7 0.469368727962
loss_train 7 0.463789257125
loss_valid 8 0.442025303633
loss_train 8 0.436487970723
loss_valid 9 0.417034096238
loss_train 9 0.411091491331
loss_valid 10 0.393494677556
loss_train 10 0.387148527022
loss_valid 11 0.37228198907
loss_train 11 0.365948239578
loss_valid 12 0.352955557773
loss_train 12 0.346633866433
loss_valid 13 0.335647089903
loss_train 13 0.329783049806
loss_valid 14 0.321870309084
loss_train 14 0.316233127915
loss_valid 15 0.309654367919
loss_train 15 0.30468934944
loss_valid 16 0.29970052753
loss_train 16 0.294920762052
loss_valid 17 0.291616011199
loss_train 17 0.287166928754
loss_valid 18 0.285156629887
loss_train 18 0.280695989329
loss_valid 19 0.27957129889
loss_train 19 0.274807204593
loss_valid 20 0.272918872438
loss_train 20 0.269096845243
loss_valid 21 0.267711580515
loss_train 21 0.263383147012
loss_valid 22 0.261040408002
loss_train 22 0.257405086661
loss_valid 23 0.255382220621
loss_train 23 0.251873695656
loss_valid 24 0.250073740061
loss_train 24 0.24717077324
loss_valid 25 0.246696162599
loss_train 25 0.243602079334
loss_valid 26 0.24284350445
loss_train 26 0.239568833208
loss_valid 27 0.240241627926
loss_train 27 0.236764426124
loss_valid 28 0.236824405075
loss_train 28 0.233742708562
loss_valid 29 0.234190538558
loss_train 29 0.230656502487
loss_valid 30 0.231387308476
loss_train 30 0.227870934493
loss_valid 31 0.229755475715
loss_train 31 0.226023711634
loss_valid 32 0.226451461188
loss_train 32 0.222823216291
loss_valid 33 0.22473061296
loss_train 33 0.220839740722
loss_valid 34 0.22355390717
loss_train 34 0.219399621241
loss_valid 35 0.223293257231
loss_train 35 0.217977396403
loss_valid 36 0.221202511968
loss_train 36 0.216270221744
loss_valid 37 0.220705682025
loss_train 37 0.214880302666
loss_valid 38 0.218187742361
loss_train 38 0.212426004002
loss_valid 39 0.217251747703
loss_train 39 0.21170535559
loss_valid 40 0.214349495999
loss_train 40 0.210291247341
loss_valid 41 0.213524478432
loss_train 41 0.207893645395
loss_valid 42 0.213722124393
loss_train 42 0.206682119293
early stop at the end of epoch:  42
[0.33840304182509506, 0, 0, 0.94345639641849621, 0.8114804366490862, 0.72881148043664912, 0.63056543603581505, 0.54029191708573532, 0.9434563964184962, 0.8114804366490862, 0.7288114804366491, 0.630565436035815, 0.5402919170857353, 8153.0, 8153.0, 8153.0, 8153.0, 8153.0, 0, 0.71745512942489997, 0.43603581503740957, nan, 0.9820594740722536, 0.9548185532323961, 0.9667798254122212, 0.9832964873495456, 0.46208414872798437, nan, 0.9339622641509434, 0.9765316205533597, 0.9546673165976115, 0.46096096096096095, 0.47377/home/jasonzhang/anaconda2/lib/python2.7/site-packages/numpy/lib/nanfunctions.py:703: RuntimeWarning: Mean of empty slice
  warnings.warn("Mean of empty slice", RuntimeWarning)
/home/jasonzhang/anaconda2/lib/python2.7/site-packages/numpy/lib/nanfunctions.py:1202: RuntimeWarning: Degrees of freedom <= 0 for slice.
  warnings.warn("Degrees of freedom <= 0 for slice.", RuntimeWarning)
578921203084, nan, 0.9346835443037975, 0.9830296980284502, 0.43937515497148527, 0.46653687028474083, 0.4865700023769907, nan, 0.9776264591439688, 0.4711209006363191, 0.4930932412432166, 0.46289156626506023, 0.4946847960444994, nan, 0, 4067, 4159, 4122, 4069, 4086, 0, 4132, 4046, 4101, 3994, 4021, 0, 3948, 4005, 4031, 4107, 4205, 0, 4110, 4084, 4052, 4148, 4043, 0, 0.67390989888768216]
{'perf': [array([  3.43307187e-01,   0.00000000e+00,   0.00000000e+00,
         9.18319721e-01,   8.19759829e-01,   7.45853076e-01,
         6.76067739e-01,   5.74001489e-01,   9.18319721e-01,
         8.19759829e-01,   7.45853076e-01,   6.76067739e-01,
         5.74001489e-01,   8.15360000e+03,   8.15360000e+03,
         8.15360000e+03,   8.15360000e+03,   8.15360000e+03,
         0.00000000e+00,   7.34414202e-01,   4.99859200e-01,
                    nan,   9.68197850e-01,   9.49620021e-01,
         9.50744101e-01,   9.64902819e-01,   5.33105782e-01,
                    nan,   9.44402410e-01,   9.55898711e-01,
         9.54082118e-01,   5.41457773e-01,   5.41003498e-01,
                    nan,   9.53946557e-01,   9.63048513e-01,
         5.35046234e-01,   5.49491573e-01,   5.48061206e-01,
                    nan,   9.68537343e-01,   5.34443094e-01,
         5.50557118e-01,   5.35485207e-01,   5.49633631e-01,
                    nan,   0.00000000e+00,   4.05880000e+03,
         4.07220000e+03,   4.10040000e+03,   4.07800000e+03,
         4.09480000e+03,   0.00000000e+00,   4.08440000e+03,
         4.09580000e+03,   4.07960000e+03,   4.08140000e+03,
         4.06920000e+03,   0.00000000e+00,   4.07000000e+03,
         4.09420000e+03,   4.05320000e+03,   4.05780000e+03,
         4.08360000e+03,   0.00000000e+00,   4.08320000e+03,
         4.07560000e+03,   4.07400000e+03,   4.05940000e+03,
         4.07040000e+03,   0.00000000e+00,   7.08875922e-01]), array([  1.04996079e-01,   0.00000000e+00,   0.00000000e+00,
         3.89196820e-02,   2.06495296e-02,   5.66218210e-02,
         8.25107614e-02,   1.22789472e-01,   3.89196820e-02,
         2.06495296e-02,   5.66218210e-02,   8.25107614e-02,
         1.22789472e-01,   4.89897949e-01,   4.89897949e-01,
         4.89897949e-01,   4.89897949e-01,   4.89897949e-01,
         0.00000000e+00,   5.71591066e-02,   1.64685915e-01,
                    nan,   1.06457053e-02,   1.97962764e-02,
         3.33905339e-02,   2.74378735e-02,   1.74656535e-01,
                    nan,   1.97892161e-02,   1.37158744e-02,
         8.22728208e-03,   1.86894454e-01,   1.77043757e-01,
                    nan,   1.60009372e-02,   2.03619777e-02,
         1.78243511e-01,   1.82276418e-01,   1.75334106e-01,
                    nan,   2.63944429e-02,   1.89715289e-01,
         1.67721453e-01,   1.81769265e-01,   1.71272220e-01,
                    nan,   0.00000000e+00,   6.45210043e+01,
         5.11914055e+01,   2.64317990e+01,   4.94206435e+01,
         6.41573067e+01,   0.00000000e+00,   3.17590932e+01,
         6.22684511e+01,   5.67365843e+01,   5.13521178e+01,
         3.18395980e+01,   0.00000000e+00,   6.56780024e+01,
         6.72826872e+01,   2.64680940e+01,   6.24416528e+01,
         6.55090833e+01,   0.00000000e+00,   3.58407589e+01,
         4.94716080e+01,   5.70298168e+01,   6.72951707e+01,
         3.62414128e+01,   0.00000000e+00,   1.15924575e-01])]}
