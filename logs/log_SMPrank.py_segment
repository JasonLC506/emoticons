SMPrank.py:290: RuntimeWarning: invalid value encountered in double_scalars
  self.probfeature[k] = self.probfeature[k] / sum_probf
/home/jasonzhang/anaconda2/lib/python2.7/site-packages/numpy/ma/core.py:867: RuntimeWarning: invalid value encountered in less_equal
  return umath.less_equal(x, self.critical_value)
N, d, L:  1664 18 7
loss_valid 0 1.57658178074
loss_train 0 1.51918877225
loss_valid 1 1.46913523674
loss_train 1 1.41442396697
loss_valid 2 1.36587997367
loss_train 2 1.31335186258
loss_valid 3 1.27162202595
loss_train 3 1.22070561396
loss_valid 4 1.18815067243
loss_train 4 1.13867420606
loss_valid 5 1.11407809669
loss_train 5 1.06572803764
loss_valid 6 1.0472672876
loss_train 6 0.999115260867
loss_valid 7 0.985546890261
loss_train 7 0.936412325552
loss_valid 8 0.927535864353
loss_train 8 0.876181079237
loss_valid 9 0.872446801686
loss_train 9 0.818090570473
loss_valid 10 0.820322667871
loss_train 10 0.763631894243
loss_valid 11 0.771181569637
loss_train 11 0.714107536466
loss_valid 12 nan
loss_train 12 0.668823564257
loss_valid 13 nan
loss_train 13 nan
loss_valid 14 nan
loss_train 14 nan
loss_valid 15 nan
loss_train 15 nan
loss_valid 16 nan
loss_train 16 nan
loss_valid 17 nan
loss_train 17 nan
loss_valid 18 nan
loss_train 18 nan
loss_valid 19 nan
loss_train 19 nan
loss_valid 20 nan
loss_train 20 nan
loss_valid 21 nan
loss_train 21 nan
loss_valid 22 nan
loss_train 22 nan
loss_valid 23 nan
loss_train 23 nan
loss_valid 24 nan
loss_train 24 nan
loss_valid 25 nan
loss_train 25 nan
loss_valid 26 nan
loss_train 26 nan
loss_valid 27 nan
loss_train 27 nan
loss_valid 28 nan
loss_train 28 nan
loss_valid 29 nan
loss_train 29 nan
loss_valid 30 nan
loss_train 30 nan
loss_valid 31 nan
loss_train 31 nan
loss_valid 32 nan
loss_train 32 nan
loss_valid 33 nan
loss_train 33 nan
loss_valid 34 nan
loss_train 34 nan
loss_valid 35 nan
loss_train 35 nan
loss_valid 36 nan
loss_train 36 nan
loss_valid 37 nan
loss_train 37 nan
loss_valid 38 nan
loss_train 38 nan
loss_valid 39 nan
loss_train 39 nan
loss_valid 40 nan
loss_train 40 nan
loss_valid 41 nan
loss_train 41 nan
loss_valid 42 nan
loss_train 42 nan
loss_valid 43 nan
loss_train 43 nan
loss_valid 44 nan
loss_train 44 nan
loss_valid 45 nan
loss_train 45 nan
loss_valid 46 nan
loss_train 46 nan
loss_valid 47 nan
loss_train 47 nan
loss_valid 48 nan
loss_train 48 nan
loss_valid 49 nan
loss_train 49 nan
loss_valid 50 nan
loss_train 50 nan
loss_valid 51 nan
loss_train 51 nan
loss_valid 52 nan
loss_train 52 nan
loss_valid 53 nan
loss_train 53 nan
loss_valid 54 nan
loss_train 54 nan
loss_valid 55 nan
loss_train 55 nan
loss_valid 56 nan
loss_train 56 nan
loss_valid 57 nan
loss_train 57 nan
loss_valid 58 nan
loss_train 58 nan
loss_valid 59 nan
loss_train 59 nan
loss_valid 60 nan
loss_train 60 nan
loss_valid 61 nan
loss_train 61 nan
loss_valid 62 nan
loss_train 62 nan
loss_valid 63 nan
loss_train 63 nan
loss_valid 64 nan
loss_train 64 nan
loss_valid 65 nan
loss_train 65 nan
loss_valid 66 nan
loss_train 66 nan
loss_valid 67 nan
loss_train 67 nan
loss_valid 68 nan
loss_train 68 nan
loss_valid 69 nan
loss_train 69 nan
loss_valid 70 nan
loss_train 70 nan
loss_valid 71 nan
loss_train 71 nan
loss_valid 72 nan
loss_train 72 nan
loss_valid 73 nan
loss_train 73 nan
loss_valid 74 nan
loss_train 74 nan
loss_valid 75 nan
loss_train 75 nan
loss_valid 76 nan
loss_train 76 nan
loss_valid 77 nan
loss_train 77 nan
loss_valid 78 nan
loss_train 78 nan
loss_valid 79 nan
loss_train 79 nan
loss_valid 80 nan
loss_train 80 nan
loss_valid 81 nan
loss_train 81 nan
loss_valid 82 nan
loss_train 82 nan
loss_valid 83 nan
loss_train 83 nan
loss_valid 84 nan
loss_train 84 nan
loss_valid 85 nan
loss_train 85 nan
loss_valid 86 nan
loss_train 86 nan
loss_valid 87 nan
loss_train 87 nan
loss_valid 88 nan
loss_train 88 nan
loss_valid 89 nan
loss_train 89 nan
loss_valid 90 nan
loss_train 90 nan
loss_valid 91 nan
loss_train 91 nan
loss_valid 92 nan
loss_train 92 nan
loss_valid 93 nan
loss_train 93 nan
loss_valid 94 nan
loss_train 94 nan
loss_valid 95 nan
loss_train 95 nan
loss_valid 96 nan
loss_train 96 nan
loss_valid 97 nan
loss_train 97 nan
loss_valid 98 nan
loss_train 98 nan
loss_valid 99 nan
loss_train 99 nan
NONERECALL: 0.0 for emoticon: 0
[0.0, 0, 0, 0.0, 0.76623376623376627, 0.093073593073593072, 0.46320346320346323, 0.45670995670995673, 0.8528138528138528, 1.0, 0.001, 0.7662337662337663, 0.09307359307359307, 0.46320346320346323, 0.45670995670995673, 0.8528138528138528, 1.0, 462.0, 462.0, 462.0, 462.0, 462.0, 462.0, 462.0, 0, 0.20014770420795305, -0.047206761492475775, nan, 0.002770083102493075, 0.00425531914893617, 0.004166666666666667, 0.005319148936170213, 0.0034602076124567475, 0.0028328611898017, 0.9904761904761905, nan, 0.011363636363636364, 0.013333333333333334, 0.00980392156862745, 0.010416666666666666, 0.005263157894736842, 0.9956709956709957, 0.9973544973544973, nan, 0.004310344827586207, 0.005988023952095809, 0.003194888178913738, 0.002785515320334262, 0.995575221238938, 0.9974424552429667, 0.9957264957264957, nan, 0.004545454545454545, 0.0025252525252525255, 0.003003003003003003, 0.9964028776978417, 0.9972527472527473, 0.9966555183946488, 0.9959349593495935, nan, 0.0033112582781456954, 0.002793296089385475, 0.9943502824858758, 0.9972972972972973, 0.9934640522875817, 0.9857142857142858, 0.9939024390243902, nan, 0.0044444444444444444, 0.9911504424778761, 0.9963768115942029, 0.9906542056074766, 0.9924812030075187, 0.9907407407407407, 0.995850622406639, nan, 0, 359, 233, 238, 186, 287, 351, 103, 0, 86, 73, 100, 94, 188, 229, 376, 0, 230, 165, 311, 357, 224, 389, 232, 0, 218, 394, 331, 276, 362, 297, 244, 0, 300, 356, 175, 368, 151, 68, 162, 0, 223, 111, 274, 105, 131, 106, 239, 0, 0.067311085752599573]
N, d, L:  1664 18 7
loss_valid 0 1.47088222933
loss_train 0 1.46814175716
loss_valid 1 1.35983559246
loss_train 1 1.36360934225
loss_valid 2 1.25969504669
loss_train 2 1.26989444278
loss_valid 3 1.17080263102
loss_train 3 1.186088044
loss_valid 4 1.0920817664
loss_train 4 1.11093983584
loss_valid 5 1.02290521525
loss_train 5 1.04364701619
loss_valid 6 0.961709319566
loss_train 6 0.982869869189
loss_valid 7 0.907346446072
loss_train 7 0.927625773221
loss_valid 8 0.858255198992
loss_train 8 0.876667056191
loss_valid 9 0.813479350275
loss_train 9 0.829119395422
loss_valid 10 0.772031987237
loss_train 10 0.784139512683
loss_valid 11 0.734415272653
loss_train 11 0.741629171937
loss_valid 12 0.700374113647
loss_train 12 0.701741154133
loss_valid 13 0.669084796327
loss_train 13 0.664646917901
loss_valid 14 0.640474659154
loss_train 14 0.630252987873
loss_valid 15 0.613963703377
loss_train 15 0.598348096611
loss_valid 16 nan
loss_train 16 nan
loss_valid 17 nan
loss_train 17 nan
loss_valid 18 nan
loss_train 18 nan
loss_valid 19 nan
loss_train 19 nan
loss_valid 20 nan
loss_train 20 nan
loss_valid 21 nan
loss_train 21 nan
loss_valid 22 nan
loss_train 22 nan
loss_valid 23 nan
loss_train 23 nan
loss_valid 24 nan
loss_train 24 nan
loss_valid 25 nan
loss_train 25 nan
loss_valid 26 nan
loss_train 26 nan
loss_valid 27 nan
loss_train 27 nan
loss_valid 28 nan
loss_train 28 nan
loss_valid 29 nan
loss_train 29 nan
loss_valid 30 nan
loss_train 30 nan
loss_valid 31 nan
loss_train 31 nan
loss_valid 32 nan
loss_train 32 nan
loss_valid 33 nan
loss_train 33 nan
loss_valid 34 nan
loss_train 34 nan
loss_valid 35 nan
loss_train 35 nan
loss_valid 36 nan
loss_train 36 nan
loss_valid 37 nan
loss_train 37 nan
loss_valid 38 nan
loss_train 38 nan
loss_valid 39 nan
loss_train 39 nan
loss_valid 40 nan
loss_train 40 nan
loss_valid 41 nan
loss_train 41 nan
loss_valid 42 nan
loss_train 42 nan
loss_valid 43 nan
loss_train 43 nan
loss_valid 44 nan
loss_train 44 nan
loss_valid 45 nan
loss_train 45 nan
loss_valid 46 nan
loss_train 46 nan
loss_valid 47 nan
loss_train 47 nan
loss_valid 48 nan
loss_train 48 nan
loss_valid 49 nan
loss_train 49 nan
loss_valid 50 nan
loss_train 50 nan
loss_valid 51 nan
loss_train 51 nan
loss_valid 52 nan
loss_train 52 nan
loss_valid 53 nan
loss_train 53 nan
loss_valid 54 nan
loss_train 54 nan
loss_valid 55 nan
loss_train 55 nan
loss_valid 56 nan
loss_train 56 nan
loss_valid 57 nan
loss_train 57 nan
loss_valid 58 nan
loss_train 58 nan
loss_valid 59 nan
loss_train 59 nan
loss_valid 60 nan
loss_train 60 nan
loss_valid 61 nan
loss_train 61 nan
loss_valid 62 nan
loss_train 62 nan
loss_valid 63 nan
loss_train 63 nan
loss_valid 64 nan
loss_train 64 nan
loss_valid 65 nan
loss_train 65 nan
loss_valid 66 nan
loss_train 66 nan
loss_valid 67 nan
loss_train 67 nan
loss_valid 68 nan
loss_train 68 nan
loss_valid 69 nan
loss_train 69 nan
loss_valid 70 nan
loss_train 70 nan
loss_valid 71 nan
loss_train 71 nan
loss_valid 72 nan
loss_train 72 nan
loss_valid 73 nan
loss_train 73 nan
loss_valid 74 nan
loss_train 74 nan
loss_valid 75 nan
loss_train 75 nan
loss_valid 76 nan
loss_train 76 nan
loss_valid 77 nan
loss_train 77 nan
loss_valid 78 nan
loss_train 78 nan
loss_valid 79 nan
loss_train 79 nan
loss_valid 80 nan
loss_train 80 nan
loss_valid 81 nan
loss_train 81 nan
loss_valid 82 nan
loss_train 82 nan
loss_valid 83 nan
loss_train 83 nan
loss_valid 84 nan
loss_train 84 nan
loss_valid 85 nan
loss_train 85 nan
loss_valid 86 nan
loss_train 86 nan
loss_valid 87 nan
loss_train 87 nan
loss_valid 88 nan
loss_train 88 nan
loss_valid 89 nan
loss_train 89 nan
loss_valid 90 nan
loss_train 90 nan
loss_valid 91 nan
loss_train 91 nan
loss_valid 92 nan
loss_train 92 nan
loss_valid 93 nan
loss_train 93 nan
loss_valid 94 nan
loss_train 94 nan
loss_valid 95 nan
loss_train 95 nan
loss_valid 96 nan
loss_train 96 nan
loss_valid 97 nan
loss_train 97 nan
loss_valid 98 nan
loss_train 98 nan
loss_valid 99 nan
loss_train 99 nan
NONERECALL: 0.0 for emoticon: 0
[0.0, 0, 0, 0.0, 0.82251082251082253, 0.07792207792207792, 0.47186147186147187, 0.40259740259740262, 0.86363636363636365, 1.0, 0.001, 0.8225108225108225, 0.07792207792207792, 0.47186147186147187, 0.4025974025974026, 0.8636363636363636, 1.0, 462.0, 462.0, 462.0, 462.0, 462.0, 462.0, 462.0, 0, 0.19446042110031478, -0.043702329416615129, nan, 0.0025906735751295338, 0.0041841004184100415, 0.00398406374501992, 0.005649717514124294, 0.0033003300330033004, 0.0027548209366391185, 0.9875, nan, 0.014492753623188406, 0.01639344262295082, 0.0125, 0.012345679012345678, 0.006329113924050633, 0.9955947136563876, 0.9974811083123426, nan, 0.004166666666666667, 0.00641025641025641, 0.003067484662576687, 0.0026954177897574125, 0.9953488372093023, 0.9975308641975309, 0.995575221238938, nan, 0.005050505050505051, 0.0024937655860349127, 0.0029411764705882353, 0.9965397923875432, 0.9974093264248705, 0.9967741935483871, 0.996268656716418, nan, 0.003215434083601286, 0.0026954177897574125, 0.9938650306748467, 0.9974025974025974, 0.9928571428571429, 0.9846153846153847, 0.9935483870967742, nan, 0.004484304932735426, 0.9902912621359223, 0.9967532467532467, 0.9894736842105263, 0.9920634920634921, 0.9894736842105263, 0.9958847736625515, nan, 0, 384, 237, 249, 175, 301, 361, 78, 0, 67, 59, 78, 79, 156, 225, 395, 0, 238, 154, 324, 369, 213, 403, 224, 0, 196, 399, 338, 287, 384, 308, 266, 0, 309, 369, 161, 383, 138, 63, 153, 0, 221, 101, 306, 93, 124, 93, 241, 0, 0.068722824164973439]
N, d, L:  1664 18 7
loss_valid 0 1.44671031875
loss_train 0 1.49353841601
loss_valid 1 1.32813401044
loss_train 1 1.3698819518
loss_valid 2 1.21844177005
loss_train 2 1.25479762763
loss_valid 3 1.12404951976
loss_train 3 1.15511938618
loss_valid 4 1.0455075877
loss_train 4 1.07116639596
loss_valid 5 0.980580794228
loss_train 5 1.0006288802
loss_valid 6 0.925962577606
loss_train 6 0.940395388659
loss_valid 7 0.878169115831
loss_train 7 0.88701167685
loss_valid 8 0.834055194968
loss_train 8 0.837538520501
loss_valid 9 0.791322733192
loss_train 9 0.789963013523
loss_valid 10 0.748852278985
loss_train 10 0.743019102516
loss_valid 11 0.70606080994
loss_train 11 0.695286736391
loss_valid 12 0.663288560962
loss_train 12 0.646736872351
loss_valid 13 0.62430523131
loss_train 13 0.602888427544
loss_valid 14 0.589823221238
loss_train 14 0.565207763881
loss_valid 15 0.55869825914
loss_train 15 nan
loss_valid 16 nan
loss_train 16 nan
loss_valid 17 nan
loss_train 17 nan
loss_valid 18 nan
loss_train 18 nan
loss_valid 19 nan
loss_train 19 nan
loss_valid 20 nan
loss_train 20 nan
loss_valid 21 nan
loss_train 21 nan
loss_valid 22 nan
loss_train 22 nan
loss_valid 23 nan
loss_train 23 nan
loss_valid 24 nan
loss_train 24 nan
loss_valid 25 nan
loss_train 25 nan
loss_valid 26 nan
loss_train 26 nan
loss_valid 27 nan
loss_train 27 nan
loss_valid 28 nan
loss_train 28 nan
loss_valid 29 nan
loss_train 29 nan
loss_valid 30 nan
loss_train 30 nan
loss_valid 31 nan
loss_train 31 nan
loss_valid 32 nan
loss_train 32 nan
loss_valid 33 nan
loss_train 33 nan
loss_valid 34 nan
loss_train 34 nan
loss_valid 35 nan
loss_train 35 nan
loss_valid 36 nan
loss_train 36 nan
loss_valid 37 nan
loss_train 37 nan
loss_valid 38 nan
loss_train 38 nan
loss_valid 39 nan
loss_train 39 nan
loss_valid 40 nan
loss_train 40 nan
loss_valid 41 nan
loss_train 41 nan
loss_valid 42 nan
loss_train 42 nan
loss_valid 43 nan
loss_train 43 nan
loss_valid 44 nan
loss_train 44 nan
loss_valid 45 nan
loss_train 45 nan
loss_valid 46 nan
loss_train 46 nan
loss_valid 47 nan
loss_train 47 nan
loss_valid 48 nan
loss_train 48 nan
loss_valid 49 nan
loss_train 49 nan
loss_valid 50 nan
loss_train 50 nan
loss_valid 51 nan
loss_train 51 nan
loss_valid 52 nan
loss_train 52 nan
loss_valid 53 nan
loss_train 53 nan
loss_valid 54 nan
loss_train 54 nan
loss_valid 55 nan
loss_train 55 nan
loss_valid 56 nan
loss_train 56 nan
loss_valid 57 nan
loss_train 57 nan
loss_valid 58 nan
loss_train 58 nan
loss_valid 59 nan
loss_train 59 nan
loss_valid 60 nan
loss_train 60 nan
loss_valid 61 nan
loss_train 61 nan
loss_valid 62 nan
loss_train 62 nan
loss_valid 63 nan
loss_train 63 nan
loss_valid 64 nan
loss_train 64 nan
loss_valid 65 nan
loss_train 65 nan
loss_valid 66 nan
loss_train 66 nan
loss_valid 67 nan
loss_train 67 nan
loss_valid 68 nan
loss_train 68 nan
loss_valid 69 nan
loss_train 69 nan
loss_valid 70 nan
loss_train 70 nan
loss_valid 71 nan
loss_train 71 nan
loss_valid 72 nan
loss_train 72 nan
loss_valid 73 nan
loss_train 73 nan
loss_valid 74 nan
loss_train 74 nan
loss_valid 75 nan
loss_train 75 nan
loss_valid 76 nan
loss_train 76 nan
loss_valid 77 nan
loss_train 77 nan
loss_valid 78 nan
loss_train 78 nan
loss_valid 79 nan
loss_train 79 nan
loss_valid 80 nan
loss_train 80 nan
loss_valid 81 nan
loss_train 81 nan
loss_valid 82 nan
loss_train 82 nan
loss_valid 83 nan
loss_train 83 nan
loss_valid 84 nan
loss_train 84 nan
loss_valid 85 nan
loss_train 85 nan
loss_valid 86 nan
loss_train 86 nan
loss_valid 87 nan
loss_train 87 nan
loss_valid 88 nan
loss_train 88 nan
loss_valid 89 nan
loss_train 89 nan
loss_valid 90 nan
loss_train 90 nan
loss_valid 91 nan
loss_train 91 nan
loss_valid 92 nan
loss_train 92 nan
loss_valid 93 nan
loss_train 93 nan
loss_valid 94 nan
loss_train 94 nan
loss_valid 95 nan
loss_train 95 nan
loss_valid 96 nan
loss_train 96 nan
loss_valid 97 nan
loss_train 97 nan
loss_valid 98 nan
loss_train 98 nan
loss_valid 99 nan
loss_train 99 nan
NONERECALL: 0.0 for emoticon: 0
[0.0, 0, 0, 0.0, 0.79653679653679654, 0.099567099567099568, 0.46753246753246752, 0.44155844155844154, 0.8614718614718615, 1.0, 0.001, 0.7965367965367965, 0.09956709956709957, 0.4675324675324675, 0.44155844155844154, 0.8614718614718615, 1.0, 462.0, 462.0, 462.0, 462.0, 462.0, 462.0, 462.0, 0, 0.20279246924688019, -0.041022469593898159, nan, 0.002652519893899204, 0.00423728813559322, 0.004098360655737705, 0.005555555555555556, 0.0033783783783783786, 0.002840909090909091, 0.9887640449438202, nan, 0.0136986301369863, 0.015384615384615385, 0.011627906976744186, 0.011363636363636364, 0.005494505494505495, 0.9956521739130435, 0.9974554707379135, nan, 0.004273504273504274, 0.0064516129032258064, 0.0030581039755351682, 0.002777777777777778, 0.9954954954954955, 0.9975062344139651, 0.9956896551724138, nan, 0.004608294930875576, 0.002506265664160401, 0.003076923076923077, 0.9965034965034965, 0.9973684210526316, 0.9967845659163987, 0.9959839357429718, nan, 0.003205128205128205, 0.002777777777777778, 0.9941176470588236, 0.9973544973544973, 0.9928057553956835, 0.9850746268656716, 0.9935064935064936, nan, 0.004464285714285714, 0.9912280701754386, 0.9964788732394366, 0.9905660377358491, 0.9929078014184397, 0.9905660377358491, 0.9958677685950413, nan, 0, 375, 234, 242, 178, 294, 350, 87, 0, 71, 63, 84, 86, 180, 228, 391, 0, 232, 153, 325, 358, 220, 399, 230, 0, 215, 397, 323, 284, 378, 309, 247, 0, 310, 358, 168, 376, 137, 65, 152, 0, 222, 112, 282, 104, 139, 104, 240, 0, 0.068283317489794285]
N, d, L:  1664 18 7
loss_valid 0 1.39088086259
loss_train 0 1.3812361922
loss_valid 1 1.28065900578
loss_train 1 1.27192154578
loss_valid 2 1.18406578735
loss_train 2 1.17517738342
loss_valid 3 1.10088671202
loss_train 3 1.09103634926
loss_valid 4 1.02970721576
loss_train 4 1.018151425
loss_valid 5 0.968261462245
loss_train 5 0.954325198827
loss_valid 6 0.913970945989
loss_train 6 0.897247162419
loss_valid 7 0.864300971593
loss_train 7 0.844894767233
loss_valid 8 0.817248467067
loss_train 8 0.795596116957
loss_valid 9 0.771617770449
loss_train 9 0.748381260541
loss_valid 10 0.726990445628
loss_train 10 0.702895420613
loss_valid 11 0.684050634606
loss_train 11 0.659424975931
loss_valid 12 0.643255867103
loss_train 12 0.618289065842
loss_valid 13 0.60537742708
loss_train 13 0.579856083005
loss_valid 14 0.570632027192
loss_train 14 0.544437491305
loss_valid 15 0.538974722378
loss_train 15 0.512024205237
loss_valid 16 0.510298652764
loss_train 16 0.482587129952
loss_valid 17 0.484541861521
loss_train 17 0.456077970864
loss_valid 18 0.461525669903
loss_train 18 0.432409661664
loss_valid 19 0.440942817293
loss_train 19 0.411409239921
loss_valid 20 0.422682192176
loss_train 20 0.392909902859
loss_valid 21 0.406539313009
loss_train 21 0.376625656656
loss_valid 22 0.392208552194
loss_train 22 0.362268622948
loss_valid 23 0.379586724564
loss_train 23 0.349572065112
loss_valid 24 nan
loss_train 24 nan
loss_valid 25 nan
loss_train 25 nan
loss_valid 26 nan
loss_train 26 nan
loss_valid 27 nan
loss_train 27 nan
loss_valid 28 nan
loss_train 28 nan
loss_valid 29 nan
loss_train 29 nan
loss_valid 30 nan
loss_train 30 nan
loss_valid 31 nan
loss_train 31 nan
loss_valid 32 nan
loss_train 32 nan
loss_valid 33 nan
loss_train 33 nan
loss_valid 34 nan
loss_train 34 nan
loss_valid 35 nan
loss_train 35 nan
loss_valid 36 nan
loss_train 36 nan
loss_valid 37 nan
loss_train 37 nan
loss_valid 38 nan
loss_train 38 nan
loss_valid 39 nan
loss_train 39 nan
loss_valid 40 nan
loss_train 40 nan
loss_valid 41 nan
loss_train 41 nan
loss_valid 42 nan
loss_train 42 nan
loss_valid 43 nan
loss_train 43 nan
loss_valid 44 nan
loss_train 44 nan
loss_valid 45 nan
loss_train 45 nan
loss_valid 46 nan
loss_train 46 nan
loss_valid 47 nan
loss_train 47 nan
loss_valid 48 nan
loss_train 48 nan
loss_valid 49 nan
loss_train 49 nan
loss_valid 50 nan
loss_train 50 nan
loss_valid 51 nan
loss_train 51 nan
loss_valid 52 nan
loss_train 52 nan
loss_valid 53 nan
loss_train 53 nan
loss_valid 54 nan
loss_train 54 nan
loss_valid 55 nan
loss_train 55 nan
loss_valid 56 nan
loss_train 56 nan
loss_valid 57 nan
loss_train 57 nan
loss_valid 58 nan
loss_train 58 nan
loss_valid 59 nan
loss_train 59 nan
loss_valid 60 nan
loss_train 60 nan
loss_valid 61 nan
loss_train 61 nan
loss_valid 62 nan
loss_train 62 nan
loss_valid 63 nan
loss_train 63 nan
loss_valid 64 nan
loss_train 64 nan
loss_valid 65 nan
loss_train 65 nan
loss_valid 66 nan
loss_train 66 nan
loss_valid 67 nan
loss_train 67 nan
loss_valid 68 nan
loss_train 68 nan
loss_valid 69 nan
loss_train 69 nan
loss_valid 70 nan
loss_train 70 nan
loss_valid 71 nan
loss_train 71 nan
loss_valid 72 nan
loss_train 72 nan
loss_valid 73 nan
loss_train 73 nan
loss_valid 74 nan
loss_train 74 nan
loss_valid 75 nan
loss_train 75 nan
loss_valid 76 nan
loss_train 76 nan
loss_valid 77 nan
loss_train 77 nan
loss_valid 78 nan
loss_train 78 nan
loss_valid 79 nan
loss_train 79 nan
loss_valid 80 nan
loss_train 80 nan
loss_valid 81 nan
loss_train 81 nan
loss_valid 82 nan
loss_train 82 nan
loss_valid 83 nan
loss_train 83 nan
loss_valid 84 nan
loss_train 84 nan
loss_valid 85 nan
loss_train 85 nan
loss_valid 86 nan
loss_train 86 nan
loss_valid 87 nan
loss_train 87 nan
loss_valid 88 nan
loss_train 88 nan
loss_valid 89 nan
loss_train 89 nan
loss_valid 90 nan
loss_train 90 nan
loss_valid 91 nan
loss_train 91 nan
loss_valid 92 nan
loss_train 92 nan
loss_valid 93 nan
loss_train 93 nan
loss_valid 94 nan
loss_train 94 nan
loss_valid 95 nan
loss_train 95 nan
loss_valid 96 nan
loss_train 96 nan
loss_valid 97 nan
loss_train 97 nan
loss_valid 98 nan
loss_train 98 nan
loss_valid 99 nan
loss_train 99 nan
NONERECALL: 0.0 for emoticon: 0
[0.0, 0, 0, 0.0, 0.7943722943722944, 0.10822510822510822, 0.45021645021645024, 0.44588744588744589, 0.87229437229437234, 1.0, 0.001, 0.7943722943722944, 0.10822510822510822, 0.45021645021645024, 0.4458874458874459, 0.8722943722943723, 1.0, 462.0, 462.0, 462.0, 462.0, 462.0, 462.0, 462.0, 0, 0.20468901766394407, -0.063698206555349399, nan, 0.002617801047120419, 0.004132231404958678, 0.004098360655737705, 0.0048543689320388345, 0.003215434083601286, 0.0027472527472527475, 0.9880952380952381, nan, 0.013888888888888888, 0.015384615384615385, 0.011904761904761904, 0.011904761904761904, 0.005208333333333333, 0.9955357142857143, 0.9974619289340102, nan, 0.004424778761061947, 0.006802721088435374, 0.0030581039755351682, 0.0026954177897574125, 0.9954954954954955, 0.9975062344139651, 0.9958333333333333, nan, 0.004672897196261682, 0.0024691358024691358, 0.002881844380403458, 0.9961538461538462, 0.9973821989528796, 0.9968652037617555, 0.996031746031746, nan, 0.003115264797507788, 0.002702702702702703, 0.9935483870967742, 0.9973821989528796, 0.9928057553956835, 0.9836065573770492, 0.993103448275862, nan, 0.0043859649122807015, 0.9901960784313726, 0.9963503649635036, 0.9894736842105263, 0.9915966386554622, 0.9895833333333334, 0.9957983193277311, nan, 0, 380, 240, 242, 204, 309, 362, 82, 0, 70, 63, 82, 82, 190, 222, 392, 0, 224, 145, 325, 369, 220, 399, 238, 0, 212, 403, 345, 258, 380, 317, 250, 0, 319, 368, 153, 380, 137, 59, 143, 0, 226, 100, 272, 93, 117, 94, 236, 0, 0.067771313259132313]
N, d, L:  1664 18 7
loss_valid 0 1.32255024203
loss_train 0 1.34193832486
loss_valid 1 1.20736341189
loss_train 1 1.22782244106
loss_valid 2 1.10691896266
loss_train 2 1.12845927897
loss_valid 3 1.01972179472
loss_train 3 1.04207484911
loss_valid 4 0.943734963165
loss_train 4 0.96681047324
loss_valid 5 0.876499528499
loss_train 5 0.900576569765
loss_valid 6 0.815677488744
loss_train 6 0.841117130224
loss_valid 7 0.759462647551
loss_train 7 0.786396975311
loss_valid 8 0.706886526936
loss_train 8 0.735083020173
loss_valid 9 0.657543255346
loss_train 9 0.686424577941
loss_valid 10 0.61140570423
loss_train 10 0.640329338441
loss_valid 11 0.568821067298
loss_train 11 0.597111279022
loss_valid 12 0.529910402746
loss_train 12 0.557041269077
loss_valid 13 0.494904964624
loss_train 13 0.52024409259
loss_valid 14 0.463771810441
loss_train 14 0.4867937295
loss_valid 15 0.436311631374
loss_train 15 0.456655665657
loss_valid 16 nan
loss_train 16 nan
loss_valid 17 nan
loss_train 17 nan
loss_valid 18 nan
loss_train 18 nan
loss_valid 19 nan
loss_train 19 nan
loss_valid 20 nan
loss_train 20 nan
loss_valid 21 nan
loss_train 21 nan
loss_valid 22 nan
loss_train 22 nan
loss_valid 23 nan
loss_train 23 nan
loss_valid 24 nan
loss_train 24 nan
loss_valid 25 nan
loss_train 25 nan
loss_valid 26 nan
loss_train 26 nan
loss_valid 27 nan
loss_train 27 nan
loss_valid 28 nan
loss_train 28 nan
loss_valid 29 nan
loss_train 29 nan
loss_valid 30 nan
loss_train 30 nan
loss_valid 31 nan
loss_train 31 nan
loss_valid 32 nan
loss_train 32 nan
loss_valid 33 nan
loss_train 33 nan
loss_valid 34 nan
loss_train 34 nan
loss_valid 35 nan
loss_train 35 nan
loss_valid 36 nan
loss_train 36 nan
loss_valid 37 nan
loss_train 37 nan
loss_valid 38 nan
loss_train 38 nan
loss_valid 39 nan
loss_train 39 nan
loss_valid 40 nan
loss_train 40 nan
loss_valid 41 nan
loss_train 41 nan
loss_valid 42 nan
loss_train 42 nan
loss_valid 43 nan
loss_train 43 nan
loss_valid 44 nan
loss_train 44 nan
loss_valid 45 nan
loss_train 45 nan
loss_valid 46 nan
loss_train 46 nan
loss_valid 47 nan
loss_train 47 nan
loss_valid 48 nan
loss_train 48 nan
loss_valid 49 nan
loss_train 49 nan
loss_valid 50 nan
loss_train 50 nan
loss_valid 51 nan
loss_train 51 nan
loss_valid 52 nan
loss_train 52 nan
loss_valid 53 nan
loss_train 53 nan
loss_valid 54 nan
loss_train 54 nan
loss_valid 55 nan
loss_train 55 nan
loss_valid 56 nan
loss_train 56 nan
loss_valid 57 nan
loss_train 57 nan
loss_valid 58 nan
loss_train 58 nan
loss_valid 59 nan
loss_train 59 nan
loss_valid 60 nan
loss_train 60 nan
loss_valid 61 nan
loss_train 61 nan
loss_valid 62 nan
loss_train 62 nan
loss_valid 63 nan
loss_tr/home/jasonzhang/anaconda2/lib/python2.7/site-packages/numpy/lib/nanfunctions.py:703: RuntimeWarning: Mean of empty slice
  warnings.warn("Mean of empty slice", RuntimeWarning)
/home/jasonzhang/anaconda2/lib/python2.7/site-packages/numpy/lib/nanfunctions.py:1202: RuntimeWarning: Degrees of freedom <= 0 for slice.
  warnings.warn("Degrees of freedom <= 0 for slice.", RuntimeWarning)
ain 63 nan
loss_valid 64 nan
loss_train 64 nan
loss_valid 65 nan
loss_train 65 nan
loss_valid 66 nan
loss_train 66 nan
loss_valid 67 nan
loss_train 67 nan
loss_valid 68 nan
loss_train 68 nan
loss_valid 69 nan
loss_train 69 nan
loss_valid 70 nan
loss_train 70 nan
loss_valid 71 nan
loss_train 71 nan
loss_valid 72 nan
loss_train 72 nan
loss_valid 73 nan
loss_train 73 nan
loss_valid 74 nan
loss_train 74 nan
loss_valid 75 nan
loss_train 75 nan
loss_valid 76 nan
loss_train 76 nan
loss_valid 77 nan
loss_train 77 nan
loss_valid 78 nan
loss_train 78 nan
loss_valid 79 nan
loss_train 79 nan
loss_valid 80 nan
loss_train 80 nan
loss_valid 81 nan
loss_train 81 nan
loss_valid 82 nan
loss_train 82 nan
loss_valid 83 nan
loss_train 83 nan
loss_valid 84 nan
loss_train 84 nan
loss_valid 85 nan
loss_train 85 nan
loss_valid 86 nan
loss_train 86 nan
loss_valid 87 nan
loss_train 87 nan
loss_valid 88 nan
loss_train 88 nan
loss_valid 89 nan
loss_train 89 nan
loss_valid 90 nan
loss_train 90 nan
loss_valid 91 nan
loss_train 91 nan
loss_valid 92 nan
loss_train 92 nan
loss_valid 93 nan
loss_train 93 nan
loss_valid 94 nan
loss_train 94 nan
loss_valid 95 nan
loss_train 95 nan
loss_valid 96 nan
loss_train 96 nan
loss_valid 97 nan
loss_train 97 nan
loss_valid 98 nan
loss_train 98 nan
loss_valid 99 nan
loss_train 99 nan
NONERECALL: 0.0 for emoticon: 0
[0.0, 0, 0, 0.0, 0.77056277056277056, 0.093073593073593072, 0.44155844155844154, 0.46969696969696972, 0.85064935064935066, 1.0, 0.001, 0.7705627705627706, 0.09307359307359307, 0.44155844155844154, 0.4696969696969697, 0.8506493506493507, 1.0, 462.0, 462.0, 462.0, 462.0, 462.0, 462.0, 462.0, 0, 0.19967008550704182, -0.047619047619047616, nan, 0.0027472527472527475, 0.004219409282700422, 0.004291845493562232, 0.005405405405405406, 0.0034602076124567475, 0.002840909090909091, 0.9901960784313726, nan, 0.011494252873563218, 0.013888888888888888, 0.009900990099009901, 0.010526315789473684, 0.005076142131979695, 0.9956331877729258, 0.9973614775725593, nan, 0.0045871559633027525, 0.005847953216374269, 0.0032258064516129032, 0.0027624309392265192, 0.9957081545064378, 0.9974619289340102, 0.9959677419354839, nan, 0.0045045045045045045, 0.002531645569620253, 0.0028653295128939827, 0.99644128113879, 0.9972602739726028, 0.9966101694915255, 0.9959016393442623, nan, 0.003355704697986577, 0.0027624309392265192, 0.9943502824858758, 0.9973045822102425, 0.9935897435897436, 0.9859154929577465, 0.9940476190476191, nan, 0.0044444444444444444, 0.9912280701754386, 0.9962825278810409, 0.9903846153846154, 0.9914529914529915, 0.9903846153846154, 0.995850622406639, nan, 0, 362, 235, 231, 183, 287, 350, 100, 0, 85, 70, 99, 93, 195, 227, 377, 0, 216, 169, 308, 360, 231, 392, 246, 0, 220, 393, 347, 279, 363, 293, 242, 0, 296, 360, 175, 369, 154, 69, 166, 0, 223, 112, 267, 102, 115, 102, 239, 0, 0.06740036212658998]
{'perf': [array([  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,
         0.00000000e+00,   7.90043290e-01,   9.43722944e-02,
         4.58874459e-01,   4.43290043e-01,   8.60173160e-01,
         1.00000000e+00,   1.00000000e-03,   7.90043290e-01,
         9.43722944e-02,   4.58874459e-01,   4.43290043e-01,
         8.60173160e-01,   1.00000000e+00,   4.62000000e+02,
         4.62000000e+02,   4.62000000e+02,   4.62000000e+02,
         4.62000000e+02,   4.62000000e+02,   4.62000000e+02,
         0.00000000e+00,   2.00351940e-01,  -4.86497629e-02,
                    nan,   2.67566607e-03,   4.20566968e-03,
         4.12785944e-03,   5.35683927e-03,   3.36291154e-03,
         2.80335061e-03,   9.89006310e-01,              nan,
         1.29876324e-02,   1.48769791e-02,   1.11475161e-02,
         1.13114119e-02,   5.47425056e-03,   9.95617357e-01,
         9.97422897e-01,              nan,   4.35249010e-03,
         6.30011351e-03,   3.12087745e-03,   2.74331192e-03,
         9.95524641e-01,   9.97489543e-01,   9.95758489e-01,
                    nan,   4.67633125e-03,   2.50521303e-03,
         2.95365529e-03,   9.96408259e-01,   9.97334594e-01,
         9.96737930e-01,   9.96024187e-01,              nan,
         3.24055801e-03,   2.74632506e-03,   9.94046326e-01,
         9.97348235e-01,   9.93104490e-01,   9.84985270e-01,
         9.93621677e-01,              nan,   4.44468889e-03,
         9.90818785e-01,   9.96448365e-01,   9.90110445e-01,
         9.92100425e-01,   9.90149682e-01,   9.95850421e-01,
                    nan,   0.00000000e+00,   3.72000000e+02,
         2.35800000e+02,   2.40400000e+02,   1.85200000e+02,
         2.95600000e+02,   3.54800000e+02,   9.00000000e+01,
         0.00000000e+00,   7.58000000e+01,   6.56000000e+01,
         8.86000000e+01,   8.68000000e+01,   1.81800000e+02,
         2.26200000e+02,   3.86200000e+02,   0.00000000e+00,
         2.28000000e+02,   1.57200000e+02,   3.18600000e+02,
         3.62600000e+02,   2.21600000e+02,   3.96400000e+02,
         2.34000000e+02,   0.00000000e+00,   2.12200000e+02,
         3.97200000e+02,   3.36800000e+02,   2.76800000e+02,
         3.73400000e+02,   3.04800000e+02,   2.49800000e+02,
         0.00000000e+00,   3.06800000e+02,   3.62200000e+02,
         1.66400000e+02,   3.75200000e+02,   1.43400000e+02,
         6.48000000e+01,   1.55200000e+02,   0.00000000e+00,
         2.23000000e+02,   1.07200000e+02,   2.80200000e+02,
         9.94000000e+01,   1.25200000e+02,   9.98000000e+01,
         2.39000000e+02,   0.00000000e+00,   6.78977806e-02]), array([  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,
         0.00000000e+00,   2.03048301e-02,   9.92843718e-03,
         1.12886622e-02,   2.25524091e-02,   7.81622082e-03,
         0.00000000e+00,   0.00000000e+00,   2.03048301e-02,
         9.92843718e-03,   1.12886622e-02,   2.25524091e-02,
         7.81622082e-03,   0.00000000e+00,   0.00000000e+00,
         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,
         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,
         0.00000000e+00,   3.46361252e-03,   7.90256901e-03,
                    nan,   7.09183459e-05,   4.36164796e-05,
         1.00811691e-04,   2.76232330e-04,   9.46977504e-05,
         4.28818206e-05,   1.16050090e-03,              nan,
         1.30005511e-03,   1.11123493e-03,   1.09475866e-03,
         7.53849457e-04,   4.48322214e-04,   4.79962324e-05,
         5.37102499e-05,              nan,   1.43414819e-04,
         3.43379302e-04,   7.37827776e-05,   3.98051622e-05,
         1.17338367e-04,   3.23865449e-05,   1.33219520e-04,
                    nan,   1.95601589e-04,   2.25121251e-05,
         7.84144306e-05,   1.35806134e-04,   6.51426018e-05,
         9.25234935e-05,   1.29940176e-04,              nan,
         8.46665926e-05,   3.98733260e-05,   3.06814905e-04,
         4.15890697e-05,   3.47684173e-04,   8.29750204e-04,
         3.30893820e-04,              nan,   3.28697169e-05,
         4.71395637e-04,   1.64990596e-04,   5.27131548e-04,
         5.42435893e-04,   5.20695699e-04,   2.89679473e-05,
                    nan,   0.00000000e+00,   9.85900604e+00,
         2.48193473e+00,   5.88557559e+00,   1.01469207e+01,
         8.47584804e+00,   5.49181209e+00,   9.85900604e+00,
         0.00000000e+00,   8.03492377e+00,   5.12249939e+00,
         9.11262860e+00,   5.91269820e+00,   1.37753403e+01,
         2.48193473e+00,   8.03492377e+00,   0.00000000e+00,
         7.48331477e+00,   8.68101377e+00,   7.49933330e+00,
         5.31413210e+00,   5.88557559e+00,   5.12249939e+00,
         7.48331477e+00,   0.00000000e+00,   8.54166260e+00,
         3.60000000e+00,   8.90842298e+00,   1.01469207e+01,
         9.11262860e+00,   8.68101377e+00,   8.54166260e+00,
         0.00000000e+00,   8.08455317e+00,   5.30659966e+00,
         8.47584804e+00,   5.91269820e+00,   7.49933330e+00,
         3.60000000e+00,   8.08455317e+00,   0.00000000e+00,
         1.67332005e+00,   5.49181209e+00,   1.37753403e+01,
         5.31413210e+00,   8.90842298e+00,   5.30659966e+00,
         1.67332005e+00,   0.00000000e+00,   5.36090580e-04])]}
SMPrank.py:290: RuntimeWarning: invalid value encountered in double_scalars
  self.probfeature[k] = self.probfeature[k] / sum_probf
/home/jasonzhang/anaconda2/lib/python2.7/site-packages/numpy/ma/core.py:867: RuntimeWarning: invalid value encountered in less_equal
  return umath.less_equal(x, self.critical_value)
dataset, N,d,L,K segment 2310 18 7 46
N, d, L:  1664 18 7
loss_valid 0 1.41165578953
loss_train 0 1.33810852938
loss_valid 1 1.23866412967
loss_train 1 1.16303422529
loss_valid 2 1.09385611443
loss_train 2 1.01784923119
loss_valid 3 0.970363240617
loss_train 3 0.893740227228
loss_valid 4 0.864551554132
loss_train 4 0.786244049194
loss_valid 5 0.776263606665
loss_train 5 0.696153117905
loss_valid 6 0.704696141764
loss_train 6 0.623210139528
loss_valid 7 0.64619635913
loss_train 7 0.563375290072
loss_valid 8 0.597537547064
loss_train 8 0.513039182113
loss_valid 9 0.557083199908
loss_train 9 0.470534220444
loss_valid 10 0.523648184683
loss_train 10 0.435186622235
loss_valid 11 nan
loss_train 11 nan
loss_valid 12 nan
loss_train 12 nan
loss_valid 13 nan
loss_train 13 nan
loss_valid 14 nan
loss_train 14 nan
loss_valid 15 nan
loss_train 15 nan
loss_valid 16 nan
loss_train 16 nan
loss_valid 17 nan
loss_train 17 nan
loss_valid 18 nan
loss_train 18 nan
loss_valid 19 nan
loss_train 19 nan
loss_valid 20 nan
loss_train 20 nan
loss_valid 21 nan
loss_train 21 nan
loss_valid 22 nan
loss_train 22 nan
loss_valid 23 nan
loss_train 23 nan
loss_valid 24 nan
loss_train 24 nan
loss_valid 25 nan
loss_train 25 nan
loss_valid 26 nan
loss_train 26 nan
loss_valid 27 nan
loss_train 27 nan
loss_valid 28 nan
loss_train 28 nan
loss_valid 29 nan
loss_train 29 nan
loss_valid 30 nan
loss_train 30 nan
loss_valid 31 nan
loss_train 31 nan
loss_valid 32 nan
loss_train 32 nan
loss_valid 33 nan
loss_train 33 nan
loss_valid 34 nan
loss_train 34 nan
loss_valid 35 nan
loss_train 35 nan
loss_valid 36 nan
loss_train 36 nan
loss_valid 37 nan
loss_train 37 nan
loss_valid 38 nan
loss_train 38 nan
loss_valid 39 nan
loss_train 39 nan
loss_valid 40 nan
loss_train 40 nan
loss_valid 41 nan
loss_train 41 nan
loss_valid 42 nan
loss_train 42 nan
loss_valid 43 nan
loss_train 43 nan
loss_valid 44 nan
loss_train 44 nan
loss_valid 45 nan
loss_train 45 nan
loss_valid 46 nan
loss_train 46 nan
loss_valid 47 nan
loss_train 47 nan
loss_valid 48 nan
loss_train 48 nan
loss_valid 49 nan
loss_train 49 nan
loss_valid 50 nan
loss_train 50 nan
loss_valid 51 nan
loss_train 51 nan
loss_valid 52 nan
loss_train 52 nan
loss_valid 53 nan
loss_train 53 nan
loss_valid 54 nan
loss_train 54 nan
loss_valid 55 nan
loss_train 55 nan
loss_valid 56 nan
loss_train 56 nan
loss_valid 57 nan
loss_train 57 nan
loss_valid 58 nan
loss_train 58 nan
loss_valid 59 nan
loss_train 59 nan
loss_valid 60 nan
loss_train 60 nan
loss_valid 61 nan
loss_train 61 nan
loss_valid 62 nan
loss_train 62 nan
loss_valid 63 nan
loss_train 63 nan
loss_valid 64 nan
loss_train 64 nan
loss_valid 65 nan
loss_train 65 nan
loss_valid 66 nan
loss_train 66 nan
loss_valid 67 nan
loss_train 67 nan
loss_valid 68 nan
loss_train 68 nan
loss_valid 69 nan
loss_train 69 nan
loss_valid 70 nan
loss_train 70 nan
loss_valid 71 nan
loss_train 71 nan
loss_valid 72 nan
loss_train 72 nan
loss_valid 73 nan
loss_train 73 nan
loss_valid 74 nan
loss_train 74 nan
loss_valid 75 nan
loss_train 75 nan
loss_valid 76 nan
loss_train 76 nan
loss_valid 77 nan
loss_train 77 nan
loss_valid 78 nan
loss_train 78 nan
loss_valid 79 nan
loss_train 79 nan
loss_valid 80 nan
loss_train 80 nan
loss_valid 81 nan
loss_train 81 nan
loss_valid 82 nan
loss_train 82 nan
loss_valid 83 nan
loss_train 83 nan
loss_valid 84 nan
loss_train 84 nan
loss_valid 85 nan
loss_train 85 nan
loss_valid 86 nan
loss_train 86 nan
loss_valid 87 nan
loss_train 87 nan
loss_valid 88 nan
loss_train 88 nan
loss_valid 89 nan
loss_train 89 nan
loss_valid 90 nan
loss_train 90 nan
loss_valid 91 nan
loss_train 91 nan
loss_valid 92 nan
loss_train 92 nan
loss_valid 93 nan
loss_train 93 nan
loss_valid 94 nan
loss_train 94 nan
loss_valid 95 nan
loss_train 95 nan
loss_valid 96 nan
loss_train 96 nan
loss_valid 97 nan
loss_train 97 nan
loss_valid 98 nan
loss_train 98 nan
loss_valid 99 nan
loss_train 99 nan
NONERECALL: 0.0 for emoticon: 0
[0.0, 0, 0, 0.0, 0.76623376623376627, 0.093073593073593072, 0.46320346320346323, 0.45670995670995673, 0.8528138528138528, 1.0, 0.001, 0.7662337662337663, 0.09307359307359307, 0.46320346320346323, 0.45670995670995673, 0.8528138528138528, 1.0, 462.0, 462.0, 462.0, 462.0, 462.0, 462.0, 462.0, 0, 0.20014770420795305, -0.047206761492475775, nan, 0.002770083102493075, 0.00425531914893617, 0.004166666666666667, 0.005319148936170213, 0.0034602076124567475, 0.0028328611898017, 0.9904761904761905, nan, 0.011363636363636364, 0.013333333333333334, 0.00980392156862745, 0.010416666666666666, 0.005263157894736842, 0.9956709956709957, 0.9973544973544973, nan, 0.004310344827586207, 0.005988023952095809, 0.003194888178913738, 0.002785515320334262, 0.995575221238938, 0.9974424552429667, 0.9957264957264957, nan, 0.004545454545454545, 0.0025252525252525255, 0.003003003003003003, 0.9964028776978417, 0.9972527472527473, 0.9966555183946488, 0.9959349593495935, nan, 0.0033112582781456954, 0.002793296089385475, 0.9943502824858758, 0.9972972972972973, 0.9934640522875817, 0.9857142857142858, 0.9939024390243902, nan, 0.0044444444444444444, 0.9911504424778761, 0.9963768115942029, 0.9906542056074766, 0.9924812030075187, 0.9907407407407407, 0.995850622406639, nan, 0, 359, 233, 238, 186, 287, 351, 103, 0, 86, 73, 100, 94, 188, 229, 376, 0, 230, 165, 311, 357, 224, 389, 232, 0, 218, 394, 331, 276, 362, 297, 244, 0, 300, 356, 175, 368, 151, 68, 162, 0, 223, 111, 274, 105, 131, 106, 239, 0, 0.067311085752599573]
N, d, L:  1664 18 7
loss_valid 0 1.33276605872
loss_train 0 1.32665851593
loss_valid 1 1.1517822423
loss_train 1 1.15191475798
loss_valid 2 1.00229346387
loss_train 2 1.00662248624
loss_valid 3 0.878357805999
loss_train 3 0.884154786711
loss_valid 4 0.775574896472
loss_train 4 0.780127195301
loss_valid 5 0.691960003574
loss_train 5 0.692715362727
loss_valid 6 0.624414112781
loss_train 6 0.619531342743
loss_valid 7 0.569888259578
loss_train 7 0.558324587879
loss_valid 8 0.525392532867
loss_train 8 0.506805307976
loss_valid 9 0.489066805663
loss_train 9 0.46388497806
loss_valid 10 0.459036151331
loss_train 10 0.42813988671
loss_valid 11 0.433953944334
loss_train 11 0.398299700749
loss_valid 12 0.412180842618
loss_train 12 0.373215339051
loss_valid 13 0.392436902566
loss_train 13 0.351910187742
loss_valid 14 0.374249751802
loss_train 14 0.333617291688
loss_valid 15 nan
loss_train 15 nan
loss_valid 16 nan
loss_train 16 nan
loss_valid 17 nan
loss_train 17 nan
loss_valid 18 nan
loss_train 18 nan
loss_valid 19 nan
loss_train 19 nan
loss_valid 20 nan
loss_train 20 nan
loss_valid 21 nan
loss_train 21 nan
loss_valid 22 nan
loss_train 22 nan
loss_valid 23 nan
loss_train 23 nan
loss_valid 24 nan
loss_train 24 nan
loss_valid 25 nan
loss_train 25 nan
loss_valid 26 nan
loss_train 26 nan
loss_valid 27 nan
loss_train 27 nan
loss_valid 28 nan
loss_train 28 nan
loss_valid 29 nan
loss_train 29 nan
loss_valid 30 nan
loss_train 30 nan
loss_valid 31 nan
loss_train 31 nan
loss_valid 32 nan
loss_train 32 nan
loss_valid 33 nan
loss_train 33 nan
loss_valid 34 nan
loss_train 34 nan
loss_valid 35 nan
loss_train 35 nan
loss_valid 36 nan
loss_train 36 nan
loss_valid 37 nan
loss_train 37 nan
loss_valid 38 nan
loss_train 38 nan
loss_valid 39 nan
loss_train 39 nan
loss_valid 40 nan
loss_train 40 nan
loss_valid 41 nan
loss_train 41 nan
loss_valid 42 nan
loss_train 42 nan
loss_valid 43 nan
loss_train 43 nan
loss_valid 44 nan
loss_train 44 nan
loss_valid 45 nan
loss_train 45 nan
loss_valid 46 nan
loss_train 46 nan
loss_valid 47 nan
loss_train 47 nan
loss_valid 48 nan
loss_train 48 nan
loss_valid 49 nan
loss_train 49 nan
loss_valid 50 nan
loss_train 50 nan
loss_valid 51 nan
loss_train 51 nan
loss_valid 52 nan
loss_train 52 nan
loss_valid 53 nan
loss_train 53 nan
loss_valid 54 nan
loss_train 54 nan
loss_valid 55 nan
loss_train 55 nan
loss_valid 56 nan
loss_train 56 nan
loss_valid 57 nan
loss_train 57 nan
loss_valid 58 nan
loss_train 58 nan
loss_valid 59 nan
loss_train 59 nan
loss_valid 60 nan
loss_train 60 nan
loss_valid 61 nan
loss_train 61 nan
loss_valid 62 nan
loss_train 62 nan
loss_valid 63 nan
loss_train 63 nan
loss_valid 64 nan
loss_train 64 nan
loss_valid 65 nan
loss_train 65 nan
loss_valid 66 nan
loss_train 66 nan
loss_valid 67 nan
loss_train 67 nan
loss_valid 68 nan
loss_train 68 nan
loss_valid 69 nan
loss_train 69 nan
loss_valid 70 nan
loss_train 70 nan
loss_valid 71 nan
loss_train 71 nan
loss_valid 72 nan
loss_train 72 nan
loss_valid 73 nan
loss_train 73 nan
loss_valid 74 nan
loss_train 74 nan
loss_valid 75 nan
loss_train 75 nan
loss_valid 76 nan
loss_train 76 nan
loss_valid 77 nan
loss_train 77 nan
loss_valid 78 nan
loss_train 78 nan
loss_valid 79 nan
loss_train 79 nan
loss_valid 80 nan
loss_train 80 nan
loss_valid 81 nan
loss_train 81 nan
loss_valid 82 nan
loss_train 82 nan
loss_valid 83 nan
loss_train 83 nan
loss_valid 84 nan
loss_train 84 nan
loss_valid 85 nan
loss_train 85 nan
loss_valid 86 nan
loss_train 86 nan
loss_valid 87 nan
loss_train 87 nan
loss_valid 88 nan
loss_train 88 nan
loss_valid 89 nan
loss_train 89 nan
loss_valid 90 nan
loss_train 90 nan
loss_valid 91 nan
loss_train 91 nan
loss_valid 92 nan
loss_train 92 nan
loss_valid 93 nan
loss_train 93 nan
loss_valid 94 nan
loss_train 94 nan
loss_valid 95 nan
loss_train 95 nan
loss_valid 96 nan
loss_train 96 nan
loss_valid 97 nan
loss_train 97 nan
loss_valid 98 nan
loss_train 98 nan
loss_valid 99 nan
loss_train 99 nan
NONERECALL: 0.0 for emoticon: 0
[0.0, 0, 0, 0.0, 0.82251082251082253, 0.07792207792207792, 0.47186147186147187, 0.40259740259740262, 0.86363636363636365, 1.0, 0.001, 0.8225108225108225, 0.07792207792207792, 0.47186147186147187, 0.4025974025974026, 0.8636363636363636, 1.0, 462.0, 462.0, 462.0, 462.0, 462.0, 462.0, 462.0, 0, 0.19446042110031478, -0.043702329416615129, nan, 0.0025906735751295338, 0.0041841004184100415, 0.00398406374501992, 0.005649717514124294, 0.0033003300330033004, 0.0027548209366391185, 0.9875, nan, 0.014492753623188406, 0.01639344262295082, 0.0125, 0.012345679012345678, 0.006329113924050633, 0.9955947136563876, 0.9974811083123426, nan, 0.004166666666666667, 0.00641025641025641, 0.003067484662576687, 0.0026954177897574125, 0.9953488372093023, 0.9975308641975309, 0.995575221238938, nan, 0.005050505050505051, 0.0024937655860349127, 0.0029411764705882353, 0.9965397923875432, 0.9974093264248705, 0.9967741935483871, 0.996268656716418, nan, 0.003215434083601286, 0.0026954177897574125, 0.9938650306748467, 0.9974025974025974, 0.9928571428571429, 0.9846153846153847, 0.9935483870967742, nan, 0.004484304932735426, 0.9902912621359223, 0.9967532467532467, 0.9894736842105263, 0.9920634920634921, 0.9894736842105263, 0.9958847736625515, nan, 0, 384, 237, 249, 175, 301, 361, 78, 0, 67, 59, 78, 79, 156, 225, 395, 0, 238, 154, 324, 369, 213, 403, 224, 0, 196, 399, 338, 287, 384, 308, 266, 0, 309, 369, 161, 383, 138, 63, 153, 0, 221, 101, 306, 93, 124, 93, 241, 0, 0.068722824164973439]
N, d, L:  1664 18 7
loss_valid 0 1.23706002922
loss_train 0 1.27315200043
loss_valid 1 1.05926275752
loss_train 1 1.08986375
loss_valid 2 0.899065468068
loss_train 2 0.923858693727
loss_valid 3 0.768419858989
loss_train 3 0.787603663773
loss_valid 4 0.674216981914
loss_train 4 0.687639474581
loss_valid 5 0.607059557507
loss_train 5 0.615380344764
loss_valid 6 0.556501780602
loss_train 6 0.560939771377
loss_valid 7 0.516582694584
loss_train 7 0.517989789174
loss_valid 8 0.483925546882
loss_train 8 0.483067593927
loss_valid 9 0.456657163552
loss_train 9 0.45397291698
loss_valid 10 0.433536838704
loss_train 10 0.429366179561
loss_valid 11 0.413715557104
loss_train 11 0.4081606162
loss_valid 12 0.396411925548
loss_train 12 0.389591914279
loss_valid 13 0.381115719733
loss_train 13 0.373212518672
loss_valid 14 0.367458917166
loss_train 14 0.358726980268
loss_valid 15 0.355194663507
loss_train 15 nan
loss_valid 16 nan
loss_train 16 nan
loss_valid 17 nan
loss_train 17 nan
loss_valid 18 nan
loss_train 18 nan
loss_valid 19 nan
loss_train 19 nan
loss_valid 20 nan
loss_train 20 nan
loss_valid 21 nan
loss_train 21 nan
loss_valid 22 nan
loss_train 22 nan
loss_valid 23 nan
loss_train 23 nan
loss_valid 24 nan
loss_train 24 nan
loss_valid 25 nan
loss_train 25 nan
loss_valid 26 nan
loss_train 26 nan
loss_valid 27 nan
loss_train 27 nan
loss_valid 28 nan
loss_train 28 nan
loss_valid 29 nan
loss_train 29 nan
loss_valid 30 nan
loss_train 30 nan
loss_valid 31 nan
loss_train 31 nan
loss_valid 32 nan
loss_train 32 nan
loss_valid 33 nan
loss_train 33 nan
loss_valid 34 nan
loss_train 34 nan
loss_valid 35 nan
loss_train 35 nan
loss_valid 36 nan
loss_train 36 nan
loss_valid 37 nan
loss_train 37 nan
loss_valid 38 nan
loss_train 38 nan
loss_valid 39 nan
loss_train 39 nan
loss_valid 40 nan
loss_train 40 nan
loss_valid 41 nan
loss_train 41 nan
loss_valid 42 nan
loss_train 42 nan
loss_valid 43 nan
loss_train 43 nan
loss_valid 44 nan
loss_train 44 nan
loss_valid 45 nan
loss_train 45 nan
loss_valid 46 nan
loss_train 46 nan
loss_valid 47 nan
loss_train 47 nan
loss_valid 48 nan
loss_train 48 nan
loss_valid 49 nan
loss_train 49 nan
loss_valid 50 nan
loss_train 50 nan
loss_valid 51 nan
loss_train 51 nan
loss_valid 52 nan
loss_train 52 nan
loss_valid 53 nan
loss_train 53 nan
loss_valid 54 nan
loss_train 54 nan
loss_valid 55 nan
loss_train 55 nan
loss_valid 56 nan
loss_train 56 nan
loss_valid 57 nan
loss_train 57 nan
loss_valid 58 nan
loss_train 58 nan
loss_valid 59 nan
loss_train 59 nan
loss_valid 60 nan
loss_train 60 nan
loss_valid 61 nan
loss_train 61 nan
loss_valid 62 nan
loss_train 62 nan
loss_valid 63 nan
loss_train 63 nan
loss_valid 64 nan
loss_train 64 nan
loss_valid 65 nan
loss_train 65 nan
loss_valid 66 nan
loss_train 66 nan
loss_valid 67 nan
loss_train 67 nan
loss_valid 68 nan
loss_train 68 nan
loss_valid 69 nan
loss_train 69 nan
loss_valid 70 nan
loss_train 70 nan
loss_valid 71 nan
loss_train 71 nan
loss_valid 72 nan
loss_train 72 nan
loss_valid 73 nan
loss_train 73 nan
loss_valid 74 nan
loss_train 74 nan
loss_valid 75 nan
loss_train 75 nan
loss_valid 76 nan
loss_train 76 nan
loss_valid 77 nan
loss_train 77 nan
loss_valid 78 nan
loss_train 78 nan
loss_valid 79 nan
loss_train 79 nan
loss_valid 80 nan
loss_train 80 nan
loss_valid 81 nan
loss_train 81 nan
loss_valid 82 nan
loss_train 82 nan
loss_valid 83 nan
loss_train 83 nan
loss_valid 84 nan
loss_train 84 nan
loss_valid 85 nan
loss_train 85 nan
loss_valid 86 nan
loss_train 86 nan
loss_valid 87 nan
loss_train 87 nan
loss_valid 88 nan
loss_train 88 nan
loss_valid 89 nan
loss_train 89 nan
loss_valid 90 nan
loss_train 90 nan
loss_valid 91 nan
loss_train 91 nan
loss_valid 92 nan
loss_train 92 nan
loss_valid 93 nan
loss_train 93 nan
loss_valid 94 nan
loss_train 94 nan
loss_valid 95 nan
loss_train 95 nan
loss_valid 96 nan
loss_train 96 nan
loss_valid 97 nan
loss_train 97 nan
loss_valid 98 nan
loss_train 98 nan
loss_valid 99 nan
loss_train 99 nan
NONERECALL: 0.0 for emoticon: 0
[0.0, 0, 0, 0.0, 0.79653679653679654, 0.099567099567099568, 0.46753246753246752, 0.44155844155844154, 0.8614718614718615, 1.0, 0.001, 0.7965367965367965, 0.09956709956709957, 0.4675324675324675, 0.44155844155844154, 0.8614718614718615, 1.0, 462.0, 462.0, 462.0, 462.0, 462.0, 462.0, 462.0, 0, 0.20279246924688019, -0.041022469593898159, nan, 0.002652519893899204, 0.00423728813559322, 0.004098360655737705, 0.005555555555555556, 0.0033783783783783786, 0.002840909090909091, 0.9887640449438202, nan, 0.0136986301369863, 0.015384615384615385, 0.011627906976744186, 0.011363636363636364, 0.005494505494505495, 0.9956521739130435, 0.9974554707379135, nan, 0.004273504273504274, 0.0064516129032258064, 0.0030581039755351682, 0.002777777777777778, 0.9954954954954955, 0.9975062344139651, 0.9956896551724138, nan, 0.004608294930875576, 0.002506265664160401, 0.003076923076923077, 0.9965034965034965, 0.9973684210526316, 0.9967845659163987, 0.9959839357429718, nan, 0.003205128205128205, 0.002777777777777778, 0.9941176470588236, 0.9973544973544973, 0.9928057553956835, 0.9850746268656716, 0.9935064935064936, nan, 0.004464285714285714, 0.9912280701754386, 0.9964788732394366, 0.9905660377358491, 0.9929078014184397, 0.9905660377358491, 0.9958677685950413, nan, 0, 375, 234, 242, 178, 294, 350, 87, 0, 71, 63, 84, 86, 180, 228, 391, 0, 232, 153, 325, 358, 220, 399, 230, 0, 215, 397, 323, 284, 378, 309, 247, 0, 310, 358, 168, 376, 137, 65, 152, 0, 222, 112, 282, 104, 139, 104, 240, 0, 0.068283317489794285]
N, d, L:  1664 18 7
loss_valid 0 1.20145430095
loss_train 0 1.19292961626
loss_valid 1 1.04468188011
loss_train 1 1.03978232931
loss_valid 2 0.918563389439
loss_train 2 0.916080760714
loss_valid 3 0.815586869559
loss_train 3 0.814695752614
loss_valid 4 0.728110968853
loss_train 4 0.728953358882
loss_valid 5 0.650523814383
loss_train 5 0.654110868277
loss_valid 6 0.580690520247
loss_train 6 0.587606891889
loss_valid 7 0.520405731413
loss_train 7 0.529445739106
loss_valid 8 0.470925244293
loss_train 8 0.480019635643
loss_valid 9 0.43121173318
loss_train 9 0.438867475743
loss_valid 10 0.399221792384
loss_train 10 0.404944385924
loss_valid 11 0.373265167251
loss_train 11 0.376952183008
loss_valid 12 0.351782192845
loss_train 12 0.353625608403
loss_valid 13 0.333667939676
loss_train 13 0.333962885998
loss_valid 14 0.318151436266
loss_train 14 0.317211299987
loss_valid 15 0.304652884069
loss_train 15 0.302949212522
loss_valid 16 0.292759930951
loss_train 16 0.290826951151
loss_valid 17 0.282314527911
loss_train 17 0.280431806776
loss_valid 18 0.272966485611
loss_train 18 0.271387531374
loss_valid 19 0.264715994351
loss_train 19 nan
loss_valid 20 nan
loss_train 20 nan
loss_valid 21 nan
loss_train 21 nan
loss_valid 22 nan
loss_train 22 nan
loss_valid 23 nan
loss_train 23 nan
loss_valid 24 nan
loss_train 24 nan
loss_valid 25 nan
loss_train 25 nan
loss_valid 26 nan
loss_train 26 nan
loss_valid 27 nan
loss_train 27 nan
loss_valid 28 nan
loss_train 28 nan
loss_valid 29 nan
loss_train 29 nan
loss_valid 30 nan
loss_train 30 nan
loss_valid 31 nan
loss_train 31 nan
loss_valid 32 nan
loss_train 32 nan
loss_valid 33 nan
loss_train 33 nan
loss_valid 34 nan
loss_train 34 nan
loss_valid 35 nan
loss_train 35 nan
loss_valid 36 nan
loss_train 36 nan
loss_valid 37 nan
loss_train 37 nan
loss_valid 38 nan
loss_train 38 nan
loss_valid 39 nan
loss_train 39 nan
loss_valid 40 nan
loss_train 40 nan
loss_valid 41 nan
loss_train 41 nan
loss_valid 42 nan
loss_train 42 nan
loss_valid 43 nan
loss_train 43 nan
loss_valid 44 nan
loss_train 44 nan
loss_valid 45 nan
loss_train 45 nan
loss_valid 46 nan
loss_train 46 nan
loss_valid 47 nan
loss_train 47 nan
loss_valid 48 nan
loss_train 48 nan
loss_valid 49 nan
loss_train 49 nan
loss_valid 50 nan
loss_train 50 nan
loss_valid 51 nan
loss_train 51 nan
loss_valid 52 nan
loss_train 52 nan
loss_valid 53 nan
loss_train 53 nan
loss_valid 54 nan
loss_train 54 nan
loss_valid 55 nan
loss_train 55 nan
loss_valid 56 nan
loss_train 56 nan
loss_valid 57 nan
loss_train 57 nan
loss_valid 58 nan
loss_train 58 nan
loss_valid 59 nan
loss_train 59 nan
loss_valid 60 nan
loss_train 60 nan
loss_valid 61 nan
loss_train 61 nan
loss_valid 62 nan
loss_train 62 nan
loss_valid 63 nan
loss_train 63 nan
loss_valid 64 nan
loss_train 64 nan
loss_valid 65 nan
loss_train 65 nan
loss_valid 66 nan
loss_train 66 nan
loss_valid 67 nan
loss_train 67 nan
loss_valid 68 nan
loss_train 68 nan
loss_valid 69 nan
loss_train 69 nan
loss_valid 70 nan
loss_train 70 nan
loss_valid 71 nan
loss_train 71 nan
loss_valid 72 nan
loss_train 72 nan
loss_valid 73 nan
loss_train 73 nan
loss_valid 74 nan
loss_train 74 nan
loss_valid 75 nan
loss_train 75 nan
loss_valid 76 nan
loss_train 76 nan
loss_valid 77 nan
loss_train 77 nan
loss_valid 78 nan
loss_train 78 nan
loss_valid 79 nan
loss_train 79 nan
loss_valid 80 nan
loss_train 80 nan
loss_valid 81 nan
loss_train 81 nan
loss_valid 82 nan
loss_train 82 nan
loss_valid 83 nan
loss_train 83 nan
loss_valid 84 nan
loss_train 84 nan
loss_valid 85 nan
loss_train 85 nan
loss_valid 86 nan
loss_train 86 nan
loss_valid 87 nan
loss_train 87 nan
loss_valid 88 nan
loss_train 88 nan
loss_valid 89 nan
loss_train 89 nan
loss_valid 90 nan
loss_train 90 nan
loss_valid 91 nan
loss_train 91 nan
loss_valid 92 nan
loss_train 92 nan
loss_valid 93 nan
loss_train 93 nan
loss_valid 94 nan
loss_train 94 nan
loss_valid 95 nan
loss_train 95 nan
loss_valid 96 nan
loss_train 96 nan
loss_valid 97 nan
loss_train 97 nan
loss_valid 98 nan
loss_train 98 nan
loss_valid 99 nan
loss_train 99 nan
NONERECALL: 0.0 for emoticon: 0
[0.0, 0, 0, 0.0, 0.7943722943722944, 0.10822510822510822, 0.45021645021645024, 0.44588744588744589, 0.87229437229437234, 1.0, 0.001, 0.7943722943722944, 0.10822510822510822, 0.45021645021645024, 0.4458874458874459, 0.8722943722943723, 1.0, 462.0, 462.0, 462.0, 462.0, 462.0, 462.0, 462.0, 0, 0.20468901766394407, -0.063698206555349399, nan, 0.002617801047120419, 0.004132231404958678, 0.004098360655737705, 0.0048543689320388345, 0.003215434083601286, 0.0027472527472527475, 0.9880952380952381, nan, 0.013888888888888888, 0.015384615384615385, 0.011904761904761904, 0.011904761904761904, 0.005208333333333333, 0.9955357142857143, 0.9974619289340102, nan, 0.004424778761061947, 0.006802721088435374, 0.0030581039755351682, 0.0026954177897574125, 0.9954954954954955, 0.9975062344139651, 0.9958333333333333, nan, 0.004672897196261682, 0.0024691358024691358, 0.002881844380403458, 0.9961538461538462, 0.9973821989528796, 0.9968652037617555, 0.996031746031746, nan, 0.003115264797507788, 0.002702702702702703, 0.9935483870967742, 0.9973821989528796, 0.9928057553956835, 0.9836065573770492, 0.993103448275862, nan, 0.0043859649122807015, 0.9901960784313726, 0.9963503649635036, 0.9894736842105263, 0.9915966386554622, 0.9895833333333334, 0.9957983193277311, nan, 0, 380, 240, 242, 204, 309, 362, 82, 0, 70, 63, 82, 82, 190, 222, 392, 0, 224, 145, 325, 369, 220, 399, 238, 0, 212, 403, 345, 258, 380, 317, 250, 0, 319, 368, 153, 380, 137, 59, 143, 0, 226, 100, 272, 93, 117, 94, 236, 0, 0.067771313259132313]
N, d, L:  1664 18 7
loss_valid 0 1.14006562663
loss_train 0 1.14588293185
loss_valid 1 0.993814233494
loss_train 1 0.997492488843
loss_valid 2 0.867994965603
loss_train 2 0.870537875231
loss_valid 3 0.759665103969
loss_train 3 0.76198761793
loss_valid 4 0.667954427644
loss_train 4 0.670615426154
loss_valid 5 0.591096980096
loss_train 5 0.594704949232
loss_valid 6 0.526776506304
loss_train 6 0.53184708365
loss_valid 7 0.472994933319
loss_train 7 0.47964477542
loss_valid 8 0.428430518647
loss_train 8 0.436294298459
loss_valid 9 0.391972590832
loss_train 9 0.400335377027
loss_valid 10 0.362585595198
loss_train 10 0.370657300659
loss_valid 11 0.339213518547
loss_train 11 0.34620294693
loss_valid 12 0.32062639898
loss_train 12 0.326048978931
loss_valid 13 0.305933677211
loss_train 13 0.309288088154
loss_valid 14 0.294251785724
loss_train 14 0.295403759796
loss_valid 15 0.28481174795
loss_train 15 0.283883008468
loss_valid 16 nan
loss_train 16 nan
loss_valid 17 nan
loss_train 17 nan
loss_valid 18 nan
loss_train 18 nan
loss_valid 19 nan
loss_train 19 nan
loss_valid 20 nan
loss_train 20 nan
loss_valid 21 nan
loss_train 21 nan
loss_valid 22 nan
loss_train 22 nan
loss_valid 23 nan
loss_train 23 nan
loss_valid 24 nan
loss_train 24 nan
loss_valid 25 nan
loss_train 25 nan
loss_valid 26 nan
loss_train 26 nan
loss_valid 27 nan
loss_train 27 nan
loss_valid 28 nan
loss_train 28 nan
loss_valid 29 nan
loss_train 29 nan
loss_valid 30 nan
loss_train 30 nan
loss_valid 31 nan
loss_train 31 nan
loss_valid 32 nan
loss_train 32 nan
loss_valid 33 nan
loss_train 33 nan
loss_valid 34 nan
loss_train 34 nan
loss_valid 35 nan
loss_train 35 nan
loss_valid 36 nan
loss_train 36 nan
loss_valid 37 nan
loss_train 37 nan
loss_valid 38 nan
loss_train 38 nan
loss_valid 39 nan
loss_train 39 nan
loss_valid 40 nan
loss_train 40 nan
loss_valid 41 nan
loss_train 41 nan
loss_valid 42 nan
loss_train 42 nan
loss_valid 43 nan
loss_train 43 nan
loss_valid 44 nan
loss_train 44 nan
loss_valid 45 nan
loss_train 45 nan
loss_valid 46 nan
loss_train 46 nan
loss_valid 47 nan
loss_train 47 nan
loss_valid 48 nan
loss_train 48 nan
loss_valid 49 nan
loss_train 49 nan
loss_valid 50 nan
loss_train 50 nan
loss_valid 51 nan
loss_train 51 nan
loss_valid 52 nan
loss_train 52 nan
loss_valid 53 nan
loss_train 53 nan
loss_valid 54 nan
loss_train 54 nan
loss_valid 55 nan
loss_train 55 nan
loss_valid 56 nan
loss_train 56 nan
loss_valid 57 nan
loss_train 57 nan
loss_valid 58 nan
loss_train 58 nan
loss_valid 59 nan
loss_train 59 nan
loss_valid 60 nan
loss_train 60 nan
loss_valid 61 nan
loss_train 61 nan
loss_valid 62 nan
loss_train 62 nan
loss_valid 63 nan
loss_train 63 nan
loss_valid 64 nan
loss_train 64 nan
loss_valid 65 nan
loss_train 65 na/home/jasonzhang/anaconda2/lib/python2.7/site-packages/numpy/lib/nanfunctions.py:703: RuntimeWarning: Mean of empty slice
  warnings.warn("Mean of empty slice", RuntimeWarning)
/home/jasonzhang/anaconda2/lib/python2.7/site-packages/numpy/lib/nanfunctions.py:1202: RuntimeWarning: Degrees of freedom <= 0 for slice.
  warnings.warn("Degrees of freedom <= 0 for slice.", RuntimeWarning)
n
loss_valid 66 nan
loss_train 66 nan
loss_valid 67 nan
loss_train 67 nan
loss_valid 68 nan
loss_train 68 nan
loss_valid 69 nan
loss_train 69 nan
loss_valid 70 nan
loss_train 70 nan
loss_valid 71 nan
loss_train 71 nan
loss_valid 72 nan
loss_train 72 nan
loss_valid 73 nan
loss_train 73 nan
loss_valid 74 nan
loss_train 74 nan
loss_valid 75 nan
loss_train 75 nan
loss_valid 76 nan
loss_train 76 nan
loss_valid 77 nan
loss_train 77 nan
loss_valid 78 nan
loss_train 78 nan
loss_valid 79 nan
loss_train 79 nan
loss_valid 80 nan
loss_train 80 nan
loss_valid 81 nan
loss_train 81 nan
loss_valid 82 nan
loss_train 82 nan
loss_valid 83 nan
loss_train 83 nan
loss_valid 84 nan
loss_train 84 nan
loss_valid 85 nan
loss_train 85 nan
loss_valid 86 nan
loss_train 86 nan
loss_valid 87 nan
loss_train 87 nan
loss_valid 88 nan
loss_train 88 nan
loss_valid 89 nan
loss_train 89 nan
loss_valid 90 nan
loss_train 90 nan
loss_valid 91 nan
loss_train 91 nan
loss_valid 92 nan
loss_train 92 nan
loss_valid 93 nan
loss_train 93 nan
loss_valid 94 nan
loss_train 94 nan
loss_valid 95 nan
loss_train 95 nan
loss_valid 96 nan
loss_train 96 nan
loss_valid 97 nan
loss_train 97 nan
loss_valid 98 nan
loss_train 98 nan
loss_valid 99 nan
loss_train 99 nan
NONERECALL: 0.0 for emoticon: 0
[0.0, 0, 0, 0.0, 0.77056277056277056, 0.093073593073593072, 0.44155844155844154, 0.46969696969696972, 0.85064935064935066, 1.0, 0.001, 0.7705627705627706, 0.09307359307359307, 0.44155844155844154, 0.4696969696969697, 0.8506493506493507, 1.0, 462.0, 462.0, 462.0, 462.0, 462.0, 462.0, 462.0, 0, 0.19967008550704182, -0.047619047619047616, nan, 0.0027472527472527475, 0.004219409282700422, 0.004291845493562232, 0.005405405405405406, 0.0034602076124567475, 0.002840909090909091, 0.9901960784313726, nan, 0.011494252873563218, 0.013888888888888888, 0.009900990099009901, 0.010526315789473684, 0.005076142131979695, 0.9956331877729258, 0.9973614775725593, nan, 0.0045871559633027525, 0.005847953216374269, 0.0032258064516129032, 0.0027624309392265192, 0.9957081545064378, 0.9974619289340102, 0.9959677419354839, nan, 0.0045045045045045045, 0.002531645569620253, 0.0028653295128939827, 0.99644128113879, 0.9972602739726028, 0.9966101694915255, 0.9959016393442623, nan, 0.003355704697986577, 0.0027624309392265192, 0.9943502824858758, 0.9973045822102425, 0.9935897435897436, 0.9859154929577465, 0.9940476190476191, nan, 0.0044444444444444444, 0.9912280701754386, 0.9962825278810409, 0.9903846153846154, 0.9914529914529915, 0.9903846153846154, 0.995850622406639, nan, 0, 362, 235, 231, 183, 287, 350, 100, 0, 85, 70, 99, 93, 195, 227, 377, 0, 216, 169, 308, 360, 231, 392, 246, 0, 220, 393, 347, 279, 363, 293, 242, 0, 296, 360, 175, 369, 154, 69, 166, 0, 223, 112, 267, 102, 115, 102, 239, 0, 0.06740036212658998]
{'perf': [array([  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,
         0.00000000e+00,   7.90043290e-01,   9.43722944e-02,
         4.58874459e-01,   4.43290043e-01,   8.60173160e-01,
         1.00000000e+00,   1.00000000e-03,   7.90043290e-01,
         9.43722944e-02,   4.58874459e-01,   4.43290043e-01,
         8.60173160e-01,   1.00000000e+00,   4.62000000e+02,
         4.62000000e+02,   4.62000000e+02,   4.62000000e+02,
         4.62000000e+02,   4.62000000e+02,   4.62000000e+02,
         0.00000000e+00,   2.00351940e-01,  -4.86497629e-02,
                    nan,   2.67566607e-03,   4.20566968e-03,
         4.12785944e-03,   5.35683927e-03,   3.36291154e-03,
         2.80335061e-03,   9.89006310e-01,              nan,
         1.29876324e-02,   1.48769791e-02,   1.11475161e-02,
         1.13114119e-02,   5.47425056e-03,   9.95617357e-01,
         9.97422897e-01,              nan,   4.35249010e-03,
         6.30011351e-03,   3.12087745e-03,   2.74331192e-03,
         9.95524641e-01,   9.97489543e-01,   9.95758489e-01,
                    nan,   4.67633125e-03,   2.50521303e-03,
         2.95365529e-03,   9.96408259e-01,   9.97334594e-01,
         9.96737930e-01,   9.96024187e-01,              nan,
         3.24055801e-03,   2.74632506e-03,   9.94046326e-01,
         9.97348235e-01,   9.93104490e-01,   9.84985270e-01,
         9.93621677e-01,              nan,   4.44468889e-03,
         9.90818785e-01,   9.96448365e-01,   9.90110445e-01,
         9.92100425e-01,   9.90149682e-01,   9.95850421e-01,
                    nan,   0.00000000e+00,   3.72000000e+02,
         2.35800000e+02,   2.40400000e+02,   1.85200000e+02,
         2.95600000e+02,   3.54800000e+02,   9.00000000e+01,
         0.00000000e+00,   7.58000000e+01,   6.56000000e+01,
         8.86000000e+01,   8.68000000e+01,   1.81800000e+02,
         2.26200000e+02,   3.86200000e+02,   0.00000000e+00,
         2.28000000e+02,   1.57200000e+02,   3.18600000e+02,
         3.62600000e+02,   2.21600000e+02,   3.96400000e+02,
         2.34000000e+02,   0.00000000e+00,   2.12200000e+02,
         3.97200000e+02,   3.36800000e+02,   2.76800000e+02,
         3.73400000e+02,   3.04800000e+02,   2.49800000e+02,
         0.00000000e+00,   3.06800000e+02,   3.62200000e+02,
         1.66400000e+02,   3.75200000e+02,   1.43400000e+02,
         6.48000000e+01,   1.55200000e+02,   0.00000000e+00,
         2.23000000e+02,   1.07200000e+02,   2.80200000e+02,
         9.94000000e+01,   1.25200000e+02,   9.98000000e+01,
         2.39000000e+02,   0.00000000e+00,   6.78977806e-02]), array([  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,
         0.00000000e+00,   2.03048301e-02,   9.92843718e-03,
         1.12886622e-02,   2.25524091e-02,   7.81622082e-03,
         0.00000000e+00,   0.00000000e+00,   2.03048301e-02,
         9.92843718e-03,   1.12886622e-02,   2.25524091e-02,
         7.81622082e-03,   0.00000000e+00,   0.00000000e+00,
         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,
         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,
         0.00000000e+00,   3.46361252e-03,   7.90256901e-03,
                    nan,   7.09183459e-05,   4.36164796e-05,
         1.00811691e-04,   2.76232330e-04,   9.46977504e-05,
         4.28818206e-05,   1.16050090e-03,              nan,
         1.30005511e-03,   1.11123493e-03,   1.09475866e-03,
         7.53849457e-04,   4.48322214e-04,   4.79962324e-05,
         5.37102499e-05,              nan,   1.43414819e-04,
         3.43379302e-04,   7.37827776e-05,   3.98051622e-05,
         1.17338367e-04,   3.23865449e-05,   1.33219520e-04,
                    nan,   1.95601589e-04,   2.25121251e-05,
         7.84144306e-05,   1.35806134e-04,   6.51426018e-05,
         9.25234935e-05,   1.29940176e-04,              nan,
         8.46665926e-05,   3.98733260e-05,   3.06814905e-04,
         4.15890697e-05,   3.47684173e-04,   8.29750204e-04,
         3.30893820e-04,              nan,   3.28697169e-05,
         4.71395637e-04,   1.64990596e-04,   5.27131548e-04,
         5.42435893e-04,   5.20695699e-04,   2.89679473e-05,
                    nan,   0.00000000e+00,   9.85900604e+00,
         2.48193473e+00,   5.88557559e+00,   1.01469207e+01,
         8.47584804e+00,   5.49181209e+00,   9.85900604e+00,
         0.00000000e+00,   8.03492377e+00,   5.12249939e+00,
         9.11262860e+00,   5.91269820e+00,   1.37753403e+01,
         2.48193473e+00,   8.03492377e+00,   0.00000000e+00,
         7.48331477e+00,   8.68101377e+00,   7.49933330e+00,
         5.31413210e+00,   5.88557559e+00,   5.12249939e+00,
         7.48331477e+00,   0.00000000e+00,   8.54166260e+00,
         3.60000000e+00,   8.90842298e+00,   1.01469207e+01,
         9.11262860e+00,   8.68101377e+00,   8.54166260e+00,
         0.00000000e+00,   8.08455317e+00,   5.30659966e+00,
         8.47584804e+00,   5.91269820e+00,   7.49933330e+00,
         3.60000000e+00,   8.08455317e+00,   0.00000000e+00,
         1.67332005e+00,   5.49181209e+00,   1.37753403e+01,
         5.31413210e+00,   8.90842298e+00,   5.30659966e+00,
         1.67332005e+00,   0.00000000e+00,   5.36090580e-04])]}
SMPrank.py:298: RuntimeWarning: invalid value encountered in double_scalars
  self.probfeature[k] = self.probfeature[k] / sum_probf
/home/jasonzhang/anaconda2/lib/python2.7/site-packages/numpy/ma/core.py:867: RuntimeWarning: invalid value encountered in less_equal
  return umath.less_equal(x, self.critical_value)
dataset, N,d,L,K segment 2310 18 7 46
N, d, L:  1664 18 7
loss_valid 0 1.41165578953
loss_train 0 1.33810852938
loss_valid 1 1.23866412967
loss_train 1 1.16303422529
loss_valid 2 1.09385611443
loss_train 2 1.01784923119
loss_valid 3 0.970363240617
loss_train 3 0.893740227228
loss_valid 4 0.864551554132
loss_train 4 0.786244049194
loss_valid 5 0.776263606665
loss_train 5 0.696153117905
loss_valid 6 0.704696141764
loss_train 6 0.623210139528
loss_valid 7 0.64619635913
loss_train 7 0.563375290072
loss_valid 8 0.597537547064
loss_train 8 0.513039182113
loss_valid 9 0.557083199908
loss_train 9 0.470534220444
loss_valid 10 0.523648184683
loss_train 10 0.435186622235
loss_valid 11 nan
loss_train 11 nan
nan train loss
early stop at epoch 11
[0.5562770562770563, 0, 0, 0.90476190476190477, 0.87878787878787878, 0.86796536796536794, 0.90043290043290047, 0.9329004329004329, 0.77922077922077926, 0.74025974025974028, 0.9047619047619048, 0.8787878787878788, 0.8679653679653679, 0.9004329004329005, 0.9329004329004329, 0.7792207792207793, 0.7402597402597403, 462.0, 462.0, 462.0, 462.0, 462.0, 462.0, 462.0, 0, 0.8551376136809784, 0.81261595547309839, nan, 0.9722991689750693, 0.8680851063829788, 0.9791666666666666, 0.8191489361702128, 0.9515570934256056, 0.9915014164305949, 0.6285714285714286, nan, 0.7159090909090909, 0.8266666666666667, 0.6764705882352942, 0.6875, 0.8736842105263158, 0.8528138528138528, 0.9523809523809523, nan, 0.896551724137931, 0.8502994011976048, 0.9680511182108626, 0.9916434540389972, 0.8495575221238938, 0.9514066496163683, 0.9529914529914529, nan, 0.9136363636363637, 0.9823232323232324, 0.990990990990991, 0.8884892086330936, 0.9642857142857143, 0.9331103678929766, 0.9878048780487805, nan, 0.9602649006622517, 0.994413407821229, 0.7796610169491526, 0.9540540540540541, 0.7973856209150327, 0.22857142857142856, 0.823170731707317, nan, 0.8977777777777778, 0.6371681415929203, 0.9311594202898551, 0.6728971962616822, 0.556390977443609, 0.6666666666666666, 0.921161825726141, nan, 0, 359, 233, 238, 186, 287, 351, 103, 0, 86, 73, 100, 94, 188, 229, 376, 0, 230, 165, 311, 357, 224, 389, 232, 0, 218, 394, 331, 276, 362, 297, 244, 0, 300, 356, 175, 368, 151, 68, 162, 0, 223, 111, 274, 105, 131, 106, 239, 0, 0.83028945867986126]
N, d, L:  1664 18 7
loss_valid 0 1.28156208243
loss_train 0 1.26908841395
loss_valid 1 1.10923225821
loss_train 1 1.10287292656
loss_valid 2 0.956441354192
loss_train 2 0.957429031885
loss_valid 3 0.825172810559
loss_train 3 0.832928173892
loss_valid 4 0.716320766859
loss_train 4 0.728961224674
loss_valid 5 0.629359144565
loss_train 5 0.644769945975
loss_valid 6 0.561684545273
loss_train 6 0.577833464788
loss_valid 7 0.50928159533
loss_train 7 0.524656545149
loss_valid 8 0.46829703255
loss_train 8 0.482056495257
loss_valid 9 0.435571511102
loss_train 9 0.44748593462
loss_valid 10 0.409031945781
loss_train 10 0.419211064473
loss_valid 11 0.387024365448
loss_train 11 0.395830204161
loss_valid 12 nan
loss_train 12 nan
nan train loss
early stop at epoch 12
NONERECALL: 0.0 for emoticon: 0
[0.0, 0, 0, 0.0, 0.82251082251082253, 0.07792207792207792, 0.47186147186147187, 0.40259740259740262, 0.86363636363636365, 1.0, 0.001, 0.8225108225108225, 0.07792207792207792, 0.47186147186147187, 0.4025974025974026, 0.8636363636363636, 1.0, 462.0, 462.0, 462.0, 462.0, 462.0, 462.0, 462.0, 0, 0.19446042110031478, -0.043702329416615129, nan, 0.0025906735751295338, 0.0041841004184100415, 0.00398406374501992, 0.005649717514124294, 0.0033003300330033004, 0.0027548209366391185, 0.9875, nan, 0.014492753623188406, 0.01639344262295082, 0.0125, 0.012345679012345678, 0.006329113924050633, 0.9955947136563876, 0.9974811083123426, nan, 0.004166666666666667, 0.00641025641025641, 0.003067484662576687, 0.0026954177897574125, 0.9953488372093023, 0.9975308641975309, 0.995575221238938, nan, 0.005050505050505051, 0.0024937655860349127, 0.0029411764705882353, 0.9965397923875432, 0.9974093264248705, 0.9967741935483871, 0.996268656716418, nan, 0.003215434083601286, 0.0026954177897574125, 0.9938650306748467, 0.9974025974025974, 0.9928571428571429, 0.9846153846153847, 0.9935483870967742, nan, 0.004484304932735426, 0.9902912621359223, 0.9967532467532467, 0.9894736842105263, 0.9920634920634921, 0.9894736842105263, 0.9958847736625515, nan, 0, 384, 237, 249, 175, 301, 361, 78, 0, 67, 59, 78, 79, 156, 225, 395, 0, 238, 154, 324, 369, 213, 403, 224, 0, 196, 399, 338, 287, 384, 308, 266, 0, 309, 369, 161, 383, 138, 63, 153, 0, 221, 101, 306, 93, 124, 93, 241, 0, 0.068722824164973439]
N, d, L:  1664 18 7
loss_valid 0 1.16828595974
loss_train 0 1.15771137058
loss_valid 1 0.998899402389
loss_train 1 0.990289517577
loss_valid 2 0.856722531776
loss_train 2 0.847643610482
loss_valid 3 0.741941704983
loss_train 3 0.729808199001
loss_valid 4 0.65381115074
loss_train 4 0.63655068482
loss_valid 5 0.588078361835
loss_train 5 0.564719393732
loss_valid 6 0.538603543903
loss_train 6 0.509352938437
loss_valid 7 0.499879999559
loss_train 7 0.465560325072
loss_valid 8 0.468016474345
loss_train 8 0.429732231282
loss_valid 9 0.440738203085
loss_train 9 0.399505789057
loss_valid 10 0.416706944612
loss_train 10 0.37341718048
loss_valid 11 0.395097455872
loss_train 11 0.351026118591
loss_valid 12 0.375674123083
loss_train 12 0.331909991677
loss_valid 13 nan
loss_train 13 nan
nan train loss
early stop at epoch 13
NONERECALL: 0.0 for emoticon: 0
[0.0, 0, 0, 0.0, 0.79653679653679654, 0.099567099567099568, 0.46753246753246752, 0.44155844155844154, 0.8614718614718615, 1.0, 0.001, 0.7965367965367965, 0.09956709956709957, 0.4675324675324675, 0.44155844155844154, 0.8614718614718615, 1.0, 462.0, 462.0, 462.0, 462.0, 462.0, 462.0, 462.0, 0, 0.20279246924688019, -0.041022469593898159, nan, 0.002652519893899204, 0.00423728813559322, 0.004098360655737705, 0.005555555555555556, 0.0033783783783783786, 0.002840909090909091, 0.9887640449438202, nan, 0.0136986301369863, 0.015384615384615385, 0.011627906976744186, 0.011363636363636364, 0.005494505494505495, 0.9956521739130435, 0.9974554707379135, nan, 0.004273504273504274, 0.0064516129032258064, 0.0030581039755351682, 0.002777777777777778, 0.9954954954954955, 0.9975062344139651, 0.9956896551724138, nan, 0.004608294930875576, 0.002506265664160401, 0.003076923076923077, 0.9965034965034965, 0.9973684210526316, 0.9967845659163987, 0.9959839357429718, nan, 0.003205128205128205, 0.002777777777777778, 0.9941176470588236, 0.9973544973544973, 0.9928057553956835, 0.9850746268656716, 0.9935064935064936, nan, 0.004464285714285714, 0.9912280701754386, 0.9964788732394366, 0.9905660377358491, 0.9929078014184397, 0.9905660377358491, 0.9958677685950413, nan, 0, 375, 234, 242, 178, 294, 350, 87, 0, 71, 63, 84, 86, 180, 228, 391, 0, 232, 153, 325, 358, 220, 399, 230, 0, 215, 397, 323, 284, 378, 309, 247, 0, 310, 358, 168, 376, 137, 65, 152, 0, 222, 112, 282, 104, 139, 104, 240, 0, 0.068283317489794285]
N, d, L:  1664 18 7
loss_valid 0 1.1600527672
loss_train 0 1.11972652262
loss_valid 1 1.02418526445
loss_train 1 0.970128247375
loss_valid 2 0.910623509872
loss_train 2 0.847054021624
loss_valid 3 0.814179705706
loss_train 3 0.74534930722
loss_valid 4 0.731420483991
loss_train 4 0.661139199293
loss_valid 5 0.659837589488
loss_train 5 0.591226423664
loss_valid 6 0.597734746467
loss_train 6 0.533023356814
loss_valid 7 0.54364157543
loss_train 7 0.484337199481
loss_valid 8 0.496632995811
loss_train 8 0.443540586547
loss_valid 9 0.455847939645
loss_train 9 0.409169152291
loss_valid 10 0.420576761836
loss_train 10 0.379940344573
loss_valid 11 0.39013643882
loss_train 11 0.354839507431
loss_valid 12 0.364621831249
loss_train 12 0.333167783115
loss_valid 13 nan
loss_train 13 nan
nan train loss
early stop at epoch 13
NONERECALL: 0.0 for emoticon: 0
[0.0, 0, 0, 0.0, 0.7943722943722944, 0.10822510822510822, 0.45021645021645024, 0.44588744588744589, 0.87229437229437234, 1.0, 0.001, 0.7943722943722944, 0.10822510822510822, 0.45021645021645024, 0.4458874458874459, 0.8722943722943723, 1.0, 462.0, 462.0, 462.0, 462.0, 462.0, 462.0, 462.0, 0, 0.20468901766394407, -0.063698206555349399, nan, 0.002617801047120419, 0.004132231404958678, 0.004098360655737705, 0.0048543689320388345, 0.003215434083601286, 0.002/home/jasonzhang/anaconda2/lib/python2.7/site-packages/numpy/lib/nanfunctions.py:703: RuntimeWarning: Mean of empty slice
  warnings.warn("Mean of empty slice", RuntimeWarning)
/home/jasonzhang/anaconda2/lib/python2.7/site-packages/numpy/lib/nanfunctions.py:1202: RuntimeWarning: Degrees of freedom <= 0 for slice.
  warnings.warn("Degrees of freedom <= 0 for slice.", RuntimeWarning)
7472527472527475, 0.9880952380952381, nan, 0.013888888888888888, 0.015384615384615385, 0.011904761904761904, 0.011904761904761904, 0.005208333333333333, 0.9955357142857143, 0.9974619289340102, nan, 0.004424778761061947, 0.006802721088435374, 0.0030581039755351682, 0.0026954177897574125, 0.9954954954954955, 0.9975062344139651, 0.9958333333333333, nan, 0.004672897196261682, 0.0024691358024691358, 0.002881844380403458, 0.9961538461538462, 0.9973821989528796, 0.9968652037617555, 0.996031746031746, nan, 0.003115264797507788, 0.002702702702702703, 0.9935483870967742, 0.9973821989528796, 0.9928057553956835, 0.9836065573770492, 0.993103448275862, nan, 0.0043859649122807015, 0.9901960784313726, 0.9963503649635036, 0.9894736842105263, 0.9915966386554622, 0.9895833333333334, 0.9957983193277311, nan, 0, 380, 240, 242, 204, 309, 362, 82, 0, 70, 63, 82, 82, 190, 222, 392, 0, 224, 145, 325, 369, 220, 399, 238, 0, 212, 403, 345, 258, 380, 317, 250, 0, 319, 368, 153, 380, 137, 59, 143, 0, 226, 100, 272, 93, 117, 94, 236, 0, 0.067771313259132313]
N, d, L:  1664 18 7
loss_valid 0 1.17152024348
loss_train 0 1.20321772217
loss_valid 1 1.01914802703
loss_train 1 1.05202008668
loss_valid 2 0.892326836688
loss_train 2 0.92640932363
loss_valid 3 0.784099167417
loss_train 3 0.819701124987
loss_valid 4 0.691994990994
loss_train 4 0.728675104279
loss_valid 5 0.615626373628
loss_train 5 0.651739895816
loss_valid 6 0.554366174137
loss_train 6 0.587762120182
loss_valid 7 0.505849514382
loss_train 7 0.534677025264
loss_valid 8 0.466928369315
loss_train 8 0.490489735769
loss_valid 9 0.434794716767
loss_train 9 0.453356132994
loss_valid 10 0.407105805927
loss_train 10 0.421609156782
loss_valid 11 0.3822147471
loss_train 11 0.394149968883
loss_valid 12 0.359456928625
loss_train 12 0.370407510769
loss_valid 13 0.339558622016
loss_train 13 0.350125459587
loss_valid 14 nan
loss_train 14 nan
nan train loss
early stop at epoch 14
NONERECALL: 0.0 for emoticon: 0
[0.0, 0, 0, 0.0, 0.77056277056277056, 0.093073593073593072, 0.44155844155844154, 0.46969696969696972, 0.85064935064935066, 1.0, 0.001, 0.7705627705627706, 0.09307359307359307, 0.44155844155844154, 0.4696969696969697, 0.8506493506493507, 1.0, 462.0, 462.0, 462.0, 462.0, 462.0, 462.0, 462.0, 0, 0.19967008550704182, -0.047619047619047616, nan, 0.0027472527472527475, 0.004219409282700422, 0.004291845493562232, 0.005405405405405406, 0.0034602076124567475, 0.002840909090909091, 0.9901960784313726, nan, 0.011494252873563218, 0.013888888888888888, 0.009900990099009901, 0.010526315789473684, 0.005076142131979695, 0.9956331877729258, 0.9973614775725593, nan, 0.0045871559633027525, 0.005847953216374269, 0.0032258064516129032, 0.0027624309392265192, 0.9957081545064378, 0.9974619289340102, 0.9959677419354839, nan, 0.0045045045045045045, 0.002531645569620253, 0.0028653295128939827, 0.99644128113879, 0.9972602739726028, 0.9966101694915255, 0.9959016393442623, nan, 0.003355704697986577, 0.0027624309392265192, 0.9943502824858758, 0.9973045822102425, 0.9935897435897436, 0.9859154929577465, 0.9940476190476191, nan, 0.0044444444444444444, 0.9912280701754386, 0.9962825278810409, 0.9903846153846154, 0.9914529914529915, 0.9903846153846154, 0.995850622406639, nan, 0, 362, 235, 231, 183, 287, 350, 100, 0, 85, 70, 99, 93, 195, 227, 377, 0, 216, 169, 308, 360, 231, 392, 246, 0, 220, 393, 347, 279, 363, 293, 242, 0, 296, 360, 175, 369, 154, 69, 166, 0, 223, 112, 267, 102, 115, 102, 239, 0, 0.06740036212658998]
{'perf': [array([  1.11255411e-01,   0.00000000e+00,   0.00000000e+00,
         1.80952381e-01,   8.12554113e-01,   2.49350649e-01,
         5.46320346e-01,   5.38528139e-01,   8.45454545e-01,
         9.48051948e-01,   1.81752381e-01,   8.12554113e-01,
         2.49350649e-01,   5.46320346e-01,   5.38528139e-01,
         8.45454545e-01,   9.48051948e-01,   4.62000000e+02,
         4.62000000e+02,   4.62000000e+02,   4.62000000e+02,
         4.62000000e+02,   4.62000000e+02,   4.62000000e+02,
         0.00000000e+00,   3.31349921e-01,   1.23314780e-01,
                    nan,   1.96581483e-01,   1.76971627e-01,
         1.99127859e-01,   1.68122797e-01,   1.92982289e-01,
         2.00537062e-01,   9.16625358e-01,              nan,
         1.53896723e-01,   1.77543646e-01,   1.44480849e-01,
         1.46728079e-01,   1.79158461e-01,   9.67045928e-01,
         9.88428188e-01,              nan,   1.82800766e-01,
         1.75162389e-01,   1.96092123e-01,   2.00514900e-01,
         9.66321101e-01,   9.88282382e-01,   9.87211481e-01,
                    nan,   1.86494513e-01,   1.98464809e-01,
         2.00551253e-01,   9.74825525e-01,   9.90741187e-01,
         9.84028900e-01,   9.94398171e-01,              nan,
         1.94631286e-01,   2.01070347e-01,   9.51108473e-01,
         9.88699586e-01,   9.53888804e-01,   8.33556698e-01,
         9.59475336e-01,              nan,   1.83111356e-01,
         9.20022325e-01,   9.83404887e-01,   9.26559044e-01,
         9.04882380e-01,   9.25334867e-01,   9.80912662e-01,
                    nan,   0.00000000e+00,   3.72000000e+02,
         2.35800000e+02,   2.40400000e+02,   1.85200000e+02,
         2.95600000e+02,   3.54800000e+02,   9.00000000e+01,
         0.00000000e+00,   7.58000000e+01,   6.56000000e+01,
         8.86000000e+01,   8.68000000e+01,   1.81800000e+02,
         2.26200000e+02,   3.86200000e+02,   0.00000000e+00,
         2.28000000e+02,   1.57200000e+02,   3.18600000e+02,
         3.62600000e+02,   2.21600000e+02,   3.96400000e+02,
         2.34000000e+02,   0.00000000e+00,   2.12200000e+02,
         3.97200000e+02,   3.36800000e+02,   2.76800000e+02,
         3.73400000e+02,   3.04800000e+02,   2.49800000e+02,
         0.00000000e+00,   3.06800000e+02,   3.62200000e+02,
         1.66400000e+02,   3.75200000e+02,   1.43400000e+02,
         6.48000000e+01,   1.55200000e+02,   0.00000000e+00,
         2.23000000e+02,   1.07200000e+02,   2.80200000e+02,
         9.94000000e+01,   1.25200000e+02,   9.98000000e+01,
         2.39000000e+02,   0.00000000e+00,   2.20493455e-01]), array([  2.22510823e-01,   0.00000000e+00,   0.00000000e+00,
         3.61904762e-01,   3.69768944e-02,   3.09465983e-01,
         1.77402576e-01,   1.98358172e-01,   3.38272287e-02,
         1.03896104e-01,   3.61504762e-01,   3.69768944e-02,
         3.09465983e-01,   1.77402576e-01,   1.98358172e-01,
         3.38272287e-02,   1.03896104e-01,   0.00000000e+00,
         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,
         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,
         0.00000000e+00,   2.61916729e-01,   3.44740420e-01,
                    nan,   3.87858846e-01,   3.45556741e-01,
         3.90019416e-01,   3.25513186e-01,   3.79287411e-01,
         3.95482179e-01,   1.44029765e-01,              nan,
         2.81008018e-01,   3.24562495e-01,   2.65996274e-01,
         2.70386641e-01,   3.47263148e-01,   5.71160517e-02,
         1.80236652e-02,              nan,   3.56875507e-01,
         3.37568645e-01,   3.85979503e-01,   3.95564279e-01,
         5.83819019e-02,   1.84378798e-02,   1.71105251e-02,
                    nan,   3.63570972e-01,   3.91929212e-01,
         3.95219876e-01,   4.31683717e-02,   1.32278334e-02,
         2.54594009e-02,   3.29890476e-03,              nan,
         3.82816815e-01,   3.96671532e-01,   8.57241423e-02,
         1.73227972e-02,   7.82521572e-02,   3.02493553e-01,
         6.81529608e-02,              nan,   3.57333213e-01,
         1.41427780e-01,   2.61232297e-02,   1.26831728e-01,
         1.74246442e-01,   1.29334811e-01,   2.98754322e-02,
                    nan,   0.00000000e+00,   9.85900604e+00,
         2.48193473e+00,   5.88557559e+00,   1.01469207e+01,
         8.47584804e+00,   5.49181209e+00,   9.85900604e+00,
         0.00000000e+00,   8.03492377e+00,   5.12249939e+00,
         9.11262860e+00,   5.91269820e+00,   1.37753403e+01,
         2.48193473e+00,   8.03492377e+00,   0.00000000e+00,
         7.48331477e+00,   8.68101377e+00,   7.49933330e+00,
         5.31413210e+00,   5.88557559e+00,   5.12249939e+00,
         7.48331477e+00,   0.00000000e+00,   8.54166260e+00,
         3.60000000e+00,   8.90842298e+00,   1.01469207e+01,
         9.11262860e+00,   8.68101377e+00,   8.54166260e+00,
         0.00000000e+00,   8.08455317e+00,   5.30659966e+00,
         8.47584804e+00,   5.91269820e+00,   7.49933330e+00,
         3.60000000e+00,   8.08455317e+00,   0.00000000e+00,
         1.67332005e+00,   5.49181209e+00,   1.37753403e+01,
         5.31413210e+00,   8.90842298e+00,   5.30659966e+00,
         1.67332005e+00,   0.00000000e+00,   3.04898332e-01])]}
