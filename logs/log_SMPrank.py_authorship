N, d, L:  605 70 4
distinct features:  100 distinct pairlabel:  17
Traceback (most recent call last):
  File "SMPrank.py", line 366, in <module>
    results = crossValidate(x,y,K=100)
  File "SMPrank.py", line 342, in crossValidate
    y_pred = SmpRank(K=K).fit(x_train, y_train).predict(x_test)
  File "SMPrank.py", line 70, in fit
    self.initialize(N,d,L, x_train, y_train)
  File "SMPrank.py", line 229, in initialize
    raise ValueError("too many prototypes")
ValueError: too many prototypes
/home/jasonzhang/anaconda2/lib/python2.7/site-packages/numpy/ma/core.py:867: RuntimeWarning: invalid value encountered in less_equal
  return umath.less_equal(x, self.critical_value)
dataset, N,d,L,K authorship 841 70 4 12
N, d, L:  605 70 4
loss_valid 0 1.50043867718
loss_train 0 1.52727128258
loss_valid 1 1.2002862444
loss_train 1 1.25927850043
loss_valid 2 0.905053284043
loss_train 2 1.01676190444
loss_valid 3 0.706613823377
loss_train 3 0.846478464263
loss_valid 4 0.577431277799
loss_train 4 0.722355833699
loss_valid 5 0.483634126181
loss_train 5 0.624815831028
loss_valid 6 0.416291867037
loss_train 6 0.550245634091
loss_valid 7 0.369220360583
loss_train 7 0.494362302489
loss_valid 8 0.337089561203
loss_train 8 0.452747852642
loss_valid 9 0.314043590729
loss_train 9 0.421352587061
loss_valid 10 0.296074956523
loss_train 10 0.397214923797
loss_valid 11 0.283551970178
loss_train 11 0.378036238113
loss_valid 12 0.273113770109
loss_train 12 0.362419333369
loss_valid 13 0.264409827059
loss_train 13 0.349526591666
loss_valid 14 0.257944908575
loss_train 14 0.338777869792
loss_valid 15 0.251846025774
loss_train 15 0.329818438707
loss_valid 16 0.247354897554
loss_train 16 0.32219715422
loss_valid 17 0.243214283631
loss_train 17 0.315600379229
loss_valid 18 0.23943888274
loss_train 18 0.309928495843
loss_valid 19 0.236308795482
loss_train 19 0.304982257138
loss_valid 20 0.233663074736
loss_train 20 0.300636110298
loss_valid 21 0.231129271045
loss_train 21 0.296751084437
loss_valid 22 0.228962660926
loss_train 22 0.293247225795
loss_valid 23 0.227312826763
loss_train 23 0.289994843639
loss_valid 24 0.225691886039
loss_train 24 0.28698424801
loss_valid 25 0.224442257014
loss_train 25 0.284092080331
loss_valid 26 0.223325373761
loss_train 26 0.281410034935
loss_valid 27 0.222266937284
loss_train 27 0.278891121842
loss_valid 28 0.221331920529
loss_train 28 0.276635270247
loss_valid 29 0.220501893201
loss_train 29 0.274617731938
loss_valid 30 0.220033154331
loss_train 30 0.272835364761
loss_valid 31 0.219363354824
loss_train 31 0.271222099036
loss_valid 32 0.218578904456
loss_train 32 0.269685052219
loss_valid 33 0.218076385729
loss_train 33 0.268101792973
loss_valid 34 0.217508301891
loss_train 34 0.266276748969
loss_valid 35 0.21667046804
loss_train 35 0.264116211035
loss_valid 36 0.215758155968
loss_train 36 0.261870967046
loss_valid 37 0.21471384649
loss_train 37 0.260095583821
loss_valid 38 0.21426665607
loss_train 38 0.258736832081
loss_valid 39 0.213715910608
loss_train 39 0.257575010934
loss_valid 40 0.213247770828
loss_train 40 0.256557311984
loss_valid 41 0.212931103273
loss_train 41 0.255549649513
loss_valid 42 0.212361848509
loss_train 42 0.25465588737
loss_valid 43 0.21160827446
loss_train 43 0.253893524011
loss_valid 44 0.211216335093
loss_train 44 0.253125591608
loss_valid 45 0.210741022077
loss_train 45 0.252433745141
loss_valid 46 0.210170669112
loss_train 46 0.25179414686
loss_valid 47 0.209604544319
loss_train 47 0.251181280286
loss_valid 48 0.209371076723
loss_train 48 0.250557977695
loss_valid 49 0.209134421125
loss_train 49 0.24995693168
loss_valid 50 0.208875797301
loss_train 50 0.249399018854
loss_valid 51 0.208830423909
loss_train 51 0.248911779693
loss_valid 52 0.208851611914
loss_train 52 0.248490610646
early stop at the end of epoch:  52
[0.0650887573964497, 0, 0, 1.0, 0.70414201183431957, 0.89349112426035504, 0.076923076923076927, 1.0, 0.7041420118343196, 0.893491124260355, 0.07692307692307693, 169.0, 169.0, 169.0, 169.0, 0, 0.46903125810497331, 0.27810650887573962, nan, 0.9888888888888889, 0.993421052631579, 0.9917355371900827, 0.04819277108433735, nan, 0.9934640522875817, 0.9908256880733946, 0.047619047619047616, 0.05, nan, 0.9523809523809523, 0.019230769230769232, 0.015625, 0.07894736842105263, nan, 0, 88, 150, 119, 81, 0, 151, 107, 19, 18, 0, 19, 50, 62, 150, 0, 0.19184934306278989]
N, d, L:  606 70 4
loss_valid 0 1.13815511959
loss_train 0 1.20072317189
loss_valid 1 0.9581519142
loss_train 1 1.04483941044
loss_valid 2 0.819264277189
loss_train 2 0.929307525063
loss_valid 3 0.694208018454
loss_train 3 0.823231149366
loss_valid 4 0.585636729285
loss_train 4 0.725593835079
loss_valid 5 0.498850248726
loss_train 5 0.639475973143
loss_valid 6 0.433655308567
loss_train 6 0.566552710879
loss_valid 7 0.386654364483
loss_train 7 0.50690824332
loss_valid 8 0.35316802503
loss_train 8 0.459539431123
loss_valid 9 0.32921771277
loss_train 9 0.422636468916
loss_valid 10 0.311082934704
loss_train 10 0.393721829478
loss_valid 11 0.297205112665
loss_train 11 0.370988493234
loss_valid 12 0.286548103433
loss_train 12 0.352716464858
loss_valid 13 0.278513438257
loss_train 13 0.337966251881
loss_valid 14 0.272892626192
loss_train 14 0.325934425688
loss_valid 15 0.268554519195
loss_train 15 0.315978975133
loss_valid 16 0.265564190096
loss_train 16 0.307582792436
loss_valid 17 0.263204733371
loss_train 17 0.300374988715
loss_valid 18 0.261296808413
loss_train 18 0.294089380846
loss_valid 19 0.259732527513
loss_train 19 0.288441471423
loss_valid 20 0.258347689373
loss_train 20 0.283287209075
loss_valid 21 0.257058611934
loss_train 21 0.278645125014
loss_valid 22 0.255843073103
loss_train 22 0.274348879921
loss_valid 23 0.254836722114
loss_train 23 0.270427157878
loss_valid 24 0.253970286852
loss_train 24 0.266727436803
loss_valid 25 0.253168588228
loss_train 25 0.263212218606
loss_valid 26 0.252491311892
loss_train 26 0.259798156065
loss_valid 27 0.251997701848
loss_train 27 0.25635359304
loss_valid 28 0.251601947288
loss_train 28 0.253121744471
loss_valid 29 0.251375156295
loss_train 29 0.250235502415
loss_valid 30 0.251169989594
loss_train 30 0.247766377515
loss_valid 31 0.251071506756
loss_train 31 0.245650992761
loss_valid 32 0.251115067827
loss_train 32 0.243789256553
early stop at the end of epoch:  32
[0.7380952380952381, 0, 0, 0.9642857142857143, 0.86309523809523814, 0.9285714285714286, 0.94047619047619047, 0.9642857142857143, 0.8630952380952381, 0.9285714285714286, 0.9404761904761905, 168.0, 168.0, 168.0, 168.0, 0, 0.92332988796571969, 0.88293650793650802, nan, 0.9318181818181818, 0.9875, 0.9848484848484849, 0.7380952380952381, nan, 0.9875776397515528, 0.9612403100775194, 0.6666666666666666, 0.7272727272727273, nan, 0.5454545454545454, 0.95, 0.8604651162790697, 0.9533333333333334, nan, 0, 86, 158, 130, 82, 0, 159, 127, 10, 9, 0, 20, 38, 41, 148, 0, 0.84411502898037571]
N, d, L:  606 70 4
loss_valid 0 1.34438429308
loss_train 0 1.33858404772
loss_valid 1 1.18514389718
loss_train 1 1.17451027724
loss_valid 2 1.07484293763
loss_train 2 1.06468651222
loss_valid 3 0.989197820449
loss_train 3 0.982439063738
loss_valid 4 0.919763278591
loss_train 4 0.913907062047
loss_valid 5 0.854336576613
loss_train 5 0.846244689754
loss_valid 6 0.787724426479
loss_train 6 0.775718147659
loss_valid 7 0.717391596044
loss_train 7 0.702670502763
loss_valid 8 0.651865206508
loss_train 8 0.633111357724
loss_valid 9 0.597093345373
loss_train 9 0.572696617297
loss_valid 10 0.553213816327
loss_train 10 0.523091450886
loss_valid 11 0.519163354737
loss_train 11 0.484274529192
loss_valid 12 0.493354570642
loss_train 12 0.453884165757
loss_valid 13 0.47337920386
loss_train 13 0.42971319356
loss_valid 14 0.458960014444
loss_train 14 0.410069882551
loss_valid 15 0.446756650227
loss_train 15 0.393858618239
loss_valid 16 0.436844105182
loss_train 16 0.380188550031
loss_valid 17 0.428924756689
loss_train 17 0.368528869011
loss_valid 18 0.422713505692
loss_train 18 0.358535365593
loss_valid 19 0.417016551962
loss_train 19 0.349798242912
loss_valid 20 0.412987251502
loss_train 20 0.342245349376
loss_valid 21 0.409776478849
loss_train 21 0.335591962586
loss_valid 22 0.40639802123
loss_train 22 0.329836673183
loss_valid 23 0.404733821397
loss_train 23 0.324772072715
loss_valid 24 0.403042296619
loss_train 24 0.320329234918
loss_valid 25 0.402174215626
loss_train 25 0.31643438013
loss_valid 26 0.402592770614
loss_train 26 0.312999360706
early stop at the end of epoch:  26
[0.7083333333333334, 0, 0, 0.92261904761904767, 0.92261904761904767, 0.95833333333333337, 0.875, 0.9226190476190477, 0.9226190476190477, 0.9583333333333334, 0.875, 168.0, 168.0, 168.0, 168.0, 0, 0.91916265315233581, 0.84325396825396814, nan, 0.8823529411764706, 0.9683544303797469, 0.9435483870967742, 0.9080459770114943, nan, 0.9683544303797469, 0.9210526315789473, 0.7857142857142857, 0.7857142857142857, nan, 0.631578947368421, 0.8125, 0.6379310344827587, 0.9738562091503268, nan, 0, 83, 156, 122, 85, 0, 156, 112, 12, 12, 0, 17, 46, 56, 151, 0, 0.8428359098062872]
N, d, L:  606 70 4
loss_valid 0 1.30240820084
loss_train 0 1.39462942201
loss_valid 1 1.10311617791
loss_train 1 1.2242571174
loss_valid 2 0.982430295773
loss_train 2 1.11523249623
loss_valid 3 0.886945145739
loss_train 3 1.02156816106
loss_valid 4 0.800421419371
loss_train 4 0.933815287132
loss_valid 5 0.726968361626
loss_train 5 0.851791409616
loss_valid 6 0.661089545312
loss_train 6 0.778312668176
loss_valid 7 0.604334150506
loss_train 7 0.710932360809
loss_valid 8 0.550974877761
loss_train 8 0.645739740449
loss_valid 9 0.500585684675
loss_train 9 0.581767231051
loss_valid 10 0.451006872479
loss_train 10 0.520187750079
loss_valid 11 0.407010757249
loss_train 11 0.468139563054
loss_valid 12 0.37173736507
loss_train 12 0.428343022683
loss_valid 13 0.344548388054
loss_train 13 0.399721458531
loss_valid 14 0.323012451604
loss_train 14 0.379385201824
loss_valid 15 0.306099413764
loss_train 15 0.364384616396
loss_valid 16 0.292501193406
loss_train 16 0.352704814313
loss_valid 17 0.282135074217
loss_train 17 0.343125426153
loss_valid 18 0.273644759957
loss_train 18 0.334809446915
loss_valid 19 0.267471515888
loss_train 19 0.327445515827
loss_valid 20 0.262948797702
loss_train 20 0.320808148615
loss_valid 21 0.25965004731
loss_train 21 0.314967786926
loss_valid 22 0.257103967935
loss_train 22 0.309781521103
loss_valid 23 0.255236777361
loss_train 23 0.3053127826
loss_valid 24 0.253757547661
loss_train 24 0.301430533403
loss_valid 25 0.252479081089
loss_train 25 0.298024304736
loss_valid 26 0.251601675284
loss_train 26 0.295065370611
loss_valid 27 0.250805600948
loss_train 27 0.292526186342
loss_valid 28 0.25028016844
loss_train 28 0.290305633262
loss_valid 29 0.249677678615
loss_train 29 0.288423338058
loss_valid 30 0.249297385992
loss_train 30 0.286785211995
loss_valid 31 0.248937133658
loss_train 31 0.285393530571
loss_valid 32 0.248617502615
loss_train 32 0.28420584435
loss_valid 33 0.248352014256
loss_train 33 0.283170622129
loss_valid 34 0.248139847319
loss_train 34 0.282277694247
loss_valid 35 0.247857419236
loss_train 35 0.28150569053
loss_valid 36 0.247611935637
loss_train 36 0.280822217845
loss_valid 37 0.24741546076
loss_train 37 0.28022341154
loss_valid 38 0.247295924422
loss_train 38 0.27968381138
loss_valid 39 0.247136219381
loss_train 39 0.279200171595
loss_valid 40 0.246938411907
loss_train 40 0.278763463177
loss_valid 41 0.246756422963
loss_train 41 0.278364204559
loss_valid 42 0.246701880119
loss_train 42 0.277997059282
loss_valid 43 0.246612976729
loss_train 43 0.277654694029
loss_valid 44 0.246544907516
loss_train 44 0.277328711553
loss_valid 45 0.246442413808
loss_train 45 0.277016650771
loss_valid 46 0.246347145591
loss_train 46 0.276710619464
loss_valid 47 0.246294399704
loss_train 47 0.276410742048
loss_valid 48 0.246239220464
loss_train 48 0.276103969747
loss_valid 49 0.24617501335
loss_train 49 0.275800637545
loss_valid 50 0.246164351583
loss_train 50 0.275498425917
loss_valid 51 0.246108343233
loss_train 51 0.275193838104
loss_valid 52 0.246032211951
loss_train 52 0.27489018565
loss_valid 53 0.245915596526
loss_train 53 0.274582183085
loss_valid 54 0.245764227303
loss_train 54 0.274264428971
loss_valid 55 0.245564328486
loss_train 55 0.273940033579
loss_valid 56 0.2453618585
loss_train 56 0.273625958613
loss_valid 57 0.245271007733
loss_train 57 0.273326988493
loss_valid 58 0.245171201344
loss_train 58 0.273043881311
loss_valid 59 0.245117744681
loss_train 59 0.272766171202
loss_valid 60 0.24513477957
loss_train 60 0.272484403569
early stop at the end of epoch:  60
[0.0, 0, 0, 1.0, 0.65476190476190477, 0.94047619047619047, 0.041666666666666664, 1.0, 0.6547619047619048, 0.9404761904761905, 0.041666666666666664, 168.0, 168.0, 168.0, 168.0, 0, 0.40022571737762414, 0.29166666666666663, nan, 0.9891304347826086, 0.9936708860759493, 0.991869918699187, 0.0125, nan, 0.99375, 0.9911504424778761, 0.07142857142857142, 0.083333333333/home/jasonzhang/anaconda2/lib/python2.7/site-packages/numpy/lib/nanfunctions.py:703: RuntimeWarning: Mean of empty slice
  warnings.warn("Mean of empty slice", RuntimeWarning)
/home/jasonzhang/anaconda2/lib/python2.7/site-packages/numpy/lib/nanfunctions.py:1202: RuntimeWarning: Degrees of freedom <= 0 for slice.
  warnings.warn("Degrees of freedom <= 0 for slice.", RuntimeWarning)
33333, nan, 0.9411764705882353, 0.02040816326530612, 0.01694915254237288, 0.0064516129032258064, nan, 0, 90, 156, 121, 78, 0, 158, 111, 12, 10, 0, 15, 47, 57, 153, 0, 0.15182908139264414]
N, d, L:  606 70 4
loss_valid 0 1.35681482801
loss_train 0 1.37304551306
loss_valid 1 1.11088554859
loss_train 1 1.13843975394
loss_valid 2 0.899337072969
loss_train 2 0.954082322341
loss_valid 3 0.737751963366
loss_train 3 0.81375438642
loss_valid 4 0.61855046587
loss_train 4 0.704802021988
loss_valid 5 0.529638302819
loss_train 5 0.619066720743
loss_valid 6 0.464934048601
loss_train 6 0.551202883792
loss_valid 7 0.418399965451
loss_train 7 0.49774101849
loss_valid 8 0.385540220696
loss_train 8 0.455554911273
loss_valid 9 0.362036277544
loss_train 9 0.42315987794
loss_valid 10 0.345002590618
loss_train 10 0.398417699902
loss_valid 11 0.333030230081
loss_train 11 0.379476258676
loss_valid 12 0.323950714686
loss_train 12 0.364591541081
loss_valid 13 0.317602650011
loss_train 13 0.35272837898
loss_valid 14 0.312982665362
loss_train 14 0.343275305038
loss_valid 15 0.309811479726
loss_train 15 0.335660451323
loss_valid 16 0.307665012641
loss_train 16 0.329439816674
loss_valid 17 0.306432231905
loss_train 17 0.32427325151
loss_valid 18 0.30532186991
loss_train 18 0.319796685848
loss_valid 19 0.304534011941
loss_train 19 0.315851878459
loss_valid 20 0.304065797143
loss_train 20 0.312271348859
loss_valid 21 0.303528061581
loss_train 21 0.308913583675
loss_valid 22 0.303087084906
loss_train 22 0.305743132463
loss_valid 23 0.302931290536
loss_train 23 0.302679076245
loss_valid 24 0.302801141543
loss_train 24 0.299717974273
loss_valid 25 0.302637023854
loss_train 25 0.296832783352
loss_valid 26 0.302581736395
loss_train 26 0.294067396579
loss_valid 27 0.301992424847
loss_train 27 0.291475236843
loss_valid 28 0.301389586378
loss_train 28 0.28910742765
loss_valid 29 0.300853295496
loss_train 29 0.286922630851
loss_valid 30 0.300364166706
loss_train 30 0.285032698851
loss_valid 31 0.299962628956
loss_train 31 0.283393831091
loss_valid 32 0.299840189935
loss_train 32 0.281972487286
loss_valid 33 0.299774522838
loss_train 33 0.280664635227
loss_valid 34 0.29977656465
loss_train 34 0.279430747417
early stop at the end of epoch:  34
[0.7559523809523809, 0, 0, 0.97619047619047616, 0.8928571428571429, 0.9642857142857143, 0.86904761904761907, 0.9761904761904762, 0.8928571428571429, 0.9642857142857143, 0.8690476190476191, 168.0, 168.0, 168.0, 168.0, 0, 0.92446710545795696, 0.88293650793650802, nan, 0.9696969696969697, 0.9875, 0.9761904761904762, 0.7671232876712328, nan, 0.9875776397515528, 0.9658119658119658, 0.5, 0.5454545454545454, nan, 0.9, 0.8478260869565217, 0.6909090909090909, 0.9753086419753086, nan, 0, 97, 158, 124, 71, 0, 159, 115, 10, 9, 0, 8, 44, 53, 160, 0, 0.82253892960887731]
{'perf': [array([   0.45349394,    0.        ,    0.        ,    0.97261905,
          0.80749507,    0.93703156,    0.56062271,    0.97261905,
          0.80749507,    0.93703156,    0.56062271,  168.2       ,
        168.2       ,  168.2       ,  168.2       ,    0.        ,
          0.72724332,    0.63578003,           nan,    0.95237748,
          0.98608927,    0.97763856,    0.49479145,           nan,
          0.98614475,    0.96601621,    0.41428571,    0.43835498,
                 nan,    0.79411818,    0.529993  ,    0.44437588,
          0.59757943,           nan,    0.        ,   88.8       ,
        155.6       ,  123.2       ,   79.4       ,    0.        ,
        156.6       ,  114.4       ,   12.6       ,   11.6       ,
          0.        ,   15.8       ,   45.        ,   53.8       ,
        152.4       ,    0.        ,    0.57063366]), array([ 0.34465561,  0.        ,  0.        ,  0.02857143,  0.10736933,
        0.02520224,  0.41025075,  0.02857143,  0.10736933,  0.02520224,
        0.41025075,  0.4       ,  0.4       ,  0.4       ,  0.4       ,
        0.        ,  0.23991422,  0.28690153,         nan,  0.04076751,
        0.00927082,  0.01798597,  0.38371763,         nan,  0.00929531,
        0.02565858,  0.30364378,  0.31383506,         nan,  0.17096012,
        0.41899611,  0.35718158,  0.45370381,         nan,  0.        ,
        4.70744092,  2.93938769,  3.76297754,  4.7581509 ,  0.        ,
        3.00665928,  6.8       ,  3.32264955,  3.38230691,  0.        ,
        4.26145515,  4.        ,  7.02566723,  4.12795349,  0.        ,
        0.32595006])]}
