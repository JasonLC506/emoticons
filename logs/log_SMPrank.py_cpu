SMPrank.py:290: RuntimeWarning: invalid value encountered in double_scalars
  self.probfeature[k] = self.probfeature[k] / sum_probf
/home/jasonzhang/anaconda2/lib/python2.7/site-packages/numpy/ma/core.py:867: RuntimeWarning: invalid value encountered in less_equal
  return umath.less_equal(x, self.critical_value)
N, d, L:  5898 6 5
loss_valid 0 1.30680677972
loss_train 0 1.30682567872
loss_valid 1 nan
loss_train 1 nan
loss_valid 2 nan
loss_train 2 nan
loss_valid 3 nan
loss_train 3 nan
loss_valid 4 nan
loss_train 4 nan
loss_valid 5 nan
loss_train 5 nan
loss_valid 6 nan
loss_train 6 nan
loss_valid 7 nan
loss_train 7 nan
loss_valid 8 nan
loss_train 8 nan
loss_valid 9 nan
loss_train 9 nan
loss_valid 10 nan
loss_train 10 nan
loss_valid 11 nan
loss_train 11 nan
loss_valid 12 nan
loss_train 12 nan
loss_valid 13 nan
loss_train 13 nan
loss_valid 14 nan
loss_train 14 nan
loss_valid 15 nan
loss_train 15 nan
loss_valid 16 nan
loss_train 16 nan
loss_valid 17 nan
loss_train 17 nan
loss_valid 18 nan
loss_train 18 nan
loss_valid 19 nan
loss_train 19 nan
loss_valid 20 nan
loss_train 20 nan
loss_valid 21 nan
loss_train 21 nan
loss_valid 22 nan
loss_train 22 nan
loss_valid 23 nan
loss_train 23 nan
loss_valid 24 nan
loss_train 24 nan
loss_valid 25 nan
loss_train 25 nan
loss_valid 26 nan
loss_train 26 nan
loss_valid 27 nan
loss_train 27 nan
loss_valid 28 nan
loss_train 28 nan
loss_valid 29 nan
loss_train 29 nan
loss_valid 30 nan
loss_train 30 nan
loss_valid 31 nan
loss_train 31 nan
loss_valid 32 nan
loss_train 32 nan
loss_valid 33 nan
loss_train 33 nan
loss_valid 34 nan
loss_train 34 nan
loss_valid 35 nan
loss_train 35 nan
loss_valid 36 nan
loss_train 36 nan
loss_valid 37 nan
loss_train 37 nan
loss_valid 38 nan
loss_train 38 nan
loss_valid 39 nan
loss_train 39 nan
loss_valid 40 nan
loss_train 40 nan
loss_valid 41 nan
loss_train 41 nan
loss_valid 42 nan
loss_train 42 nan
loss_valid 43 nan
loss_train 43 nan
loss_valid 44 nan
loss_train 44 nan
loss_valid 45 nan
loss_train 45 nan
loss_valid 46 nan
loss_train 46 nan
loss_valid 47 nan
loss_train 47 nan
loss_valid 48 nan
loss_train 48 nan
loss_valid 49 nan
loss_train 49 nan
loss_valid 50 nan
loss_train 50 nan
loss_valid 51 nan
loss_train 51 nan
loss_valid 52 nan
loss_train 52 nan
loss_valid 53 nan
loss_train 53 nan
loss_valid 54 nan
loss_train 54 nan
loss_valid 55 nan
loss_train 55 nan
loss_valid 56 nan
loss_train 56 nan
loss_valid 57 nan
loss_train 57 nan
loss_valid 58 nan
loss_train 58 nan
loss_valid 59 nan
loss_train 59 nan
loss_valid 60 nan
loss_train 60 nan
loss_valid 61 nan
loss_train 61 nan
loss_valid 62 nan
loss_train 62 nan
loss_valid 63 nan
loss_train 63 nan
loss_valid 64 nan
loss_train 64 nan
loss_valid 65 nan
loss_train 65 nan
loss_valid 66 nan
loss_train 66 nan
loss_valid 67 nan
loss_train 67 nan
loss_valid 68 nan
loss_train 68 nan
loss_valid 69 nan
loss_train 69 nan
loss_valid 70 nan
loss_train 70 nan
loss_valid 71 nan
loss_train 71 nan
loss_valid 72 nan
loss_train 72 nan
loss_valid 73 nan
loss_train 73 nan
loss_valid 74 nan
loss_train 74 nan
loss_valid 75 nan
loss_train 75 nan
loss_valid 76 nan
loss_train 76 nan
loss_valid 77 nan
loss_train 77 nan
loss_valid 78 nan
loss_train 78 nan
loss_valid 79 nan
loss_train 79 nan
loss_valid 80 nan
loss_train 80 nan
loss_valid 81 nan
loss_train 81 nan
loss_valid 82 nan
loss_train 82 nan
loss_valid 83 nan
loss_train 83 nan
loss_valid 84 nan
loss_train 84 nan
loss_valid 85 nan
loss_train 85 nan
loss_valid 86 nan
loss_train 86 nan
loss_valid 87 nan
loss_train 87 nan
loss_valid 88 nan
loss_train 88 nan
loss_valid 89 nan
loss_train 89 nan
loss_valid 90 nan
loss_train 90 nan
loss_valid 91 nan
loss_train 91 nan
loss_valid 92 nan
loss_train 92 nan
loss_valid 93 nan
loss_train 93 nan
loss_valid 94 nan
loss_train 94 nan
loss_valid 95 nan
loss_train 95 nan
loss_valid 96 nan
loss_train 96 nan
loss_valid 97 nan
loss_train 97 nan
loss_valid 98 nan
loss_train 98 nan
loss_valid 99 nan
loss_train 99 nan
[0.027455765710799267, 0, 0, 0.080536912751677847, 0.49115314215985356, 0.6235509456985967, 0.74130567419158022, 1.0, 0.08053691275167785, 0.49115314215985356, 0.6235509456985967, 0.7413056741915802, 1.0, 1639.0, 1639.0, 1639.0, 1639.0, 1639.0, 0, 0.44917568026575938, -0.018913971934106162, nan, 0.0010141987829614604, 0.001081081081081081, 0.0019047619047619048, 0.0009569377990430622, 0.9984779299847792, nan, 0.0016233766233766235, 0.0015313935681470138, 0.0011806375442739079, 0.9986072423398329, 0.9990262901655307, nan, 0.0017543859649122807, 0.0009596928982725527, 0.9991055456171736, 0.998989898989899, 0.9990680335507922, nan, 0.0008613264427217916, 0.9983277591973244, 0.9987437185929648, 0.9983361064891847, 0.9979253112033195, nan, 0, 984, 923, 523, 1043, 655, 0, 614, 651, 845, 716, 1025, 0, 568, 1040, 1116, 988, 1071, 0, 1159, 596, 794, 599, 480, 0, 0.035171967700775082]
N, d, L:  5898 6 5
loss_valid 0 1.30106578379
loss_train 0 1.30055359205
loss_valid 1 nan
loss_train 1 nan
loss_valid 2 nan
loss_train 2 nan
loss_valid 3 nan
loss_train 3 nan
loss_valid 4 nan
loss_train 4 nan
loss_valid 5 nan
loss_train 5 nan
loss_valid 6 nan
loss_train 6 nan
loss_valid 7 nan
loss_train 7 nan
loss_valid 8 nan
loss_train 8 nan
loss_valid 9 nan
loss_train 9 nan
loss_valid 10 nan
loss_train 10 nan
loss_valid 11 nan
loss_train 11 nan
loss_valid 12 nan
loss_train 12 nan
loss_valid 13 nan
loss_train 13 nan
loss_valid 14 nan
loss_train 14 nan
loss_valid 15 nan
loss_train 15 nan
loss_valid 16 nan
loss_train 16 nan
loss_valid 17 nan
loss_train 17 nan
loss_valid 18 nan
loss_train 18 nan
loss_valid 19 nan
loss_train 19 nan
loss_valid 20 nan
loss_train 20 nan
loss_valid 21 nan
loss_train 21 nan
loss_valid 22 nan
loss_train 22 nan
loss_valid 23 nan
loss_train 23 nan
loss_valid 24 nan
loss_train 24 nan
loss_valid 25 nan
loss_train 25 nan
loss_valid 26 nan
loss_train 26 nan
loss_valid 27 nan
loss_train 27 nan
loss_valid 28 nan
loss_train 28 nan
loss_valid 29 nan
loss_train 29 nan
loss_valid 30 nan
loss_train 30 nan
loss_valid 31 nan
loss_train 31 nan
loss_valid 32 nan
loss_train 32 nan
loss_valid 33 nan
loss_train 33 nan
loss_valid 34 nan
loss_train 34 nan
loss_valid 35 nan
loss_train 35 nan
loss_valid 36 nan
loss_train 36 nan
loss_valid 37 nan
loss_train 37 nan
loss_valid 38 nan
loss_train 38 nan
loss_valid 39 nan
loss_train 39 nan
loss_valid 40 nan
loss_train 40 nan
loss_valid 41 nan
loss_train 41 nan
loss_valid 42 nan
loss_train 42 nan
loss_valid 43 nan
loss_train 43 nan
loss_valid 44 nan
loss_train 44 nan
loss_valid 45 nan
loss_train 45 nan
loss_valid 46 nan
loss_train 46 nan
loss_valid 47 nan
loss_train 47 nan
loss_valid 48 nan
loss_train 48 nan
loss_valid 49 nan
loss_train 49 nan
loss_valid 50 nan
loss_train 50 nan
loss_valid 51 nan
loss_train 51 nan
loss_valid 52 nan
loss_train 52 nan
loss_valid 53 nan
loss_train 53 nan
loss_valid 54 nan
loss_train 54 nan
loss_valid 55 nan
loss_train 55 nan
loss_valid 56 nan
loss_train 56 nan
loss_valid 57 nan
loss_train 57 nan
loss_valid 58 nan
loss_train 58 nan
loss_valid 59 nan
loss_train 59 nan
loss_valid 60 nan
loss_train 60 nan
loss_valid 61 nan
loss_train 61 nan
loss_valid 62 nan
loss_train 62 nan
loss_valid 63 nan
loss_train 63 nan
loss_valid 64 nan
loss_train 64 nan
loss_valid 65 nan
loss_train 65 nan
loss_valid 66 nan
loss_train 66 nan
loss_valid 67 nan
loss_train 67 nan
loss_valid 68 nan
loss_train 68 nan
loss_valid 69 nan
loss_train 69 nan
loss_valid 70 nan
loss_train 70 nan
loss_valid 71 nan
loss_train 71 nan
loss_valid 72 nan
loss_train 72 nan
loss_valid 73 nan
loss_train 73 nan
loss_valid 74 nan
loss_train 74 nan
loss_valid 75 nan
loss_train 75 nan
loss_valid 76 nan
loss_train 76 nan
loss_valid 77 nan
loss_train 77 nan
loss_valid 78 nan
loss_train 78 nan
loss_valid 79 nan
loss_train 79 nan
loss_valid 80 nan
loss_train 80 nan
loss_valid 81 nan
loss_train 81 nan
loss_valid 82 nan
loss_train 82 nan
loss_valid 83 nan
loss_train 83 nan
loss_valid 84 nan
loss_train 84 nan
loss_valid 85 nan
loss_train 85 nan
loss_valid 86 nan
loss_train 86 nan
loss_valid 87 nan
loss_train 87 nan
loss_valid 88 nan
loss_train 88 nan
loss_valid 89 nan
loss_train 89 nan
loss_valid 90 nan
loss_train 90 nan
loss_valid 91 nan
loss_train 91 nan
loss_valid 92 nan
loss_train 92 nan
loss_valid 93 nan
loss_train 93 nan
loss_valid 94 nan
loss_train 94 nan
loss_valid 95 nan
loss_train 95 nan
loss_valid 96 nan
loss_train 96 nan
loss_valid 97 nan
loss_train 97 nan
loss_valid 98 nan
loss_train 98 nan
loss_valid 99 nan
loss_train 99 nan
[0.021964612568639415, 0, 0, 0.090298962782184258, 0.46308724832214765, 0.65466748017083587, 0.73825503355704702, 1.0, 0.09029896278218426, 0.46308724832214765, 0.6546674801708359, 0.738255033557047, 1.0, 1639.0, 1639.0, 1639.0, 1639.0, 1639.0, 0, 0.45826274674174039, -0.0082977425259304471, nan, 0.0010834236186348862, 0.001049317943336831, 0.0019193857965451055, 0.000984251968503937, 0.9986111111111111, nan, 0.0014992503748125937, 0.0014970059880239522, 0.0011737089201877935, 0.9985507246376811, 0.9989754098360656, nan, 0.00196078431372549, 0.001004016064257028, 0.9991087344028521, 0.9989743589743589, 0.999117387466902, nan, 0.0008496176720475786, 0.9984051036682615, 0.9987357774968394, 0.9984544049459042, 0.9978540772532188, nan, 0, 921, 951, 519, 1014, 718, 0, 665, 666, 850, 688, 974, 0, 508, 994, 1120, 973, 1131, 0, 1175, 625, 789, 645, 464, 0, 0.035360157068322448]
N, d, L:  5899 6 5
loss_valid 0 1.27305036246
loss_train 0 1.28209762938
loss_valid 1 1.08586722209
loss_train 1 nan
loss_valid 2 nan
loss_train 2 nan
loss_valid 3 nan
loss_train 3 nan
loss_valid 4 nan
loss_train 4 nan
loss_valid 5 nan
loss_train 5 nan
loss_valid 6 nan
loss_train 6 nan
loss_valid 7 nan
loss_train 7 nan
loss_valid 8 nan
loss_train 8 nan
loss_valid 9 nan
loss_train 9 nan
loss_valid 10 nan
loss_train 10 nan
loss_valid 11 nan
loss_train 11 nan
loss_valid 12 nan
loss_train 12 nan
loss_valid 13 nan
loss_train 13 nan
loss_valid 14 nan
loss_train 14 nan
loss_valid 15 nan
loss_train 15 nan
loss_valid 16 nan
loss_train 16 nan
loss_valid 17 nan
loss_train 17 nan
loss_valid 18 nan
loss_train 18 nan
loss_valid 19 nan
loss_train 19 nan
loss_valid 20 nan
loss_train 20 nan
loss_valid 21 nan
loss_train 21 nan
loss_valid 22 nan
loss_train 22 nan
loss_valid 23 nan
loss_train 23 nan
loss_valid 24 nan
loss_train 24 nan
loss_valid 25 nan
loss_train 25 nan
loss_valid 26 nan
loss_train 26 nan
loss_valid 27 nan
loss_train 27 nan
loss_valid 28 nan
loss_train 28 nan
loss_valid 29 nan
loss_train 29 nan
loss_valid 30 nan
loss_train 30 nan
loss_valid 31 nan
loss_train 31 nan
loss_valid 32 nan
loss_train 32 nan
loss_valid 33 nan
loss_train 33 nan
loss_valid 34 nan
loss_train 34 nan
loss_valid 35 nan
loss_train 35 nan
loss_valid 36 nan
loss_train 36 nan
loss_valid 37 nan
loss_train 37 nan
loss_valid 38 nan
loss_train 38 nan
loss_valid 39 nan
loss_train 39 nan
loss_valid 40 nan
loss_train 40 nan
loss_valid 41 nan
loss_train 41 nan
loss_valid 42 nan
loss_train 42 nan
loss_valid 43 nan
loss_train 43 nan
loss_valid 44 nan
loss_train 44 nan
loss_valid 45 nan
loss_train 45 nan
loss_valid 46 nan
loss_train 46 nan
loss_valid 47 nan
loss_train 47 nan
loss_valid 48 nan
loss_train 48 nan
loss_valid 49 nan
loss_train 49 nan
loss_valid 50 nan
loss_train 50 nan
loss_valid 51 nan
loss_train 51 nan
loss_valid 52 nan
loss_train 52 nan
loss_valid 53 nan
loss_train 53 nan
loss_valid 54 nan
loss_train 54 nan
loss_valid 55 nan
loss_train 55 nan
loss_valid 56 nan
loss_train 56 nan
loss_valid 57 nan
loss_train 57 nan
loss_valid 58 nan
loss_train 58 nan
loss_valid 59 nan
loss_train 59 nan
loss_valid 60 nan
loss_train 60 nan
loss_valid 61 nan
loss_train 61 nan
loss_valid 62 nan
loss_train 62 nan
loss_valid 63 nan
loss_train 63 nan
loss_valid 64 nan
loss_train 64 nan
loss_valid 65 nan
loss_train 65 nan
loss_valid 66 nan
loss_train 66 nan
loss_valid 67 nan
loss_train 67 nan
loss_valid 68 nan
loss_train 68 nan
loss_valid 69 nan
loss_train 69 nan
loss_valid 70 nan
loss_train 70 nan
loss_valid 71 nan
loss_train 71 nan
loss_valid 72 nan
loss_train 72 nan
loss_valid 73 nan
loss_train 73 nan
loss_valid 74 nan
loss_train 74 nan
loss_valid 75 nan
loss_train 75 nan
loss_valid 76 nan
loss_train 76 nan
loss_valid 77 nan
loss_train 77 nan
loss_valid 78 nan
loss_train 78 nan
loss_valid 79 nan
loss_train 79 nan
loss_valid 80 nan
loss_train 80 nan
loss_valid 81 nan
loss_train 81 nan
loss_valid 82 nan
loss_train 82 nan
loss_valid 83 nan
loss_train 83 nan
loss_valid 84 nan
loss_train 84 nan
loss_valid 85 nan
loss_train 85 nan
loss_valid 86 nan
loss_train 86 nan
loss_valid 87 nan
loss_train 87 nan
loss_valid 88 nan
loss_train 88 nan
loss_valid 89 nan
loss_train 89 nan
loss_valid 90 nan
loss_train 90 nan
loss_valid 91 nan
loss_train 91 nan
loss_valid 92 nan
loss_train 92 nan
loss_valid 93 nan
loss_train 93 nan
loss_valid 94 nan
loss_train 94 nan
loss_valid 95 nan
loss_train 95 nan
loss_valid 96 nan
loss_train 96 nan
loss_valid 97 nan
loss_train 97 nan
loss_valid 98 nan
loss_train 98 nan
loss_valid 99 nan
loss_train 99 nan
[0.029304029304029304, 0, 0, 0.078754578754578752, 0.45238095238095238, 0.65201465201465203, 0.75641025641025639, 1.0, 0.07875457875457875, 0.4523809523809524, 0.652014652014652, 0.7564102564102564, 1.0, 1638.0, 1638.0, 1638.0, 1638.0, 1638.0, 0, 0.44561415470883736, -0.021855921855921855, nan, 0.0011098779134295228, 0.0010683760683760685, 0.001893939393939394, 0.0009514747859181732, 0.9986504723346828, nan, 0.0014771048744460858, 0.0014858841010401188, 0.0011148272017837235, 0.9985835694050992, 0.9989637305699481, nan, 0.001869158878504673, 0.000975609756097561, 0.9991023339317774, 0.9989680082559339, 0.999096657633243, nan, 0.0008576329331046312, 0.9983079526226735, 0.9986577181208054, 0.9983792544570502, 0.9978991596638656, nan, 0, 899, 934, 526, 1049, 739, 0, 675, 671, 895, 704, 963, 0, 533, 1023, 1112, 967, 1105, 0, 1164, 589, 743, 615, 474, 0, 0.035102538369273163]
N, d, L:  5899 6 5
loss_valid 0 1.33669431107
loss_train 0 1.337405648
loss_valid 1 1.13393037406
loss_train 1 1.13117407272
loss_valid 2 1.02161809882
loss_train 2 nan
loss_valid 3 nan
loss_train 3 nan
loss_valid 4 nan
loss_train 4 nan
loss_valid 5 nan
loss_train 5 nan
loss_valid 6 nan
loss_train 6 nan
loss_valid 7 nan
loss_train 7 nan
loss_valid 8 nan
loss_train 8 nan
loss_valid 9 nan
loss_train 9 nan
loss_valid 10 nan
loss_train 10 nan
loss_valid 11 nan
loss_train 11 nan
loss_valid 12 nan
loss_train 12 nan
loss_valid 13 nan
loss_train 13 nan
loss_valid 14 nan
loss_train 14 nan
loss_valid 15 nan
loss_train 15 nan
loss_valid 16 nan
loss_train 16 nan
loss_valid 17 nan
loss_train 17 nan
loss_valid 18 nan
loss_train 18 nan
loss_valid 19 nan
loss_train 19 nan
loss_valid 20 nan
loss_train 20 nan
loss_valid 21 nan
loss_train 21 nan
loss_valid 22 nan
loss_train 22 nan
loss_valid 23 nan
loss_train 23 nan
loss_valid 24 nan
loss_train 24 nan
loss_valid 25 nan
loss_train 25 nan
loss_valid 26 nan
loss_train 26 nan
loss_valid 27 nan
loss_train 27 nan
loss_valid 28 nan
loss_train 28 nan
loss_valid 29 nan
loss_train 29 nan
loss_valid 30 nan
loss_train 30 nan
loss_valid 31 nan
loss_train 31 nan
loss_valid 32 nan
loss_train 32 nan
loss_valid 33 nan
loss_train 33 nan
loss_valid 34 nan
loss_train 34 nan
loss_valid 35 nan
loss_train 35 nan
loss_valid 36 nan
loss_train 36 nan
loss_valid 37 nan
loss_train 37 nan
loss_valid 38 nan
loss_train 38 nan
loss_valid 39 nan
loss_train 39 nan
loss_valid 40 nan
loss_train 40 nan
loss_valid 41 nan
loss_train 41 nan
loss_valid 42 nan
loss_train 42 nan
loss_valid 43 nan
loss_train 43 nan
loss_valid 44 nan
loss_train 44 nan
loss_valid 45 nan
loss_train 45 nan
loss_valid 46 nan
loss_train 46 nan
loss_valid 47 nan
loss_train 47 nan
loss_valid 48 nan
loss_train 48 nan
loss_valid 49 nan
loss_train 49 nan
loss_valid 50 nan
loss_train 50 nan
loss_valid 51 nan
loss_train 51 nan
loss_valid 52 nan
loss_train 52 nan
loss_valid 53 nan
loss_train 53 nan
loss_valid 54 nan
loss_train 54 nan
loss_valid 55 nan
loss_train 55 nan
loss_valid 56 nan
loss_train 56 nan
loss_valid 57 nan
loss_train 57 nan
loss_valid 58 nan
loss_train 58 nan
loss_valid 59 nan
loss_train 59 nan
loss_valid 60 nan
loss_train 60 nan
loss_valid 61 nan
loss_train 61 nan
loss_valid 62 nan
loss_train 62 nan
loss_valid 63 nan
loss_train 63 nan
loss_valid 64 nan
loss_train 64 nan
loss_valid 65 nan
loss_train 65 nan
loss_valid 66 nan
loss_train 66 nan
loss_valid 67 nan
loss_train 67 nan
loss_valid 68 nan
loss_train 68 nan
loss_valid 69 nan
loss_train 69 nan
loss_valid 70 nan
loss_train 70 nan
loss_valid 71 nan
loss_train 71 nan
loss_valid 72 nan
loss_train 72 nan
loss_valid 73 nan
loss_train 73 nan
loss_valid 74 nan
loss_train 74 nan
loss_valid 75 nan
loss_train 75 nan
loss_valid 76 nan
loss_train 76 nan
loss_valid 77 nan
loss_train 77 nan
loss_valid 78 nan
loss_train 78 nan
loss_valid 79 nan
loss_train 79 nan
loss_valid 80 nan
loss_train 80 nan
loss_valid 81 nan
loss_train 81 nan
loss_valid 82 nan
loss_train 82 nan
loss_valid 83 nan
loss_train 83 nan
loss_valid 84 nan
loss_train 84 nan
loss_valid 85 nan
loss_train 85 nan
loss_valid 86 nan
loss_train 86 nan
loss_valid 87 nan
loss_train 87 nan
loss_valid 88 nan
loss_train 88 nan
loss_valid 89 nan
loss_train 89 nan
loss_valid 90 nan
loss_train 90 nan
loss_valid 91 nan
loss_train 91 nan
loss_valid 92 nan
loss_train 92 nan
loss_valid 93 nan
loss_train 93 nan
loss_valid 94 nan
loss_train 94 nan
loss_valid 95 nan
loss_train 95 nan
loss_valid 96 nan
loss_train 96 nan
loss_valid 97 nan
loss_train 97 nan
loss_valid 98 nan
loss_train 98 nan
loss_valid 99 nan
loss_train 99 nan
[0.025030525030525032, 0, 0, 0.073870573870573872, 0.46825396825396826, 0.66117216117216115, 0.75152625152625152, 1.0, 0.07387057387057387, 0.46825396825396826, 0.6611721611721612, 0.7515262515262515, 1.0, 1638.0, 1638.0, 1638.0, 1638.0, 1638.0, 0, 0.44365188471805167, -0.011111111111111113, nan, 0.0010471204188481676, 0.0010152284263959391, 0.0018656716417910447, 0.000984251968503937, 0.9985443959243085, nan, 0.0014903129657228018, 0.0015037593984962407, 0.0012077294685990338, 0.9984779299847792, 0.9989701338825953, nan, 0.0019342359767891683, 0.00101010101010101, 0.9990958408679927, 0.9989764585465711, 0.9991111111111111, nan, 0.0008787346221441124, 0.9984025559105432, 0.9987714987714987, 0.9984662576687117, 0.998015873015873, nan, 0, 953, 983, 534, 1014, 685, 0, 669, 663, 826, 655, 969, 0, 515, 988, 1104, 975, 1123, 0, 1136, 624, 812, 650, 502, 0, 0.035285545071196404]
N, d, L:  5899 6 5
loss_valid 0 1.30594931966
loss_train 0 1.30966928285
loss_valid 1 1.11012507605
loss_train 1 1.11672464996
loss_valid 2 1.00098938256
loss_train 2 1.01103850308
loss_valid 3 0.93463984655
loss_train 3 nan
loss_valid 4 nan
loss_train 4 nan
loss_valid 5 nan
loss_train 5 nan
loss_valid 6 nan
loss_train 6 nan
loss_valid 7 nan
loss_train 7 nan
loss_valid 8 nan
loss_train 8 nan
loss_valid 9 nan
loss_train 9 nan
loss_valid 10 nan
loss_train 10 nan
loss_valid 11 nan
loss_train 11 nan
loss_valid 12 nan
loss_train 12 nan
loss_valid 13 nan
loss_train 13 nan
loss_valid 14 nan
loss_train 14 nan
loss_valid 15 nan
loss_train 15 nan
loss_valid 16 nan
loss_train 16 nan
loss_valid 17 nan
loss_train 17 nan
loss_valid 18 nan
loss_train 18 nan
loss_valid 19 nan
loss_train 19 nan
loss_valid 20 nan
loss_train 20 nan
loss_valid 21 nan
loss_train 21 nan
loss_valid 22 nan
loss_train 22 nan
loss_valid 23 nan
loss_train 23 nan
loss_valid 24 nan
loss_train 24 nan
loss_valid 25 nan
loss_train 25 nan
loss_valid 26 nan
loss_train 26 nan
loss_valid 27 nan
loss_train 27 nan
loss_valid 28 nan
loss_train 28 nan
loss_valid 29 nan
loss_train 29 nan
loss_valid 30 nan
loss_train 30 nan
loss_valid 31 nan
loss_train 31 nan
loss_valid 32 nan
loss_train 32 nan
loss_valid 33 nan
loss_train 33 nan
loss_valid 34 nan
loss_train 34 nan
loss_valid 35 nan
loss_train 35 nan
loss_valid 36 nan
loss_train 36 nan
loss_valid 37 nan
loss_train 37 nan
loss_valid 38 nan
loss_train 38 nan
loss_valid 39 nan
loss_train 39 nan
loss_valid 40 nan
loss_train 40 nan
loss_valid 41 nan
loss_train 41 nan
loss_valid 42 nan
loss_train 42 nan
loss_valid 43 nan
loss_train 43 nan
loss_valid 44 nan
loss_train 44 nan
loss_valid 45 nan
loss_train 45 nan
loss_valid 46 nan
loss_train 46 nan
loss_valid 47 nan
loss_train 47 nan
loss_valid 48 nan
loss_train 48 nan
loss_valid 49 nan
loss_train 49 nan
loss_valid 50 nan
loss_train 50 nan
loss_valid 51 nan
loss_train 51 nan
loss_valid 52 nan
loss_train 52 nan
loss_valid 53 nan
loss_train 53 nan
loss_valid 54 nan
loss_train 54 nan
loss_valid 55 nan
loss_train 55 nan
loss_valid 56 nan
loss_train 56 nan
loss_valid 57 nan
loss_train 57 nan
loss_valid 58 nan
loss_train 58 nan
loss_valid 59 nan
loss_train 59 nan
loss_valid 60 nan
loss_train 60 nan
loss_valid 61 nan
loss_train 61 nan
loss_valid 62 nan
loss_train 62 nan
loss_valid 63 nan
loss_train 63 nan
loss_valid 64 nan
loss_train 64 nan
loss_valid 65 nan/home/jasonzhang/anaconda2/lib/python2.7/site-packages/numpy/lib/nanfunctions.py:703: RuntimeWarning: Mean of empty slice
  warnings.warn("Mean of empty slice", RuntimeWarning)
/home/jasonzhang/anaconda2/lib/python2.7/site-packages/numpy/lib/nanfunctions.py:1202: RuntimeWarning: Degrees of freedom <= 0 for slice.
  warnings.warn("Degrees of freedom <= 0 for slice.", RuntimeWarning)

loss_train 65 nan
loss_valid 66 nan
loss_train 66 nan
loss_valid 67 nan
loss_train 67 nan
loss_valid 68 nan
loss_train 68 nan
loss_valid 69 nan
loss_train 69 nan
loss_valid 70 nan
loss_train 70 nan
loss_valid 71 nan
loss_train 71 nan
loss_valid 72 nan
loss_train 72 nan
loss_valid 73 nan
loss_train 73 nan
loss_valid 74 nan
loss_train 74 nan
loss_valid 75 nan
loss_train 75 nan
loss_valid 76 nan
loss_train 76 nan
loss_valid 77 nan
loss_train 77 nan
loss_valid 78 nan
loss_train 78 nan
loss_valid 79 nan
loss_train 79 nan
loss_valid 80 nan
loss_train 80 nan
loss_valid 81 nan
loss_train 81 nan
loss_valid 82 nan
loss_train 82 nan
loss_valid 83 nan
loss_train 83 nan
loss_valid 84 nan
loss_train 84 nan
loss_valid 85 nan
loss_train 85 nan
loss_valid 86 nan
loss_train 86 nan
loss_valid 87 nan
loss_train 87 nan
loss_valid 88 nan
loss_train 88 nan
loss_valid 89 nan
loss_train 89 nan
loss_valid 90 nan
loss_train 90 nan
loss_valid 91 nan
loss_train 91 nan
loss_valid 92 nan
loss_train 92 nan
loss_valid 93 nan
loss_train 93 nan
loss_valid 94 nan
loss_train 94 nan
loss_valid 95 nan
loss_train 95 nan
loss_valid 96 nan
loss_train 96 nan
loss_valid 97 nan
loss_train 97 nan
loss_valid 98 nan
loss_train 98 nan
loss_valid 99 nan
loss_train 99 nan
[0.03296703296703297, 0, 0, 0.085470085470085472, 0.49389499389499392, 0.64468864468864473, 0.74053724053724057, 1.0, 0.08547008547008547, 0.4938949938949939, 0.6446886446886447, 0.7405372405372406, 1.0, 1638.0, 1638.0, 1638.0, 1638.0, 1638.0, 0, 0.45800385908448127, 0.0092796092796092779, nan, 0.0010384215991692627, 0.0010559662090813093, 0.0018975332068311196, 0.000998003992015968, 0.9985272459499264, nan, 0.001610305958132045, 0.001564945226917058, 0.0012515644555694619, 0.9985611510791367, 0.9990205680705191, nan, 0.0018975332068311196, 0.0010131712259371835, 0.9991031390134529, 0.9990029910269193, 0.9991031390134529, nan, 0.00089126559714795, 0.9984375, 0.9988137603795967, 0.9984732824427481, 0.9980769230769231, nan, 0, 961, 945, 525, 1000, 677, 0, 619, 637, 797, 693, 1019, 0, 525, 985, 1113, 1001, 1113, 0, 1120, 638, 841, 653, 518, 0, 0.035663535314392605]
{'perf': [array([  2.73443931e-02,   0.00000000e+00,   0.00000000e+00,
         8.17862227e-02,   4.73754061e-01,   6.47218777e-01,
         7.45606891e-01,   1.00000000e+00,   8.17862227e-02,
         4.73754061e-01,   6.47218777e-01,   7.45606891e-01,
         1.00000000e+00,   1.63840000e+03,   1.63840000e+03,
         1.63840000e+03,   1.63840000e+03,   1.63840000e+03,
         0.00000000e+00,   4.50941665e-01,  -1.01798276e-02,
                    nan,   1.05860847e-03,   1.05399395e-03,
         1.89625839e-03,   9.74984103e-04,   9.98562231e-01,
                    nan,   1.54007016e-03,   1.51659766e-03,
         1.18569352e-03,   9.98556123e-01,   9.98991227e-01,
                    nan,   1.88321967e-03,   9.92518191e-04,
         9.99103119e-01,   9.98982343e-01,   9.99099266e-01,
                    nan,   8.67715453e-04,   9.98376174e-01,
         9.98744495e-01,   9.98421861e-01,   9.97954269e-01,
                    nan,   0.00000000e+00,   9.43600000e+02,
         9.47200000e+02,   5.25400000e+02,   1.02400000e+03,
         6.94800000e+02,   0.00000000e+00,   6.48400000e+02,
         6.57600000e+02,   8.42600000e+02,   6.91200000e+02,
         9.90000000e+02,   0.00000000e+00,   5.29800000e+02,
         1.00600000e+03,   1.11300000e+03,   9.80800000e+02,
         1.10860000e+03,   0.00000000e+00,   1.15080000e+03,
         6.14400000e+02,   7.95800000e+02,   6.32400000e+02,
         4.87600000e+02,   0.00000000e+00,   3.53167487e-02]), array([  3.73530938e-03,   0.00000000e+00,   0.00000000e+00,
         5.64706517e-03,   1.61816370e-02,   1.29595396e-02,
         7.07111254e-03,   0.00000000e+00,   5.64706517e-03,
         1.61816370e-02,   1.29595396e-02,   7.07111254e-03,
         0.00000000e+00,   4.89897949e-01,   4.89897949e-01,
         4.89897949e-01,   4.89897949e-01,   4.89897949e-01,
         0.00000000e+00,   6.13375558e-03,   1.09150875e-02,
                    nan,   3.39294940e-05,   2.22219651e-05,
         1.76053331e-05,   1.77767209e-05,   6.13587862e-05,
                    nan,   6.32134394e-05,   2.84548182e-05,
         4.47566571e-05,   4.36693567e-05,   2.66138308e-05,
                    nan,   7.15816237e-05,   2.11248644e-05,
         4.26636046e-06,   1.25482297e-05,   1.71228731e-05,
                    nan,   1.51396018e-05,   4.95821714e-05,
         5.12517725e-05,   5.44857615e-05,   8.09358467e-05,
                    nan,   0.00000000e+00,   3.00772339e+01,
         2.03017241e+01,   4.92341345e+00,   1.87723200e+01,
         2.99626434e+01,   0.00000000e+00,   2.62876397e+01,
         1.22245654e+01,   3.21284920e+01,   2.05075596e+01,
         2.64272587e+01,   0.00000000e+00,   2.09131538e+01,
         2.16979262e+01,   5.29150262e+00,   1.22049170e+01,
         2.07615028e+01,   0.00000000e+00,   1.99739831e+01,
         1.86826122e+01,   3.20711709e+01,   2.14997674e+01,
         1.96529896e+01,   0.00000000e+00,   1.94918707e-04])]}
SMPrank.py:290: RuntimeWarning: invalid value encountered in double_scalars
  self.probfeature[k] = self.probfeature[k] / sum_probf
/home/jasonzhang/anaconda2/lib/python2.7/site-packages/numpy/ma/core.py:867: RuntimeWarning: invalid value encountered in less_equal
  return umath.less_equal(x, self.critical_value)
dataset, N,d,L,K cpu 8192 6 5 60
N, d, L:  5898 6 5
loss_valid 0 1.15442973917
loss_train 0 1.16253052778
loss_valid 1 nan
loss_train 1 nan
loss_valid 2 nan
loss_train 2 nan
loss_valid 3 nan
loss_train 3 nan
loss_valid 4 nan
loss_train 4 nan
loss_valid 5 nan
loss_train 5 nan
loss_valid 6 nan
loss_train 6 nan
loss_valid 7 nan
loss_train 7 nan
loss_valid 8 nan
loss_train 8 nan
loss_valid 9 nan
loss_train 9 nan
loss_valid 10 nan
loss_train 10 nan
loss_valid 11 nan
loss_train 11 nan
loss_valid 12 nan
loss_train 12 nan
loss_valid 13 nan
loss_train 13 nan
loss_valid 14 nan
loss_train 14 nan
loss_valid 15 nan
loss_train 15 nan
loss_valid 16 nan
loss_train 16 nan
loss_valid 17 nan
loss_train 17 nan
loss_valid 18 nan
loss_train 18 nan
loss_valid 19 nan
loss_train 19 nan
loss_valid 20 nan
loss_train 20 nan
loss_valid 21 nan
loss_train 21 nan
loss_valid 22 nan
loss_train 22 nan
loss_valid 23 nan
loss_train 23 nan
loss_valid 24 nan
loss_train 24 nan
loss_valid 25 nan
loss_train 25 nan
loss_valid 26 nan
loss_train 26 nan
loss_valid 27 nan
loss_train 27 nan
loss_valid 28 nan
loss_train 28 nan
loss_valid 29 nan
loss_train 29 nan
loss_valid 30 nan
loss_train 30 nan
loss_valid 31 nan
loss_train 31 nan
loss_valid 32 nan
loss_train 32 nan
loss_valid 33 nan
loss_train 33 nan
loss_valid 34 nan
loss_train 34 nan
loss_valid 35 nan
loss_train 35 nan
loss_valid 36 nan
loss_train 36 nan
loss_valid 37 nan
loss_train 37 nan
loss_valid 38 nan
loss_train 38 nan
loss_valid 39 nan
loss_train 39 nan
loss_valid 40 nan
loss_train 40 nan
loss_valid 41 nan
loss_train 41 nan
loss_valid 42 nan
loss_train 42 nan
loss_valid 43 nan
loss_train 43 nan
loss_valid 44 nan
loss_train 44 nan
loss_valid 45 nan
loss_train 45 nan
loss_valid 46 nan
loss_train 46 nan
loss_valid 47 nan
loss_train 47 nan
loss_valid 48 nan
loss_train 48 nan
loss_valid 49 nan
loss_train 49 nan
loss_valid 50 nan
loss_train 50 nan
loss_valid 51 nan
loss_train 51 nan
loss_valid 52 nan
loss_train 52 nan
loss_valid 53 nan
loss_train 53 nan
loss_valid 54 nan
loss_train 54 nan
loss_valid 55 nan
loss_train 55 nan
loss_valid 56 nan
loss_train 56 nan
loss_valid 57 nan
loss_train 57 nan
loss_valid 58 nan
loss_train 58 nan
loss_valid 59 nan
loss_train 59 nan
loss_valid 60 nan
loss_train 60 nan
loss_valid 61 nan
loss_train 61 nan
loss_valid 62 nan
loss_train 62 nan
loss_valid 63 nan
loss_train 63 nan
loss_valid 64 nan
loss_train 64 nan
loss_valid 65 nan
loss_train 65 nan
loss_valid 66 nan
loss_train 66 nan
loss_valid 67 nan
loss_train 67 nan
loss_valid 68 nan
loss_train 68 nan
loss_valid 69 nan
loss_train 69 nan
loss_valid 70 nan
loss_train 70 nan
loss_valid 71 nan
loss_train 71 nan
loss_valid 72 nan
loss_train 72 nan
loss_valid 73 nan
loss_train 73 nan
loss_valid 74 nan
loss_train 74 nan
loss_valid 75 nan
loss_train 75 nan
loss_valid 76 nan
loss_train 76 nan
loss_valid 77 nan
loss_train 77 nan
loss_valid 78 nan
loss_train 78 nan
loss_valid 79 nan
loss_train 79 nan
loss_valid 80 nan
loss_train 80 nan
loss_valid 81 nan
loss_train 81 nan
loss_valid 82 nan
loss_train 82 nan
loss_valid 83 nan
loss_train 83 nan
loss_valid 84 nan
loss_train 84 nan
loss_valid 85 nan
loss_train 85 nan
loss_valid 86 nan
loss_train 86 nan
loss_valid 87 nan
loss_train 87 nan
loss_valid 88 nan
loss_train 88 nan
loss_valid 89 nan
loss_train 89 nan
loss_valid 90 nan
loss_train 90 nan
loss_valid 91 nan
loss_train 91 nan
loss_valid 92 nan
loss_train 92 nan
loss_valid 93 nan
loss_train 93 nan
loss_valid 94 nan
loss_train 94 nan
loss_valid 95 nan
loss_train 95 nan
loss_valid 96 nan
loss_train 96 nan
loss_valid 97 nan
loss_train 97 nan
loss_valid 98 nan
loss_train 98 nan
loss_valid 99 nan
loss_train 99 nan
[0.027455765710799267, 0, 0, 0.080536912751677847, 0.49115314215985356, 0.6235509456985967, 0.74130567419158022, 1.0, 0.08053691275167785, 0.49115314215985356, 0.6235509456985967, 0.7413056741915802, 1.0, 1639.0, 1639.0, 1639.0, 1639.0, 1639.0, 0, 0.44917568026575938, -0.018913971934106162, nan, 0.0010141987829614604, 0.001081081081081081, 0.0019047619047619048, 0.0009569377990430622, 0.9984779299847792, nan, 0.0016233766233766235, 0.0015313935681470138, 0.0011806375442739079, 0.9986072423398329, 0.9990262901655307, nan, 0.0017543859649122807, 0.0009596928982725527, 0.9991055456171736, 0.998989898989899, 0.9990680335507922, nan, 0.0008613264427217916, 0.9983277591973244, 0.9987437185929648, 0.9983361064891847, 0.9979253112033195, nan, 0, 984, 923, 523, 1043, 655, 0, 614, 651, 845, 716, 1025, 0, 568, 1040, 1116, 988, 1071, 0, 1159, 596, 794, 599, 480, 0, 0.035171967700775082]
N, d, L:  5898 6 5
loss_valid 0 1.12905310215
loss_train 0 1.13814282207
loss_valid 1 nan
loss_train 1 nan
loss_valid 2 nan
loss_train 2 nan
loss_valid 3 nan
loss_train 3 nan
loss_valid 4 nan
loss_train 4 nan
loss_valid 5 nan
loss_train 5 nan
loss_valid 6 nan
loss_train 6 nan
loss_valid 7 nan
loss_train 7 nan
loss_valid 8 nan
loss_train 8 nan
loss_valid 9 nan
loss_train 9 nan
loss_valid 10 nan
loss_train 10 nan
loss_valid 11 nan
loss_train 11 nan
loss_valid 12 nan
loss_train 12 nan
loss_valid 13 nan
loss_train 13 nan
loss_valid 14 nan
loss_train 14 nan
loss_valid 15 nan
loss_train 15 nan
loss_valid 16 nan
loss_train 16 nan
loss_valid 17 nan
loss_train 17 nan
loss_valid 18 nan
loss_train 18 nan
loss_valid 19 nan
loss_train 19 nan
loss_valid 20 nan
loss_train 20 nan
loss_valid 21 nan
loss_train 21 nan
loss_valid 22 nan
loss_train 22 nan
loss_valid 23 nan
loss_train 23 nan
loss_valid 24 nan
loss_train 24 nan
loss_valid 25 nan
loss_train 25 nan
loss_valid 26 nan
loss_train 26 nan
loss_valid 27 nan
loss_train 27 nan
loss_valid 28 nan
loss_train 28 nan
loss_valid 29 nan
loss_train 29 nan
loss_valid 30 nan
loss_train 30 nan
loss_valid 31 nan
loss_train 31 nan
loss_valid 32 nan
loss_train 32 nan
loss_valid 33 nan
loss_train 33 nan
loss_valid 34 nan
loss_train 34 nan
loss_valid 35 nan
loss_train 35 nan
loss_valid 36 nan
loss_train 36 nan
loss_valid 37 nan
loss_train 37 nan
loss_valid 38 nan
loss_train 38 nan
loss_valid 39 nan
loss_train 39 nan
loss_valid 40 nan
loss_train 40 nan
loss_valid 41 nan
loss_train 41 nan
loss_valid 42 nan
loss_train 42 nan
loss_valid 43 nan
loss_train 43 nan
loss_valid 44 nan
loss_train 44 nan
loss_valid 45 nan
loss_train 45 nan
loss_valid 46 nan
loss_train 46 nan
loss_valid 47 nan
loss_train 47 nan
loss_valid 48 nan
loss_train 48 nan
loss_valid 49 nan
loss_train 49 nan
loss_valid 50 nan
loss_train 50 nan
loss_valid 51 nan
loss_train 51 nan
loss_valid 52 nan
loss_train 52 nan
loss_valid 53 nan
loss_train 53 nan
loss_valid 54 nan
loss_train 54 nan
loss_valid 55 nan
loss_train 55 nan
loss_valid 56 nan
loss_train 56 nan
loss_valid 57 nan
loss_train 57 nan
loss_valid 58 nan
loss_train 58 nan
loss_valid 59 nan
loss_train 59 nan
loss_valid 60 nan
loss_train 60 nan
loss_valid 61 nan
loss_train 61 nan
loss_valid 62 nan
loss_train 62 nan
loss_valid 63 nan
loss_train 63 nan
loss_valid 64 nan
loss_train 64 nan
loss_valid 65 nan
loss_train 65 nan
loss_valid 66 nan
loss_train 66 nan
loss_valid 67 nan
loss_train 67 nan
loss_valid 68 nan
loss_train 68 nan
loss_valid 69 nan
loss_train 69 nan
loss_valid 70 nan
loss_train 70 nan
loss_valid 71 nan
loss_train 71 nan
loss_valid 72 nan
loss_train 72 nan
loss_valid 73 nan
loss_train 73 nan
loss_valid 74 nan
loss_train 74 nan
loss_valid 75 nan
loss_train 75 nan
loss_valid 76 nan
loss_train 76 nan
loss_valid 77 nan
loss_train 77 nan
loss_valid 78 nan
loss_train 78 nan
loss_valid 79 nan
loss_train 79 nan
loss_valid 80 nan
loss_train 80 nan
loss_valid 81 nan
loss_train 81 nan
loss_valid 82 nan
loss_train 82 nan
loss_valid 83 nan
loss_train 83 nan
loss_valid 84 nan
loss_train 84 nan
loss_valid 85 nan
loss_train 85 nan
loss_valid 86 nan
loss_train 86 nan
loss_valid 87 nan
loss_train 87 nan
loss_valid 88 nan
loss_train 88 nan
loss_valid 89 nan
loss_train 89 nan
loss_valid 90 nan
loss_train 90 nan
loss_valid 91 nan
loss_train 91 nan
loss_valid 92 nan
loss_train 92 nan
loss_valid 93 nan
loss_train 93 nan
loss_valid 94 nan
loss_train 94 nan
loss_valid 95 nan
loss_train 95 nan
loss_valid 96 nan
loss_train 96 nan
loss_valid 97 nan
loss_train 97 nan
loss_valid 98 nan
loss_train 98 nan
loss_valid 99 nan
loss_train 99 nan
[0.021964612568639415, 0, 0, 0.090298962782184258, 0.46308724832214765, 0.65466748017083587, 0.73825503355704702, 1.0, 0.09029896278218426, 0.46308724832214765, 0.6546674801708359, 0.738255033557047, 1.0, 1639.0, 1639.0, 1639.0, 1639.0, 1639.0, 0, 0.45826274674174039, -0.0082977425259304471, nan, 0.0010834236186348862, 0.001049317943336831, 0.0019193857965451055, 0.000984251968503937, 0.9986111111111111, nan, 0.0014992503748125937, 0.0014970059880239522, 0.0011737089201877935, 0.9985507246376811, 0.9989754098360656, nan, 0.00196078431372549, 0.001004016064257028, 0.9991087344028521, 0.9989743589743589, 0.999117387466902, nan, 0.0008496176720475786, 0.9984051036682615, 0.9987357774968394, 0.9984544049459042, 0.9978540772532188, nan, 0, 921, 951, 519, 1014, 718, 0, 665, 666, 850, 688, 974, 0, 508, 994, 1120, 973, 1131, 0, 1175, 625, 789, 645, 464, 0, 0.035360157068322448]
N, d, L:  5899 6 5
loss_valid 0 1.14762601762
loss_train 0 1.14031739511
loss_valid 1 0.986158907148
loss_train 1 nan
loss_valid 2 nan
loss_train 2 nan
loss_valid 3 nan
loss_train 3 nan
loss_valid 4 nan
loss_train 4 nan
loss_valid 5 nan
loss_train 5 nan
loss_valid 6 nan
loss_train 6 nan
loss_valid 7 nan
loss_train 7 nan
loss_valid 8 nan
loss_train 8 nan
loss_valid 9 nan
loss_train 9 nan
loss_valid 10 nan
loss_train 10 nan
loss_valid 11 nan
loss_train 11 nan
loss_valid 12 nan
loss_train 12 nan
loss_valid 13 nan
loss_train 13 nan
loss_valid 14 nan
loss_train 14 nan
loss_valid 15 nan
loss_train 15 nan
loss_valid 16 nan
loss_train 16 nan
loss_valid 17 nan
loss_train 17 nan
loss_valid 18 nan
loss_train 18 nan
loss_valid 19 nan
loss_train 19 nan
loss_valid 20 nan
loss_train 20 nan
loss_valid 21 nan
loss_train 21 nan
loss_valid 22 nan
loss_train 22 nan
loss_valid 23 nan
loss_train 23 nan
loss_valid 24 nan
loss_train 24 nan
loss_valid 25 nan
loss_train 25 nan
loss_valid 26 nan
loss_train 26 nan
loss_valid 27 nan
loss_train 27 nan
loss_valid 28 nan
loss_train 28 nan
loss_valid 29 nan
loss_train 29 nan
loss_valid 30 nan
loss_train 30 nan
loss_valid 31 nan
loss_train 31 nan
loss_valid 32 nan
loss_train 32 nan
loss_valid 33 nan
loss_train 33 nan
loss_valid 34 nan
loss_train 34 nan
loss_valid 35 nan
loss_train 35 nan
loss_valid 36 nan
loss_train 36 nan
loss_valid 37 nan
loss_train 37 nan
loss_valid 38 nan
loss_train 38 nan
loss_valid 39 nan
loss_train 39 nan
loss_valid 40 nan
loss_train 40 nan
loss_valid 41 nan
loss_train 41 nan
loss_valid 42 nan
loss_train 42 nan
loss_valid 43 nan
loss_train 43 nan
loss_valid 44 nan
loss_train 44 nan
loss_valid 45 nan
loss_train 45 nan
loss_valid 46 nan
loss_train 46 nan
loss_valid 47 nan
loss_train 47 nan
loss_valid 48 nan
loss_train 48 nan
loss_valid 49 nan
loss_train 49 nan
loss_valid 50 nan
loss_train 50 nan
loss_valid 51 nan
loss_train 51 nan
loss_valid 52 nan
loss_train 52 nan
loss_valid 53 nan
loss_train 53 nan
loss_valid 54 nan
loss_train 54 nan
loss_valid 55 nan
loss_train 55 nan
loss_valid 56 nan
loss_train 56 nan
loss_valid 57 nan
loss_train 57 nan
loss_valid 58 nan
loss_train 58 nan
loss_valid 59 nan
loss_train 59 nan
loss_valid 60 nan
loss_train 60 nan
loss_valid 61 nan
loss_train 61 nan
loss_valid 62 nan
loss_train 62 nan
loss_valid 63 nan
loss_train 63 nan
loss_valid 64 nan
loss_train 64 nan
loss_valid 65 nan
loss_train 65 nan
loss_valid 66 nan
loss_train 66 nan
loss_valid 67 nan
loss_train 67 nan
loss_valid 68 nan
loss_train 68 nan
loss_valid 69 nan
loss_train 69 nan
loss_valid 70 nan
loss_train 70 nan
loss_valid 71 nan
loss_train 71 nan
loss_valid 72 nan
loss_train 72 nan
loss_valid 73 nan
loss_train 73 nan
loss_valid 74 nan
loss_train 74 nan
loss_valid 75 nan
loss_train 75 nan
loss_valid 76 nan
loss_train 76 nan
loss_valid 77 nan
loss_train 77 nan
loss_valid 78 nan
loss_train 78 nan
loss_valid 79 nan
loss_train 79 nan
loss_valid 80 nan
loss_train 80 nan
loss_valid 81 nan
loss_train 81 nan
loss_valid 82 nan
loss_train 82 nan
loss_valid 83 nan
loss_train 83 nan
loss_valid 84 nan
loss_train 84 nan
loss_valid 85 nan
loss_train 85 nan
loss_valid 86 nan
loss_train 86 nan
loss_valid 87 nan
loss_train 87 nan
loss_valid 88 nan
loss_train 88 nan
loss_valid 89 nan
loss_train 89 nan
loss_valid 90 nan
loss_train 90 nan
loss_valid 91 nan
loss_train 91 nan
loss_valid 92 nan
loss_train 92 nan
loss_valid 93 nan
loss_train 93 nan
loss_valid 94 nan
loss_train 94 nan
loss_valid 95 nan
loss_train 95 nan
loss_valid 96 nan
loss_train 96 nan
loss_valid 97 nan
loss_train 97 nan
loss_valid 98 nan
loss_train 98 nan
loss_valid 99 nan
loss_train 99 nan
[0.029304029304029304, 0, 0, 0.078754578754578752, 0.45238095238095238, 0.65201465201465203, 0.75641025641025639, 1.0, 0.07875457875457875, 0.4523809523809524, 0.652014652014652, 0.7564102564102564, 1.0, 1638.0, 1638.0, 1638.0, 1638.0, 1638.0, 0, 0.44561415470883736, -0.021855921855921855, nan, 0.0011098779134295228, 0.0010683760683760685, 0.001893939393939394, 0.0009514747859181732, 0.9986504723346828, nan, 0.0014771048744460858, 0.0014858841010401188, 0.0011148272017837235, 0.9985835694050992, 0.9989637305699481, nan, 0.001869158878504673, 0.000975609756097561, 0.9991023339317774, 0.9989680082559339, 0.999096657633243, nan, 0.0008576329331046312, 0.9983079526226735, 0.9986577181208054, 0.9983792544570502, 0.9978991596638656, nan, 0, 899, 934, 526, 1049, 739, 0, 675, 671, 895, 704, 963, 0, 533, 1023, 1112, 967, 1105, 0, 1164, 589, 743, 615, 474, 0, 0.035102538369273163]
N, d, L:  5899 6 5
loss_valid 0 1.15995868257
loss_train 0 1.1668547224
loss_valid 1 0.988023625799
loss_train 1 0.991999635348
loss_valid 2 0.915548108031
loss_train 2 nan
loss_valid 3 nan
loss_train 3 nan
loss_valid 4 nan
loss_train 4 nan
loss_valid 5 nan
loss_train 5 nan
loss_valid 6 nan
loss_train 6 nan
loss_valid 7 nan
loss_train 7 nan
loss_valid 8 nan
loss_train 8 nan
loss_valid 9 nan
loss_train 9 nan
loss_valid 10 nan
loss_train 10 nan
loss_valid 11 nan
loss_train 11 nan
loss_valid 12 nan
loss_train 12 nan
loss_valid 13 nan
loss_train 13 nan
loss_valid 14 nan
loss_train 14 nan
loss_valid 15 nan
loss_train 15 nan
loss_valid 16 nan
loss_train 16 nan
loss_valid 17 nan
loss_train 17 nan
loss_valid 18 nan
loss_train 18 nan
loss_valid 19 nan
loss_train 19 nan
loss_valid 20 nan
loss_train 20 nan
loss_valid 21 nan
loss_train 21 nan
loss_valid 22 nan
loss_train 22 nan
loss_valid 23 nan
loss_train 23 nan
loss_valid 24 nan
loss_train 24 nan
loss_valid 25 nan
loss_train 25 nan
loss_valid 26 nan
loss_train 26 nan
loss_valid 27 nan
loss_train 27 nan
loss_valid 28 nan
loss_train 28 nan
loss_valid 29 nan
loss_train 29 nan
loss_valid 30 nan
loss_train 30 nan
loss_valid 31 nan
loss_train 31 nan
loss_valid 32 nan
loss_train 32 nan
loss_valid 33 nan
loss_train 33 nan
loss_valid 34 nan
loss_train 34 nan
loss_valid 35 nan
loss_train 35 nan
loss_valid 36 nan
loss_train 36 nan
loss_valid 37 nan
loss_train 37 nan
loss_valid 38 nan
loss_train 38 nan
loss_valid 39 nan
loss_train 39 nan
loss_valid 40 nan
loss_train 40 nan
loss_valid 41 nan
loss_train 41 nan
loss_valid 42 nan
loss_train 42 nan
loss_valid 43 nan
loss_train 43 nan
loss_valid 44 nan
loss_train 44 nan
loss_valid 45 nan
loss_train 45 nan
loss_valid 46 nan
loss_train 46 nan
loss_valid 47 nan
loss_train 47 nan
loss_valid 48 nan
loss_train 48 nan
loss_valid 49 nan
loss_train 49 nan
loss_valid 50 nan
loss_train 50 nan
loss_valid 51 nan
loss_train 51 nan
loss_valid 52 nan
loss_train 52 nan
loss_valid 53 nan
loss_train 53 nan
loss_valid 54 nan
loss_train 54 nan
loss_valid 55 nan
loss_train 55 nan
loss_valid 56 nan
loss_train 56 nan
loss_valid 57 nan
loss_train 57 nan
loss_valid 58 nan
loss_train 58 nan
loss_valid 59 nan
loss_train 59 nan
loss_valid 60 nan
loss_train 60 nan
loss_valid 61 nan
loss_train 61 nan
loss_valid 62 nan
loss_train 62 nan
loss_valid 63 nan
loss_train 63 nan
loss_valid 64 nan
loss_train 64 nan
loss_valid 65 nan
loss_train 65 nan
loss_valid 66 nan
loss_train 66 nan
loss_valid 67 nan
loss_train 67 nan
loss_valid 68 nan
loss_train 68 nan
loss_valid 69 nan
loss_train 69 nan
loss_valid 70 nan
loss_train 70 nan
loss_valid 71 nan
loss_train 71 nan
loss_valid 72 nan
loss_train 72 nan
loss_valid 73 nan
loss_train 73 nan
loss_valid 74 nan
loss_train 74 nan
loss_valid 75 nan
loss_train 75 nan
loss_valid 76 nan
loss_train 76 nan
loss_valid 77 nan
loss_train 77 nan
loss_valid 78 nan
loss_train 78 nan
loss_valid 79 nan
loss_train 79 nan
loss_valid 80 nan
loss_train 80 nan
loss_valid 81 nan
loss_train 81 nan
loss_valid 82 nan
loss_train 82 nan
loss_valid 83 nan
loss_train 83 nan
loss_valid 84 nan
loss_train 84 nan
loss_valid 85 nan
loss_train 85 nan
loss_valid 86 nan
loss_train 86 nan
loss_valid 87 nan
loss_train 87 nan
loss_valid 88 nan
loss_train 88 nan
loss_valid 89 nan
loss_train 89 nan
loss_valid 90 nan
loss_train 90 nan
loss_valid 91 nan
loss_train 91 nan
loss_valid 92 nan
loss_train 92 nan
loss_valid 93 nan
loss_train 93 nan
loss_valid 94 nan
loss_train 94 nan
loss_valid 95 nan
loss_train 95 nan
loss_valid 96 nan
loss_train 96 nan
loss_valid 97 nan
loss_train 97 nan
loss_valid 98 nan
loss_train 98 nan
loss_valid 99 nan
loss_train 99 nan
[0.025030525030525032, 0, 0, 0.073870573870573872, 0.46825396825396826, 0.66117216117216115, 0.75152625152625152, 1.0, 0.07387057387057387, 0.46825396825396826, 0.6611721611721612, 0.7515262515262515, 1.0, 1638.0, 1638.0, 1638.0, 1638.0, 1638.0, 0, 0.44365188471805167, -0.011111111111111113, nan, 0.0010471204188481676, 0.0010152284263959391, 0.0018656716417910447, 0.000984251968503937, 0.9985443959243085, nan, 0.0014903129657228018, 0.0015037593984962407, 0.0012077294685990338, 0.9984779299847792, 0.9989701338825953, nan, 0.0019342359767891683, 0.00101010101010101, 0.9990958408679927, 0.9989764585465711, 0.9991111111111111, nan, 0.0008787346221441124, 0.9984025559105432, 0.9987714987714987, 0.9984662576687117, 0.998015873015873, nan, 0, 953, 983, 534, 1014, 685, 0, 669, 663, 826, 655, 969, 0, 515, 988, 1104, 975, 1123, 0, 1136, 624, 812, 650, 502, 0, 0.035285545071196404]
N, d, L:  5899 6 5
loss_valid 0 1.16788684761
loss_train 0 1.1705769554
loss_valid 1 0.993075717876
loss_train 1 0.999261963795
loss_valid 2 0.915660509154
loss_train 2 0.9245459426
loss_valid 3 0.875591277475
loss_train 3 nan
loss_valid 4 nan
loss_train 4 nan
loss_valid 5 nan
loss_train 5 nan
loss_valid 6 nan
loss_train 6 nan
loss_valid 7 nan
loss_train 7 nan
loss_valid 8 nan
loss_train 8 nan
loss_valid 9 nan
loss_train 9 nan
loss_valid 10 nan
loss_train 10 nan
loss_valid 11 nan
loss_train 11 nan
loss_valid 12 nan
loss_train 12 nan
loss_valid 13 nan
loss_train 13 nan
loss_valid 14 nan
loss_train 14 nan
loss_valid 15 nan
loss_train 15 nan
loss_valid 16 nan
loss_train 16 nan
loss_valid 17 nan
loss_train 17 nan
loss_valid 18 nan
loss_train 18 nan
loss_valid 19 nan
loss_train 19 nan
loss_valid 20 nan
loss_train 20 nan
loss_valid 21 nan
loss_train 21 nan
loss_valid 22 nan
loss_train 22 nan
loss_valid 23 nan
loss_train 23 nan
loss_valid 24 nan
loss_train 24 nan
loss_valid 25 nan
loss_train 25 nan
loss_valid 26 nan
loss_train 26 nan
loss_valid 27 nan
loss_train 27 nan
loss_valid 28 nan
loss_train 28 nan
loss_valid 29 nan
loss_train 29 nan
loss_valid 30 nan
loss_train 30 nan
loss_valid 31 nan
loss_train 31 nan
loss_valid 32 nan
loss_train 32 nan
loss_valid 33 nan
loss_train 33 nan
loss_valid 34 nan
loss_train 34 nan
loss_valid 35 nan
loss_train 35 nan
loss_valid 36 nan
loss_train 36 nan
loss_valid 37 nan
loss_train 37 nan
loss_valid 38 nan
loss_train 38 nan
loss_valid 39 nan
loss_train 39 nan
loss_valid 40 nan
loss_train 40 nan
loss_valid 41 nan
loss_train 41 nan
loss_valid 42 nan
loss_train 42 nan
loss_valid 43 nan
loss_train 43 nan
loss_valid 44 nan
loss_train 44 nan
loss_valid 45 nan
loss_train 45 nan
loss_valid 46 nan
loss_train 46 nan
loss_valid 47 nan
loss_train 47 nan
loss_valid 48 nan
loss_train 48 nan
loss_valid 49 nan
loss_train 49 nan
loss_valid 50 nan
loss_train 50 nan
loss_valid 51 nan
loss_train 51 nan
loss_valid 52 nan
loss_train 52 nan
loss_valid 53 nan
loss_train 53 nan
loss_valid 54 nan
loss_train 54 nan
loss_valid 55 nan
loss_train 55 nan
loss_valid 56 nan
loss_train 56 nan
loss_valid 57 nan
loss_train 57 nan
loss_valid 58 nan
loss_train 58 nan
loss_valid 59 nan
loss_train 59 nan
loss_valid 60 nan
loss_train 60 nan
loss_valid 61 nan
loss_train 61 nan
loss_valid 62 nan
loss_train 62 nan
loss_valid 63 nan
loss_train 63 nan
loss_valid 64/home/jasonzhang/anaconda2/lib/python2.7/site-packages/numpy/lib/nanfunctions.py:703: RuntimeWarning: Mean of empty slice
  warnings.warn("Mean of empty slice", RuntimeWarning)
/home/jasonzhang/anaconda2/lib/python2.7/site-packages/numpy/lib/nanfunctions.py:1202: RuntimeWarning: Degrees of freedom <= 0 for slice.
  warnings.warn("Degrees of freedom <= 0 for slice.", RuntimeWarning)
 nan
loss_train 64 nan
loss_valid 65 nan
loss_train 65 nan
loss_valid 66 nan
loss_train 66 nan
loss_valid 67 nan
loss_train 67 nan
loss_valid 68 nan
loss_train 68 nan
loss_valid 69 nan
loss_train 69 nan
loss_valid 70 nan
loss_train 70 nan
loss_valid 71 nan
loss_train 71 nan
loss_valid 72 nan
loss_train 72 nan
loss_valid 73 nan
loss_train 73 nan
loss_valid 74 nan
loss_train 74 nan
loss_valid 75 nan
loss_train 75 nan
loss_valid 76 nan
loss_train 76 nan
loss_valid 77 nan
loss_train 77 nan
loss_valid 78 nan
loss_train 78 nan
loss_valid 79 nan
loss_train 79 nan
loss_valid 80 nan
loss_train 80 nan
loss_valid 81 nan
loss_train 81 nan
loss_valid 82 nan
loss_train 82 nan
loss_valid 83 nan
loss_train 83 nan
loss_valid 84 nan
loss_train 84 nan
loss_valid 85 nan
loss_train 85 nan
loss_valid 86 nan
loss_train 86 nan
loss_valid 87 nan
loss_train 87 nan
loss_valid 88 nan
loss_train 88 nan
loss_valid 89 nan
loss_train 89 nan
loss_valid 90 nan
loss_train 90 nan
loss_valid 91 nan
loss_train 91 nan
loss_valid 92 nan
loss_train 92 nan
loss_valid 93 nan
loss_train 93 nan
loss_valid 94 nan
loss_train 94 nan
loss_valid 95 nan
loss_train 95 nan
loss_valid 96 nan
loss_train 96 nan
loss_valid 97 nan
loss_train 97 nan
loss_valid 98 nan
loss_train 98 nan
loss_valid 99 nan
loss_train 99 nan
[0.03296703296703297, 0, 0, 0.085470085470085472, 0.49389499389499392, 0.64468864468864473, 0.74053724053724057, 1.0, 0.08547008547008547, 0.4938949938949939, 0.6446886446886447, 0.7405372405372406, 1.0, 1638.0, 1638.0, 1638.0, 1638.0, 1638.0, 0, 0.45800385908448127, 0.0092796092796092779, nan, 0.0010384215991692627, 0.0010559662090813093, 0.0018975332068311196, 0.000998003992015968, 0.9985272459499264, nan, 0.001610305958132045, 0.001564945226917058, 0.0012515644555694619, 0.9985611510791367, 0.9990205680705191, nan, 0.0018975332068311196, 0.0010131712259371835, 0.9991031390134529, 0.9990029910269193, 0.9991031390134529, nan, 0.00089126559714795, 0.9984375, 0.9988137603795967, 0.9984732824427481, 0.9980769230769231, nan, 0, 961, 945, 525, 1000, 677, 0, 619, 637, 797, 693, 1019, 0, 525, 985, 1113, 1001, 1113, 0, 1120, 638, 841, 653, 518, 0, 0.035663535314392605]
{'perf': [array([  2.73443931e-02,   0.00000000e+00,   0.00000000e+00,
         8.17862227e-02,   4.73754061e-01,   6.47218777e-01,
         7.45606891e-01,   1.00000000e+00,   8.17862227e-02,
         4.73754061e-01,   6.47218777e-01,   7.45606891e-01,
         1.00000000e+00,   1.63840000e+03,   1.63840000e+03,
         1.63840000e+03,   1.63840000e+03,   1.63840000e+03,
         0.00000000e+00,   4.50941665e-01,  -1.01798276e-02,
                    nan,   1.05860847e-03,   1.05399395e-03,
         1.89625839e-03,   9.74984103e-04,   9.98562231e-01,
                    nan,   1.54007016e-03,   1.51659766e-03,
         1.18569352e-03,   9.98556123e-01,   9.98991227e-01,
                    nan,   1.88321967e-03,   9.92518191e-04,
         9.99103119e-01,   9.98982343e-01,   9.99099266e-01,
                    nan,   8.67715453e-04,   9.98376174e-01,
         9.98744495e-01,   9.98421861e-01,   9.97954269e-01,
                    nan,   0.00000000e+00,   9.43600000e+02,
         9.47200000e+02,   5.25400000e+02,   1.02400000e+03,
         6.94800000e+02,   0.00000000e+00,   6.48400000e+02,
         6.57600000e+02,   8.42600000e+02,   6.91200000e+02,
         9.90000000e+02,   0.00000000e+00,   5.29800000e+02,
         1.00600000e+03,   1.11300000e+03,   9.80800000e+02,
         1.10860000e+03,   0.00000000e+00,   1.15080000e+03,
         6.14400000e+02,   7.95800000e+02,   6.32400000e+02,
         4.87600000e+02,   0.00000000e+00,   3.53167487e-02]), array([  3.73530938e-03,   0.00000000e+00,   0.00000000e+00,
         5.64706517e-03,   1.61816370e-02,   1.29595396e-02,
         7.07111254e-03,   0.00000000e+00,   5.64706517e-03,
         1.61816370e-02,   1.29595396e-02,   7.07111254e-03,
         0.00000000e+00,   4.89897949e-01,   4.89897949e-01,
         4.89897949e-01,   4.89897949e-01,   4.89897949e-01,
         0.00000000e+00,   6.13375558e-03,   1.09150875e-02,
                    nan,   3.39294940e-05,   2.22219651e-05,
         1.76053331e-05,   1.77767209e-05,   6.13587862e-05,
                    nan,   6.32134394e-05,   2.84548182e-05,
         4.47566571e-05,   4.36693567e-05,   2.66138308e-05,
                    nan,   7.15816237e-05,   2.11248644e-05,
         4.26636046e-06,   1.25482297e-05,   1.71228731e-05,
                    nan,   1.51396018e-05,   4.95821714e-05,
         5.12517725e-05,   5.44857615e-05,   8.09358467e-05,
                    nan,   0.00000000e+00,   3.00772339e+01,
         2.03017241e+01,   4.92341345e+00,   1.87723200e+01,
         2.99626434e+01,   0.00000000e+00,   2.62876397e+01,
         1.22245654e+01,   3.21284920e+01,   2.05075596e+01,
         2.64272587e+01,   0.00000000e+00,   2.09131538e+01,
         2.16979262e+01,   5.29150262e+00,   1.22049170e+01,
         2.07615028e+01,   0.00000000e+00,   1.99739831e+01,
         1.86826122e+01,   3.20711709e+01,   2.14997674e+01,
         1.96529896e+01,   0.00000000e+00,   1.94918707e-04])]}
SMPrank.py:298: RuntimeWarning: invalid value encountered in double_scalars
  self.probfeature[k] = self.probfeature[k] / sum_probf
/home/jasonzhang/anaconda2/lib/python2.7/site-packages/numpy/ma/core.py:867: RuntimeWarning: invalid value encountered in less_equal
  return umath.less_equal(x, self.critical_value)
dataset, N,d,L,K cpu 8192 6 5 60
N, d, L:  5898 6 5
loss_valid 0 1.15442973917
loss_train 0 1.16253052778
loss_valid 1 nan
loss_train 1 nan
nan train loss
early stop at epoch 1
[0.027455765710799267, 0, 0, 0.080536912751677847, 0.49115314215985356, 0.6235509456985967, 0.74130567419158022, 1.0, 0.08053691275167785, 0.49115314215985356, 0.6235509456985967, 0.7413056741915802, 1.0, 1639.0, 1639.0, 1639.0, 1639.0, 1639.0, 0, 0.44917568026575938, -0.018913971934106162, nan, 0.0010141987829614604, 0.001081081081081081, 0.0019047619047619048, 0.0009569377990430622, 0.9984779299847792, nan, 0.0016233766233766235, 0.0015313935681470138, 0.0011806375442739079, 0.9986072423398329, 0.9990262901655307, nan, 0.0017543859649122807, 0.0009596928982725527, 0.9991055456171736, 0.998989898989899, 0.9990680335507922, nan, 0.0008613264427217916, 0.9983277591973244, 0.9987437185929648, 0.9983361064891847, 0.9979253112033195, nan, 0, 984, 923, 523, 1043, 655, 0, 614, 651, 845, 716, 1025, 0, 568, 1040, 1116, 988, 1071, 0, 1159, 596, 794, 599, 480, 0, 0.035171967700775082]
N, d, L:  5898 6 5
loss_valid 0 1.14675708368
loss_train 0 1.15174771681
loss_valid 1 0.983146463224
loss_train 1 0.99112135364
loss_valid 2 0.910538488038
loss_train 2 0.920468243659
loss_valid 3 0.871461721929
loss_train 3 0.88279605843
loss_valid 4 nan
loss_train 4 nan
nan train loss
early stop at epoch 4
[0.021964612568639415, 0, 0, 0.090298962782184258, 0.46308724832214765, 0.65466748017083587, 0.73825503355704702, 1.0, 0.09029896278218426, 0.46308724832214765, 0.6546674801708359, 0.738255033557047, 1.0, 1639.0, 1639.0, 1639.0, 1639.0, 1639.0, 0, 0.45826274674174039, -0.0082977425259304471, nan, 0.0010834236186348862, 0.001049317943336831, 0.0019193857965451055, 0.000984251968503937, 0.9986111111111111, nan, 0.0014992503748125937, 0.0014970059880239522, 0.0011737089201877935, 0.9985507246376811, 0.9989754098360656, nan, 0.00196078431372549, 0.001004016064257028, 0.9991087344028521, 0.9989743589743589, 0.999117387466902, nan, 0.0008496176720475786, 0.9984051036682615, 0.9987357774968394, 0.9984544049459042, 0.9978540772532188, nan, 0, 921, 951, 519, 1014, 718, 0, 665, 666, 850, 688, 974, 0, 508, 994, 1120, 973, 1131, 0, 1175, 625, 789, 645, 464, 0, 0.035360157068322448]
N, d, L:  5899 6 5
loss_valid 0 1.15703111202
loss_train 0 1.14000597666
loss_valid 1 0.994739089592
loss_train 1 0.975418782581
loss_valid 2 nan
loss_train 2 nan
nan train loss
early stop at epoch 2
[0.029304029304029304, 0, 0, 0.078754578754578752, 0.45238095238095238, 0.65201465201465203, 0.75641025641025639, 1.0, 0.07875457875457875, 0.4523809523809524, 0.652014652014652, 0.7564102564102564, 1.0, 1638.0, 1638.0, 1638.0, 1638.0, 1638.0, 0, 0.44561415470883736, -0.021855921855921855, nan, 0.0011098779134295228, 0.0010683760683760685, 0.001893939393939394, 0.0009514747859181732, 0.9986504723346828, nan, 0.0014771048744460858, 0.0014858841010401188, 0.0011148272017837235, 0.9985835694050992, 0.9989637305699481, nan, 0.001869158878504673, 0.000975609756097561, 0.9991023339317774, 0.9989680082559339, 0.999096657633243, nan, 0.0008576329331046312, 0.9983079526226735, 0.9986577181208054, 0.9983792544570502, 0.9978991596638656, nan, 0, 899, 934, 526, 1049, 739, 0, 675, 671, 895, 704, 963, 0, 533, 1023, 1112, 967, 1105, 0, 1164, 589, 743, 615, 474, 0, 0.035102538369273163]
N, d, L:  5899 6 5
loss_valid 0 1.14211884618
loss_train 0 1.14562081598
loss_valid 1 0.976474444634
loss_train 1 0.981440608835
loss_valid 2 0.907918501937
loss_train 2 0.912567351249
loss_valid 3 nan
loss_train 3 nan
nan train loss
early stop at epoch 3
[0.025030525030525032, 0, 0, 0.073870573870573872, 0.46825396825396826, 0.66117216117216115, 0.75152625152625152, 1.0, 0.07387057387057387, 0.46825396825396826, 0.6611721611721612, 0.7515262515262515, 1.0, 1638.0, 1638.0, 1638.0, 1638.0, 1638.0, 0, 0.44365188471805167, -0.011111111111111113, nan, 0.0010471204188481676, 0.0010152284263959391, 0.0018656716417910447, 0.000984251968503937, 0.9985443959243085, nan, 0.0014903129657228018, 0.0015037593984962407, 0.0012077294685990338, 0.9984779299847/home/jasonzhang/anaconda2/lib/python2.7/site-packages/numpy/lib/nanfunctions.py:703: RuntimeWarning: Mean of empty slice
  warnings.warn("Mean of empty slice", RuntimeWarning)
/home/jasonzhang/anaconda2/lib/python2.7/site-packages/numpy/lib/nanfunctions.py:1202: RuntimeWarning: Degrees of freedom <= 0 for slice.
  warnings.warn("Degrees of freedom <= 0 for slice.", RuntimeWarning)
792, 0.9989701338825953, nan, 0.0019342359767891683, 0.00101010101010101, 0.9990958408679927, 0.9989764585465711, 0.9991111111111111, nan, 0.0008787346221441124, 0.9984025559105432, 0.9987714987714987, 0.9984662576687117, 0.998015873015873, nan, 0, 953, 983, 534, 1014, 685, 0, 669, 663, 826, 655, 969, 0, 515, 988, 1104, 975, 1123, 0, 1136, 624, 812, 650, 502, 0, 0.035285545071196404]
N, d, L:  5899 6 5
loss_valid 0 1.11963871891
loss_train 0 1.12764391954
loss_valid 1 0.962320566441
loss_train 1 0.976401881293
loss_valid 2 0.89489061986
loss_train 2 0.913331903918
loss_valid 3 nan
loss_train 3 nan
nan train loss
early stop at epoch 3
[0.03296703296703297, 0, 0, 0.085470085470085472, 0.49389499389499392, 0.64468864468864473, 0.74053724053724057, 1.0, 0.08547008547008547, 0.4938949938949939, 0.6446886446886447, 0.7405372405372406, 1.0, 1638.0, 1638.0, 1638.0, 1638.0, 1638.0, 0, 0.45800385908448127, 0.0092796092796092779, nan, 0.0010384215991692627, 0.0010559662090813093, 0.0018975332068311196, 0.000998003992015968, 0.9985272459499264, nan, 0.001610305958132045, 0.001564945226917058, 0.0012515644555694619, 0.9985611510791367, 0.9990205680705191, nan, 0.0018975332068311196, 0.0010131712259371835, 0.9991031390134529, 0.9990029910269193, 0.9991031390134529, nan, 0.00089126559714795, 0.9984375, 0.9988137603795967, 0.9984732824427481, 0.9980769230769231, nan, 0, 961, 945, 525, 1000, 677, 0, 619, 637, 797, 693, 1019, 0, 525, 985, 1113, 1001, 1113, 0, 1120, 638, 841, 653, 518, 0, 0.035663535314392605]
{'perf': [array([  2.73443931e-02,   0.00000000e+00,   0.00000000e+00,
         8.17862227e-02,   4.73754061e-01,   6.47218777e-01,
         7.45606891e-01,   1.00000000e+00,   8.17862227e-02,
         4.73754061e-01,   6.47218777e-01,   7.45606891e-01,
         1.00000000e+00,   1.63840000e+03,   1.63840000e+03,
         1.63840000e+03,   1.63840000e+03,   1.63840000e+03,
         0.00000000e+00,   4.50941665e-01,  -1.01798276e-02,
                    nan,   1.05860847e-03,   1.05399395e-03,
         1.89625839e-03,   9.74984103e-04,   9.98562231e-01,
                    nan,   1.54007016e-03,   1.51659766e-03,
         1.18569352e-03,   9.98556123e-01,   9.98991227e-01,
                    nan,   1.88321967e-03,   9.92518191e-04,
         9.99103119e-01,   9.98982343e-01,   9.99099266e-01,
                    nan,   8.67715453e-04,   9.98376174e-01,
         9.98744495e-01,   9.98421861e-01,   9.97954269e-01,
                    nan,   0.00000000e+00,   9.43600000e+02,
         9.47200000e+02,   5.25400000e+02,   1.02400000e+03,
         6.94800000e+02,   0.00000000e+00,   6.48400000e+02,
         6.57600000e+02,   8.42600000e+02,   6.91200000e+02,
         9.90000000e+02,   0.00000000e+00,   5.29800000e+02,
         1.00600000e+03,   1.11300000e+03,   9.80800000e+02,
         1.10860000e+03,   0.00000000e+00,   1.15080000e+03,
         6.14400000e+02,   7.95800000e+02,   6.32400000e+02,
         4.87600000e+02,   0.00000000e+00,   3.53167487e-02]), array([  3.73530938e-03,   0.00000000e+00,   0.00000000e+00,
         5.64706517e-03,   1.61816370e-02,   1.29595396e-02,
         7.07111254e-03,   0.00000000e+00,   5.64706517e-03,
         1.61816370e-02,   1.29595396e-02,   7.07111254e-03,
         0.00000000e+00,   4.89897949e-01,   4.89897949e-01,
         4.89897949e-01,   4.89897949e-01,   4.89897949e-01,
         0.00000000e+00,   6.13375558e-03,   1.09150875e-02,
                    nan,   3.39294940e-05,   2.22219651e-05,
         1.76053331e-05,   1.77767209e-05,   6.13587862e-05,
                    nan,   6.32134394e-05,   2.84548182e-05,
         4.47566571e-05,   4.36693567e-05,   2.66138308e-05,
                    nan,   7.15816237e-05,   2.11248644e-05,
         4.26636046e-06,   1.25482297e-05,   1.71228731e-05,
                    nan,   1.51396018e-05,   4.95821714e-05,
         5.12517725e-05,   5.44857615e-05,   8.09358467e-05,
                    nan,   0.00000000e+00,   3.00772339e+01,
         2.03017241e+01,   4.92341345e+00,   1.87723200e+01,
         2.99626434e+01,   0.00000000e+00,   2.62876397e+01,
         1.22245654e+01,   3.21284920e+01,   2.05075596e+01,
         2.64272587e+01,   0.00000000e+00,   2.09131538e+01,
         2.16979262e+01,   5.29150262e+00,   1.22049170e+01,
         2.07615028e+01,   0.00000000e+00,   1.99739831e+01,
         1.86826122e+01,   3.20711709e+01,   2.14997674e+01,
         1.96529896e+01,   0.00000000e+00,   1.94918707e-04])]}
